{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synthetic NIRS Generator - Final Evaluation",
        "",
        "**Systematic workflow for fitting and evaluating synthetic NIRS data generation.**",
        "",
        "This notebook follows a structured approach:",
        "1. **Pure Bands Fitting** - Unconstrained Gaussian profiles",
        "2. **Real Bands Fitting** - Using known NIR band assignments",
        "3. **Component Fitting** - Category-constrained chemical components",
        "4. **Optimized Component Selection** - Greedy search with refinement",
        "5. **Combined Fitting** - Components + Bands (4 combinations)",
        "6. **Results Display** - All approaches compared",
        "7. **Noise Evaluation** - Real vs synthetic randomness",
        "8. **Variance Fitting** - Operator-based and PCA-based",
        "9. **Variance Evaluation** - All variants with noise",
        "10. **Discriminator Training** - Real vs synthetic classification",
        "11. **Global Summary** - Final charts and statistics",
        "",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys",
        "from pathlib import Path",
        "import numpy as np",
        "import pandas as pd",
        "import matplotlib.pyplot as plt",
        "from sklearn.decomposition import PCA",
        "from sklearn.covariance import EmpiricalCovariance",
        "from sklearn.ensemble import RandomForestClassifier",
        "from sklearn.model_selection import train_test_split",
        "from sklearn.preprocessing import StandardScaler",
        "from sklearn.metrics import accuracy_score",
        "from scipy import signal as scipy_signal",
        "from scipy.optimize import nnls, minimize",
        "from scipy.stats import ks_2samp, wasserstein_distance",
        "from dataclasses import dataclass, field",
        "from typing import List, Tuple, Dict, Optional, Any",
        "import warnings",
        "warnings.filterwarnings('ignore')",
        "",
        "# Add project root to path",
        "root = Path.cwd().parent.parent",
        "if str(root) not in sys.path:",
        "    sys.path.insert(0, str(root))",
        "",
        "from nirs4all.data import DatasetConfigs, detect_signal_type",
        "from nirs4all.data.synthetic._constants import get_predefined_components",
        "from nirs4all.data.synthetic._bands import (",
        "    NIR_BANDS, BandAssignment, get_bands_in_range,",
        ")",
        "",
        "%matplotlib inline",
        "plt.rcParams['figure.dpi'] = 100",
        "plt.rcParams['figure.figsize'] = (14, 6)",
        "",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "## 1. Configuration",
        "",
        "Define datasets, categories, and component mappings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset configuration",
        "DATASET_BASE = Path(\"/home/delete/NIRS DB/x_bank\")",
        "",
        "DATASET_NAMES = [",
        "    \"Beer_OriginalExtract_60_KS\",",
        "    \"Biscuit_Fat_40_RandomSplit\",",
        "    \"DIESEL_bp50_246_b-a\",",
        "    \"grapevine_chloride_556_KS\",",
        "    \"LUCAS_SOC_Organic_1102_NocitaKS\",",
        "    \"Milk_Fat_1224_KS\",",
        "    \"Poultry_manure_CaO_KS\",",
        "    \"Rice_Amylose_313_YbasedSplit\",",
        "    \"TABLET_Escitalopramt_310_Zhao\",",
        "]",
        "",
        "# Dataset to category mapping",
        "DATASET_CATEGORIES = {",
        "    'Beer_OriginalExtract_60_KS': ['carbohydrates', 'alcohols', 'organic_acids', 'water_related'],",
        "    'Biscuit_Fat_40_RandomSplit': ['lipids', 'carbohydrates', 'proteins', 'water_related'],",
        "    'DIESEL_bp50_246_b-a': ['petroleum', 'hydrocarbons'],",
        "    'grapevine_chloride_556_KS': ['pigments', 'carbohydrates', 'water_related', 'minerals'],",
        "    'LUCAS_SOC_Organic_1102_NocitaKS': ['carbohydrates', 'minerals', 'water_related', 'organic_matter'],",
        "    'Milk_Fat_1224_KS': ['lipids', 'proteins', 'carbohydrates', 'water_related'],",
        "    'Poultry_manure_CaO_KS': ['proteins', 'minerals', 'water_related', 'organic_matter'],",
        "    'Rice_Amylose_313_YbasedSplit': ['carbohydrates', 'proteins', 'water_related'],",
        "    'TABLET_Escitalopramt_310_Zhao': ['pharmaceuticals', 'carbohydrates', 'water_related'],",
        "}",
        "",
        "# Component to category mapping",
        "COMPONENT_CATEGORIES = {",
        "    'water_related': ['water', 'moisture'],",
        "    'proteins': ['protein', 'nitrogen_compound', 'urea', 'amino_acid', 'casein', 'gluten',",
        "                 'albumin', 'collagen', 'keratin', 'zein', 'gelatin', 'whey'],",
        "    'lipids': ['lipid', 'oil', 'saturated_fat', 'unsaturated_fat', 'waxes',",
        "               'oleic_acid', 'linoleic_acid', 'linolenic_acid', 'palmitic_acid',",
        "               'stearic_acid', 'phospholipid', 'cholesterol', 'cocoa_butter'],",
        "    'hydrocarbons': ['aromatic', 'alkane'],",
        "    'petroleum': ['crude_oil', 'diesel', 'gasoline', 'kerosene', 'pah'],",
        "    'carbohydrates': ['starch', 'cellulose', 'glucose', 'fructose', 'sucrose',",
        "                      'hemicellulose', 'lignin', 'lactose', 'cotton', 'dietary_fiber'],",
        "    'alcohols': ['ethanol', 'methanol', 'glycerol', 'propanol', 'butanol',",
        "                 'sorbitol', 'mannitol', 'xylitol', 'isopropanol'],",
        "    'organic_acids': ['acetic_acid', 'citric_acid', 'lactic_acid', 'malic_acid',",
        "                      'tartaric_acid', 'formic_acid', 'oxalic_acid'],",
        "    'pigments': ['chlorophyll', 'chlorophyll_a', 'chlorophyll_b', 'carotenoid',",
        "                 'beta_carotene', 'lycopene', 'lutein', 'anthocyanin'],",
        "    'minerals': ['carbonates', 'gypsum', 'kaolinite', 'montmorillonite', 'illite', 'goethite'],",
        "    'pharmaceuticals': ['caffeine', 'aspirin', 'paracetamol', 'ibuprofen', 'microcrystalline_cellulose'],",
        "    'organic_matter': ['lignin', 'cellulose', 'hemicellulose', 'protein'],",
        "}",
        "",
        "# Excluded components (overfit risk)",
        "EXCLUDED_COMPONENTS = {'cytochrome_c', 'myoglobin', 'hemoglobin_oxy', 'hemoglobin_deoxy', 'bilirubin', 'melanin', 'pah'}",
        "UNIVERSAL_COMPONENTS = {'water', 'moisture'}",
        "",
        "def get_priority_components(dataset_name: str) -> list:",
        "    categories = DATASET_CATEGORIES.get(dataset_name, [])",
        "    priority = []",
        "    seen = set()",
        "    for cat in categories:",
        "        for comp in COMPONENT_CATEGORIES.get(cat, []):",
        "            if comp not in seen:",
        "                priority.append(comp)",
        "                seen.add(comp)",
        "    return priority",
        "",
        "print(f\"Configured {len(DATASET_NAMES)} datasets\")",
        "print(f\"Component categories: {len(COMPONENT_CATEGORIES)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Dataset Loading\n",
        "\n",
        "Load all datasets with signal analysis and diversity metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(name: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load dataset with full signal analysis.\"\"\"\n",
        "    csv_path = DATASET_BASE / f\"{name}.csv\"\n",
        "    config = {\"x_train\": str(csv_path), \"delimiter\": \",\", \"has_header\": True, \"header_unit\": \"nm\"}\n",
        "    ds = DatasetConfigs(config).get_datasets()[0]\n",
        "    X = ds.x({}, layout='2d')\n",
        "    wl = ds.wavelengths_nm(0)\n",
        "    if wl is None:\n",
        "        wl = np.arange(X.shape[1])\n",
        "\n",
        "    signal_type, confidence, reason = detect_signal_type(X, wl, \"nm\")\n",
        "\n",
        "    min_val, max_val = X.min(), X.max()\n",
        "    mean_val = X.mean()\n",
        "    \n",
        "    # Detect preprocessing type\n",
        "    is_derivative = min_val < -0.5 or (min_val < 0 and abs(mean_val) < 0.1)\n",
        "    \n",
        "    if is_derivative:\n",
        "        preprocessing_type = \"first_derivative\"\n",
        "    elif max_val > 10:\n",
        "        preprocessing_type = \"raw_reflectance\"\n",
        "    elif max_val > 3.0:\n",
        "        preprocessing_type = \"high_absorbance\"\n",
        "    else:\n",
        "        preprocessing_type = \"raw_absorbance\"\n",
        "\n",
        "    median = np.median(X, axis=0)\n",
        "    std_profile = X.std(axis=0)\n",
        "\n",
        "    return {\n",
        "        \"name\": name, \"X\": X, \"wl\": wl,\n",
        "        \"n_samples\": X.shape[0], \"n_wavelengths\": X.shape[1],\n",
        "        \"signal_type\": signal_type,\n",
        "        \"min\": min_val, \"max\": max_val, \"mean\": mean_val,\n",
        "        \"is_derivative\": is_derivative,\n",
        "        \"preprocessing_type\": preprocessing_type,\n",
        "        \"median\": median, \"std_profile\": std_profile,\n",
        "        \"categories\": DATASET_CATEGORIES.get(name, []),\n",
        "        \"priority_components\": get_priority_components(name),\n",
        "    }\n",
        "\n",
        "def compute_diversity_metrics(X: np.ndarray) -> dict:\n",
        "    \"\"\"Compute diversity metrics for a dataset.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    centroid = X.mean(axis=0)\n",
        "    euclidean_dists = np.linalg.norm(X - centroid, axis=1)\n",
        "\n",
        "    try:\n",
        "        cov = EmpiricalCovariance().fit(X)\n",
        "        mahal_dists = np.sqrt(cov.mahalanobis(X))\n",
        "    except:\n",
        "        std = X.std(axis=0) + 1e-10\n",
        "        X_std = (X - centroid) / std\n",
        "        mahal_dists = np.linalg.norm(X_std, axis=1)\n",
        "\n",
        "    n_components = min(20, n_samples - 1, X.shape[1])\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(X)\n",
        "\n",
        "    return {\n",
        "        'euclidean': {'mean': np.mean(euclidean_dists), 'std': np.std(euclidean_dists)},\n",
        "        'mahalanobis': {'mean': np.mean(mahal_dists), 'std': np.std(mahal_dists)},\n",
        "        'pca': {\n",
        "            'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),\n",
        "            'cumsum_90': int(np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9) + 1),\n",
        "        },\n",
        "        'variance': {'mean_var': np.mean(X.var(axis=0)), 'std_profile': X.std(axis=0)},\n",
        "    }\n",
        "\n",
        "# Load all datasets\n",
        "datasets = []\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Name':<40} {'Shape':>12} {'Range':>18} {'Type':>18}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for name in DATASET_NAMES:\n",
        "    try:\n",
        "        d = load_dataset(name)\n",
        "        d['diversity_metrics'] = compute_diversity_metrics(d['X'])\n",
        "        datasets.append(d)\n",
        "        print(f\"{d['name'][:40]:<40} {str(d['X'].shape):>12} [{d['min']:.2f}, {d['max']:.2f}] {d['preprocessing_type']:>18}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{name[:40]:<40} FAILED: {e}\")\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(f\"Loaded {len(datasets)} datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all datasets\n",
        "n_datasets = len(datasets)\n",
        "n_cols = 3\n",
        "n_rows = (n_datasets + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3.5 * n_rows))\n",
        "axes = axes.flatten() if n_datasets > 1 else [axes]\n",
        "\n",
        "for i, d in enumerate(datasets):\n",
        "    ax = axes[i]\n",
        "    X, wl = d['X'], d['wl']\n",
        "    \n",
        "    # Plot percentile envelope\n",
        "    p05, p95 = np.percentile(X, [5, 95], axis=0)\n",
        "    ax.fill_between(wl, p05, p95, alpha=0.3, color='steelblue', label='5-95%')\n",
        "    ax.plot(wl, d['median'], 'b-', lw=1.5, label='Median')\n",
        "    \n",
        "    ax.set_title(f\"{d['name'][:30]}\\n({d['n_samples']} samples)\", fontsize=9)\n",
        "    ax.set_xlabel('Wavelength (nm)', fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "for i in range(n_datasets, len(axes)):\n",
        "    axes[i].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Dataset Overview', y=1.02, fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Pure Bands Fitting\n",
        "\n",
        "Fit spectra using unconstrained Gaussian profiles. This is the simplest approach:\n",
        "- No prior knowledge of band positions\n",
        "- Iteratively adds bands to minimize residual\n",
        "- Good baseline for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PureBandFitter:\n",
        "    \"\"\"\n",
        "    Fit spectra using Gaussian profiles with iterative refinement.\n",
        "    Simple, fast, effective approach without prior band knowledge.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_bands=50, min_sigma=1.0, max_sigma=300.0,\n",
        "                 baseline_order=3, target_r2=0.97, preprocessing=None):\n",
        "        self.max_bands = max_bands\n",
        "        self.min_sigma = min_sigma\n",
        "        self.max_sigma = max_sigma\n",
        "        self.baseline_order = baseline_order\n",
        "        self.target_r2 = target_r2\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def _compute_bands(self, wl, centers, sigmas, amplitudes):\n",
        "        result = np.zeros_like(wl, dtype=float)\n",
        "        for c, s, a in zip(np.atleast_1d(centers),\n",
        "                           np.maximum(np.atleast_1d(sigmas), 0.1),\n",
        "                           np.atleast_1d(amplitudes)):\n",
        "            result += a * np.exp(-0.5 * ((wl - c) / s) ** 2)\n",
        "        return result\n",
        "\n",
        "    def _build_baseline(self, wl, coeffs):\n",
        "        wl_norm = (wl - wl.mean()) / (wl.max() - wl.min())\n",
        "        return np.polyval(coeffs[::-1], wl_norm)\n",
        "\n",
        "    def _apply_preprocessing(self, spectrum, wl):\n",
        "        if self.preprocessing is None:\n",
        "            return spectrum\n",
        "        wl_step = np.median(np.diff(wl))\n",
        "        window = min(15, len(wl) // 10 * 2 + 1) | 1\n",
        "        window = max(5, window)\n",
        "        if self.preprocessing == 'first_derivative':\n",
        "            return scipy_signal.savgol_filter(spectrum, window, min(3, window-2), deriv=1) / wl_step\n",
        "        elif self.preprocessing == 'second_derivative':\n",
        "            return scipy_signal.savgol_filter(spectrum, window, min(4, window-2), deriv=2) / (wl_step ** 2)\n",
        "        return spectrum\n",
        "\n",
        "    def detect_peaks(self, spectrum, wl):\n",
        "        peaks = []\n",
        "        wl_range = wl.max() - wl.min()\n",
        "\n",
        "        for window in [51, 21, 11, 5]:\n",
        "            if window >= len(wl):\n",
        "                continue\n",
        "            window = window | 1\n",
        "            smoothed = scipy_signal.savgol_filter(spectrum, window, min(3, window - 2))\n",
        "            prominence = np.std(smoothed) * 0.01\n",
        "            for idx in np.concatenate([\n",
        "                scipy_signal.find_peaks(smoothed, prominence=prominence, distance=2)[0],\n",
        "                scipy_signal.find_peaks(-smoothed, prominence=prominence, distance=2)[0]\n",
        "            ]):\n",
        "                if 0 <= idx < len(wl):\n",
        "                    peaks.append({'center': wl[idx], 'sigma': wl_range / 20,\n",
        "                                  'amplitude': spectrum[idx] - np.median(spectrum)})\n",
        "\n",
        "        n_grid = max(25, int(wl_range / 40))\n",
        "        for i in range(n_grid):\n",
        "            center = wl.min() + wl_range * (i + 0.5) / n_grid\n",
        "            idx = np.argmin(np.abs(wl - center))\n",
        "            peaks.append({'center': center, 'sigma': wl_range / n_grid,\n",
        "                          'amplitude': (spectrum[idx] - np.median(spectrum)) * 0.5})\n",
        "\n",
        "        peaks = sorted(peaks, key=lambda p: -abs(p['amplitude']))\n",
        "        merged = []\n",
        "        min_dist = wl_range / 100\n",
        "        for p in peaks:\n",
        "            if not any(abs(p['center'] - m['center']) < min_dist for m in merged):\n",
        "                merged.append(p)\n",
        "            if len(merged) >= self.max_bands:\n",
        "                break\n",
        "        return merged\n",
        "\n",
        "    def fit(self, spectrum, wl, n_iterations=5, max_total_iterations=20):\n",
        "        wl_min, wl_max = wl.min(), wl.max()\n",
        "        wl_range = wl_max - wl_min\n",
        "        spec_range = spectrum.max() - spectrum.min()\n",
        "        n_baseline = self.baseline_order + 1\n",
        "\n",
        "        peaks = self.detect_peaks(spectrum, wl)\n",
        "        n_bands = len(peaks)\n",
        "        amp_bound = spec_range * 4\n",
        "\n",
        "        x0, bounds_lo, bounds_hi = [], [], []\n",
        "        for p in peaks:\n",
        "            sigma = np.clip(p['sigma'], self.min_sigma, self.max_sigma)\n",
        "            x0.extend([p['center'], sigma, p['amplitude']])\n",
        "            bounds_lo.extend([wl_min - wl_range * 0.05, self.min_sigma, -amp_bound])\n",
        "            bounds_hi.extend([wl_max + wl_range * 0.05, self.max_sigma, amp_bound])\n",
        "\n",
        "        wl_norm = (wl - wl.mean()) / wl_range\n",
        "        baseline_init = np.polyfit(wl_norm, spectrum, self.baseline_order)[::-1]\n",
        "        for i in range(n_baseline):\n",
        "            x0.append(baseline_init[i] if i < len(baseline_init) else 0.0)\n",
        "            bounds_lo.append(-amp_bound * 10)\n",
        "            bounds_hi.append(amp_bound * 10)\n",
        "        x0 = np.array(x0)\n",
        "\n",
        "        def model(params, n_b):\n",
        "            centers = params[0:n_b*3:3]\n",
        "            sigmas = params[1:n_b*3:3]\n",
        "            amplitudes = params[2:n_b*3:3]\n",
        "            baseline_coeffs = params[n_b*3:n_b*3+n_baseline]\n",
        "            fitted = self._compute_bands(wl, centers, sigmas, amplitudes)\n",
        "            fitted += self._build_baseline(wl, baseline_coeffs)\n",
        "            return self._apply_preprocessing(fitted, wl)\n",
        "\n",
        "        def objective(params, n_b):\n",
        "            return np.sum((spectrum - model(params, n_b)) ** 2)\n",
        "\n",
        "        best_result = None\n",
        "        best_r2 = -np.inf\n",
        "\n",
        "        for iteration in range(n_iterations + 1):\n",
        "            if best_r2 >= self.target_r2:\n",
        "                break\n",
        "\n",
        "            if iteration > 0 and best_result is not None:\n",
        "                residual = spectrum - best_result['fitted']\n",
        "                res_std = np.std(residual)\n",
        "                res_peaks = np.concatenate([\n",
        "                    scipy_signal.find_peaks(residual, prominence=res_std * 0.02, distance=2)[0],\n",
        "                    scipy_signal.find_peaks(-residual, prominence=res_std * 0.02, distance=2)[0]\n",
        "                ])\n",
        "                res_peaks = sorted(res_peaks, key=lambda i: -abs(residual[i]))\n",
        "\n",
        "                new_params = list(best_result['params'][:n_bands*3])\n",
        "                for idx in res_peaks[:15]:\n",
        "                    if len(new_params) // 3 < self.max_bands:\n",
        "                        new_params.extend([wl[idx], wl_range / 80, residual[idx]])\n",
        "\n",
        "                if len(new_params) == n_bands * 3:\n",
        "                    top_idx = np.argsort(-np.abs(residual))[:8]\n",
        "                    for idx in top_idx:\n",
        "                        if len(new_params) // 3 < self.max_bands:\n",
        "                            new_params.extend([wl[idx], wl_range / 100, residual[idx]])\n",
        "\n",
        "                n_bands = len(new_params) // 3\n",
        "                x0 = np.array(new_params + list(best_result['params'][-n_baseline:]))\n",
        "\n",
        "                bounds_lo, bounds_hi = [], []\n",
        "                for _ in range(n_bands):\n",
        "                    bounds_lo.extend([wl_min - wl_range * 0.05, self.min_sigma, -amp_bound])\n",
        "                    bounds_hi.extend([wl_max + wl_range * 0.05, self.max_sigma, amp_bound])\n",
        "                for _ in range(n_baseline):\n",
        "                    bounds_lo.append(-amp_bound * 10)\n",
        "                    bounds_hi.append(amp_bound * 10)\n",
        "\n",
        "            try:\n",
        "                res = minimize(lambda p: objective(p, n_bands), x0, method='L-BFGS-B',\n",
        "                              bounds=list(zip(bounds_lo, bounds_hi)),\n",
        "                              options={'maxiter': 1500, 'ftol': 1e-14})\n",
        "                fitted = model(res.x, n_bands)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            ss_res = np.sum((spectrum - fitted) ** 2)\n",
        "            ss_tot = np.sum((spectrum - np.mean(spectrum)) ** 2)\n",
        "            r2 = 1 - (ss_res / ss_tot) if ss_tot > 1e-10 else 0.0\n",
        "\n",
        "            bands = [{'center': res.x[i*3], 'sigma': res.x[i*3+1], 'amplitude': res.x[i*3+2]}\n",
        "                     for i in range(n_bands) if abs(res.x[i*3+2]) > amp_bound * 1e-6]\n",
        "\n",
        "            result = {\n",
        "                'bands': sorted(bands, key=lambda b: b['center']),\n",
        "                'fitted': fitted, 'r_squared': r2,\n",
        "                'rmse': np.sqrt(np.mean((spectrum - fitted) ** 2)),\n",
        "                'n_bands': len(bands),\n",
        "                'baseline_coeffs': res.x[n_bands*3:n_bands*3+n_baseline].tolist(),\n",
        "                'params': res.x,\n",
        "            }\n",
        "\n",
        "            if r2 > best_r2:\n",
        "                best_r2 = r2\n",
        "                best_result = result\n",
        "\n",
        "        return best_result if best_result else result\n",
        "\n",
        "print(\"PureBandFitter class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit pure bands for all datasets\n",
        "print(\"\\nPURE BAND FITTING\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for d in datasets:\n",
        "    wl, median = d['wl'], d['median']\n",
        "    window = min(21, len(wl) // 10 * 2 + 1) | 1\n",
        "    target = scipy_signal.savgol_filter(median, window, 2)\n",
        "    \n",
        "    # Determine preprocessing for derivative data\n",
        "    preprocessing = 'first_derivative' if d['is_derivative'] else None\n",
        "    \n",
        "    fitter = PureBandFitter(max_bands=40, target_r2=0.98, preprocessing=preprocessing)\n",
        "    result = fitter.fit(target, wl)\n",
        "    d['pure_band_result'] = result\n",
        "    \n",
        "    status = \"\u2713\" if result['r_squared'] > 0.95 else \"\u25b3\" if result['r_squared'] > 0.8 else \"\u2717\"\n",
        "    print(f\"{status} {d['name'][:45]:<45} R\u00b2={result['r_squared']:.4f}  Bands={result['n_bands']:2d}\")\n",
        "\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize pure band fitting\n",
        "n_cols = 3\n",
        "n_rows = (len(datasets) + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3.5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, d in enumerate(datasets):\n",
        "    ax = axes[i]\n",
        "    wl = d['wl']\n",
        "    window = min(21, len(wl) // 10 * 2 + 1) | 1\n",
        "    target = scipy_signal.savgol_filter(d['median'], window, 2)\n",
        "    result = d['pure_band_result']\n",
        "    \n",
        "    ax.plot(wl, target, 'b-', lw=1.5, label='Real', alpha=0.8)\n",
        "    ax.plot(wl, result['fitted'], 'r--', lw=1.5, label=f\"Fitted (R\u00b2={result['r_squared']:.3f})\")\n",
        "    ax.fill_between(wl, target, result['fitted'], alpha=0.2, color='gray')\n",
        "    \n",
        "    ax.set_title(f\"{d['name'][:30]}\\n{result['n_bands']} bands\", fontsize=9)\n",
        "    ax.legend(fontsize=7, loc='best')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "for i in range(len(datasets), len(axes)):\n",
        "    axes[i].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Pure Band Fitting Results', y=1.02, fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Real Bands Fitting\n",
        "\n",
        "Fit spectra using known NIR band assignments from literature.\n",
        "Unlike pure bands, this uses:\n",
        "- Fixed band centers from known assignments\n",
        "- Constrained sigma ranges\n",
        "- Only amplitude optimization (more interpretable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RealBandFitter:\n",
        "    \"\"\"\n",
        "    Fit spectra using REAL NIR band assignments from _bands.py.\n",
        "    Uses fixed centers from known literature for physically meaningful decomposition.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, baseline_order=3, target_r2=0.95,\n",
        "                 allow_sigma_variation=True, sigma_margin=0.3):\n",
        "        self.baseline_order = baseline_order\n",
        "        self.target_r2 = target_r2\n",
        "        self.allow_sigma_variation = allow_sigma_variation\n",
        "        self.sigma_margin = sigma_margin\n",
        "\n",
        "    def get_candidate_bands(self, wl_min: float, wl_max: float) -> List[BandAssignment]:\n",
        "        return get_bands_in_range(wl_min, wl_max)\n",
        "\n",
        "    def _compute_band(self, wl, center, sigma, amplitude):\n",
        "        return amplitude * np.exp(-0.5 * ((wl - center) / sigma) ** 2)\n",
        "\n",
        "    def _compute_all_bands(self, wl, bands, amplitudes, sigmas=None):\n",
        "        result = np.zeros_like(wl, dtype=float)\n",
        "        for i, band in enumerate(bands):\n",
        "            sigma = sigmas[i] if sigmas is not None else (band.sigma_range[0] + band.sigma_range[1]) / 2\n",
        "            result += self._compute_band(wl, band.center, sigma, amplitudes[i])\n",
        "        return result\n",
        "\n",
        "    def _build_baseline(self, wl, coeffs):\n",
        "        wl_norm = (wl - wl.mean()) / (wl.max() - wl.min())\n",
        "        return np.polyval(coeffs[::-1], wl_norm)\n",
        "\n",
        "    def fit(self, spectrum, wl, max_bands=50, n_iterations=3):\n",
        "        wl_min, wl_max = wl.min(), wl.max()\n",
        "        spec_range = spectrum.max() - spectrum.min()\n",
        "        spec_mean = spectrum.mean()\n",
        "        n_baseline = self.baseline_order + 1\n",
        "\n",
        "        all_bands = self.get_candidate_bands(wl_min - 50, wl_max + 50)\n",
        "        candidate_bands = [b for b in all_bands if wl_min <= b.center <= wl_max]\n",
        "\n",
        "        if not candidate_bands:\n",
        "            candidate_bands = all_bands[:max_bands]\n",
        "\n",
        "        if not candidate_bands:\n",
        "            wl_norm = (wl - wl.mean()) / (wl.max() - wl.min())\n",
        "            baseline_coeffs = np.polyfit(wl_norm, spectrum, self.baseline_order)[::-1]\n",
        "            fitted = self._build_baseline(wl, baseline_coeffs)\n",
        "            ss_res = np.sum((spectrum - fitted) ** 2)\n",
        "            ss_tot = np.sum((spectrum - np.mean(spectrum)) ** 2)\n",
        "            r2 = 1 - ss_res / ss_tot if ss_tot > 1e-10 else 0.0\n",
        "            return {'bands': [], 'fitted': fitted, 'r_squared': r2, 'n_bands': 0}\n",
        "\n",
        "        intensity_order = {'very_strong': 0, 'strong': 1, 'medium': 2, 'weak': 3, 'very_weak': 4}\n",
        "        candidate_bands = sorted(candidate_bands, key=lambda b: (intensity_order.get(b.intensity, 5), b.center))\n",
        "        candidate_bands = candidate_bands[:max_bands]\n",
        "        n_bands = len(candidate_bands)\n",
        "\n",
        "        band_sigmas_mid = np.array([(b.sigma_range[0] + b.sigma_range[1]) / 2 for b in candidate_bands])\n",
        "        band_sigmas_lo = np.array([b.sigma_range[0] for b in candidate_bands])\n",
        "        band_sigmas_hi = np.array([b.sigma_range[1] for b in candidate_bands])\n",
        "\n",
        "        wl_norm = (wl - wl.mean()) / (wl.max() - wl.min())\n",
        "        baseline_init = np.polyfit(wl_norm, spectrum, self.baseline_order)[::-1]\n",
        "\n",
        "        if self.allow_sigma_variation:\n",
        "            n_params = n_bands * 2 + n_baseline\n",
        "            x0 = np.zeros(n_params)\n",
        "            for i, band in enumerate(candidate_bands):\n",
        "                idx = np.argmin(np.abs(wl - band.center))\n",
        "                x0[i] = max(0, (spectrum[idx] - spec_mean) * 0.5)\n",
        "            x0[n_bands:2*n_bands] = band_sigmas_mid\n",
        "            x0[2*n_bands:] = baseline_init\n",
        "\n",
        "            bounds_lo = ([-spec_range * 2] * n_bands +\n",
        "                        list(band_sigmas_lo * (1 - self.sigma_margin)) +\n",
        "                        [-spec_range * 10] * n_baseline)\n",
        "            bounds_hi = ([spec_range * 2] * n_bands +\n",
        "                        list(band_sigmas_hi * (1 + self.sigma_margin)) +\n",
        "                        [spec_range * 10] * n_baseline)\n",
        "\n",
        "            def model(params):\n",
        "                amplitudes = params[:n_bands]\n",
        "                sigmas = params[n_bands:2*n_bands]\n",
        "                baseline_coeffs = params[2*n_bands:]\n",
        "                return self._compute_all_bands(wl, candidate_bands, amplitudes, sigmas) + self._build_baseline(wl, baseline_coeffs)\n",
        "        else:\n",
        "            n_params = n_bands + n_baseline\n",
        "            x0 = np.zeros(n_params)\n",
        "            for i, band in enumerate(candidate_bands):\n",
        "                idx = np.argmin(np.abs(wl - band.center))\n",
        "                x0[i] = max(0, (spectrum[idx] - spec_mean) * 0.5)\n",
        "            x0[n_bands:] = baseline_init\n",
        "\n",
        "            bounds_lo = [-spec_range * 2] * n_bands + [-spec_range * 10] * n_baseline\n",
        "            bounds_hi = [spec_range * 2] * n_bands + [spec_range * 10] * n_baseline\n",
        "\n",
        "            def model(params):\n",
        "                amplitudes = params[:n_bands]\n",
        "                baseline_coeffs = params[n_bands:]\n",
        "                return self._compute_all_bands(wl, candidate_bands, amplitudes, band_sigmas_mid) + self._build_baseline(wl, baseline_coeffs)\n",
        "\n",
        "        def objective(params):\n",
        "            return np.sum((spectrum - model(params)) ** 2)\n",
        "\n",
        "        best_result = None\n",
        "        best_r2 = -np.inf\n",
        "\n",
        "        for iteration in range(n_iterations):\n",
        "            try:\n",
        "                res = minimize(objective, x0, method='L-BFGS-B',\n",
        "                              bounds=list(zip(bounds_lo, bounds_hi)),\n",
        "                              options={'maxiter': 2000, 'ftol': 1e-12})\n",
        "                fitted = model(res.x)\n",
        "\n",
        "                ss_res = np.sum((spectrum - fitted) ** 2)\n",
        "                ss_tot = np.sum((spectrum - np.mean(spectrum)) ** 2)\n",
        "                r2 = 1 - ss_res / ss_tot if ss_tot > 1e-10 else 0.0\n",
        "\n",
        "                if r2 > best_r2:\n",
        "                    best_r2 = r2\n",
        "                    if self.allow_sigma_variation:\n",
        "                        amplitudes = res.x[:n_bands]\n",
        "                        sigmas = res.x[n_bands:2*n_bands]\n",
        "                    else:\n",
        "                        amplitudes = res.x[:n_bands]\n",
        "                        sigmas = band_sigmas_mid\n",
        "\n",
        "                    band_info = []\n",
        "                    for i, band in enumerate(candidate_bands):\n",
        "                        if abs(amplitudes[i]) > spec_range * 1e-6:\n",
        "                            band_info.append({\n",
        "                                'center': band.center,\n",
        "                                'sigma': sigmas[i],\n",
        "                                'amplitude': amplitudes[i],\n",
        "                                'name': f\"{band.functional_group}/{band.overtone_level}\",\n",
        "                            })\n",
        "\n",
        "                    best_result = {\n",
        "                        'bands': sorted(band_info, key=lambda b: b['center']),\n",
        "                        'fitted': fitted,\n",
        "                        'r_squared': r2,\n",
        "                        'n_bands': len([a for a in amplitudes if abs(a) > spec_range * 1e-6]),\n",
        "                    }\n",
        "\n",
        "                if r2 >= self.target_r2:\n",
        "                    break\n",
        "                x0 = res.x + np.random.normal(0, 0.01, len(res.x))\n",
        "                x0 = np.clip(x0, bounds_lo, bounds_hi)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return best_result if best_result else {'bands': [], 'fitted': np.zeros_like(spectrum), 'r_squared': 0.0, 'n_bands': 0}\n",
        "\n",
        "print(\"RealBandFitter class defined.\")\n",
        "print(f\"Available NIR_BANDS groups: {list(NIR_BANDS.keys())}\")\n",
        "print(f\"Total bands: {sum(len(v) for v in NIR_BANDS.values())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit real bands for all datasets\n",
        "print(\"\\nREAL BAND FITTING\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for d in datasets:\n",
        "    wl, median = d['wl'], d['median']\n",
        "    window = min(21, len(wl) // 10 * 2 + 1) | 1\n",
        "    target = scipy_signal.savgol_filter(median, window, 2)\n",
        "    \n",
        "    fitter = RealBandFitter(target_r2=0.95)\n",
        "    result = fitter.fit(target, wl)\n",
        "    d['real_band_result'] = result\n",
        "    \n",
        "    status = \"\u2713\" if result['r_squared'] > 0.95 else \"\u25b3\" if result['r_squared'] > 0.8 else \"\u2717\"\n",
        "    print(f\"{status} {d['name'][:45]:<45} R\u00b2={result['r_squared']:.4f}  Bands={result['n_bands']:2d}\")\n",
        "\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare pure vs real band fitting\n",
        "print(\"\\nPURE vs REAL BAND COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Dataset':<35} {'Pure R\u00b2':>10} {'Real R\u00b2':>10} {'Better':>10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for d in datasets:\n",
        "    pure_r2 = d['pure_band_result']['r_squared']\n",
        "    real_r2 = d['real_band_result']['r_squared']\n",
        "    better = \"Pure\" if pure_r2 > real_r2 else \"Real\" if real_r2 > pure_r2 else \"Tie\"\n",
        "    print(f\"{d['name'][:35]:<35} {pure_r2:>10.4f} {real_r2:>10.4f} {better:>10}\")\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Component Fitting (Constrained)\n",
        "\n",
        "Fit spectra using predefined chemical component spectra.\n",
        "- Uses category-based component selection (domain knowledge)\n",
        "- Excludes problematic components that overfit\n",
        "- Non-negative least squares for interpretable quantities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConstrainedComponentFitter:\n",
        "    \"\"\"\n",
        "    Fit spectral components with constraints:\n",
        "    - Prioritize components from dataset-specific categories\n",
        "    - Exclude inappropriate/overfitting components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_components=15, regularization=0.01):\n",
        "        self.max_components = max_components\n",
        "        self.regularization = regularization\n",
        "        self.all_components = get_predefined_components()\n",
        "\n",
        "    def build_component_matrix(self, wl, component_names, exclude=None):\n",
        "        exclude = exclude or set()\n",
        "        names, spectra = [], []\n",
        "        for name in component_names:\n",
        "            if name in exclude or name not in self.all_components:\n",
        "                continue\n",
        "            comp = self.all_components[name]\n",
        "            spec = comp.compute(wl)\n",
        "            if np.max(np.abs(spec)) > 1e-10:\n",
        "                names.append(name)\n",
        "                spectra.append(spec)\n",
        "        A = np.column_stack(spectra) if spectra else np.zeros((len(wl), 0))\n",
        "        return A, names\n",
        "\n",
        "    def fit_constrained(self, target, wl, priority_components, baseline=None):\n",
        "        if baseline is None:\n",
        "            from scipy.ndimage import minimum_filter1d\n",
        "            baseline = minimum_filter1d(target, size=max(len(wl)//20, 5))\n",
        "\n",
        "        priority_expanded = list(priority_components) + list(UNIVERSAL_COMPONENTS)\n",
        "        priority_expanded = list(dict.fromkeys(priority_expanded))\n",
        "\n",
        "        A, names = self.build_component_matrix(wl, priority_expanded, exclude=EXCLUDED_COMPONENTS)\n",
        "        quantities = np.zeros(len(names))\n",
        "\n",
        "        if A.shape[1] > 0:\n",
        "            col_norms = np.linalg.norm(A, axis=0)\n",
        "            col_norms[col_norms < 1e-10] = 1.0\n",
        "            A_norm = A / col_norms\n",
        "            quantities_norm, _ = nnls(A_norm, target)\n",
        "            quantities = quantities_norm / col_norms\n",
        "\n",
        "        fitted = A @ quantities if len(quantities) > 0 else np.zeros_like(target)\n",
        "        residual = target - fitted\n",
        "\n",
        "        max_quantity = np.max(quantities) if len(quantities) > 0 else 0\n",
        "        threshold = max(max_quantity * 0.001, 1e-10)\n",
        "        top_indices = np.argsort(-quantities)[:self.max_components]\n",
        "        top_indices = [i for i in top_indices if quantities[i] > threshold]\n",
        "\n",
        "        ss_res = np.sum(residual ** 2)\n",
        "        ss_tot = np.sum((target - np.mean(target)) ** 2)\n",
        "        r2 = 1 - ss_res / ss_tot if ss_tot > 1e-10 else 0.0\n",
        "\n",
        "        return {\n",
        "            'components': [names[i] for i in top_indices],\n",
        "            'quantities': [quantities[i] for i in top_indices],\n",
        "            'all_quantities': quantities,\n",
        "            'all_names': names,\n",
        "            'fitted': fitted,\n",
        "            'residual': residual,\n",
        "            'r_squared': r2,\n",
        "            'A': A,\n",
        "        }\n",
        "\n",
        "print(\"ConstrainedComponentFitter class defined.\")\n",
        "print(f\"Available components: {len(get_predefined_components())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit components for all datasets\n",
        "print(\"\\nCONSTRAINED COMPONENT FITTING\")\n",
        "print(\"=\" * 120)\n",
        "\n",
        "for d in datasets:\n",
        "    wl, median = d['wl'], d['median']\n",
        "    window = min(21, len(wl) // 10 * 2 + 1) | 1\n",
        "    target = scipy_signal.savgol_filter(median, window, 2)\n",
        "    \n",
        "    fitter = ConstrainedComponentFitter(max_components=12)\n",
        "    result = fitter.fit_constrained(target, wl, d['priority_components'])\n",
        "    d['component_result'] = result\n",
        "    \n",
        "    status = \"\u2713\" if result['r_squared'] > 0.7 else \"\u25b3\" if result['r_squared'] > 0.3 else \"\u2717\"\n",
        "    print(f\"{status} {d['name'][:40]:<40} R\u00b2={result['r_squared']:.4f}  Components: {result['components'][:5]}\")\n",
        "\n",
        "print(\"=\" * 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Optimized Component Selection\n",
        "\n",
        "Improve component selection using greedy search with swap refinement.\n",
        "- Evaluates component combinations for best R\u00b2\n",
        "- Greedy forward selection as starting point\n",
        "- Swap refinement to escape local optima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimizedComponentFitter:\n",
        "    \"\"\"\n",
        "    Optimize component selection using greedy search + swap refinement.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_components=10, preprocessing=None, sg_window=15, sg_polyorder=3):\n",
        "        self.max_components = max_components\n",
        "        self.preprocessing = preprocessing\n",
        "        self.sg_window = sg_window\n",
        "        self.sg_polyorder = sg_polyorder\n",
        "        self.all_components = get_predefined_components()\n",
        "\n",
        "    def _apply_preprocessing(self, spectrum, wl):\n",
        "        if self.preprocessing is None:\n",
        "            return spectrum\n",
        "        wl_step = np.median(np.diff(wl))\n",
        "        window = min(self.sg_window, len(wl) // 10 * 2 + 1)\n",
        "        window = max(5, window) | 1\n",
        "        if self.preprocessing == 'first_derivative':\n",
        "            return scipy_signal.savgol_filter(spectrum, window, min(self.sg_polyorder, window-2), deriv=1) / wl_step\n",
        "        elif self.preprocessing == 'second_derivative':\n",
        "            return scipy_signal.savgol_filter(spectrum, window, min(self.sg_polyorder+1, window-2), deriv=2) / (wl_step ** 2)\n",
        "        return spectrum\n",
        "\n",
        "    def _compute_component_spectra(self, wl):\n",
        "        spectra = {}\n",
        "        for name, comp in self.all_components.items():\n",
        "            if name in EXCLUDED_COMPONENTS:\n",
        "                continue\n",
        "            spec = comp.compute(wl)\n",
        "            spec_prep = self._apply_preprocessing(spec, wl)\n",
        "            if np.max(np.abs(spec_prep)) > 1e-10:\n",
        "                spectra[name] = spec_prep\n",
        "        return spectra\n",
        "\n",
        "    def _fit_with_components(self, target, component_spectra, component_names):\n",
        "        if not component_names:\n",
        "            return 0.0, np.zeros(0)\n",
        "        valid_names = [n for n in component_names if n in component_spectra]\n",
        "        if not valid_names:\n",
        "            return 0.0, np.zeros(0)\n",
        "\n",
        "        A = np.column_stack([component_spectra[n] for n in valid_names])\n",
        "        col_norms = np.linalg.norm(A, axis=0)\n",
        "        col_norms[col_norms < 1e-10] = 1.0\n",
        "        A_norm = A / col_norms\n",
        "\n",
        "        try:\n",
        "            quantities_norm, _ = nnls(A_norm, target)\n",
        "        except:\n",
        "            return 0.0, np.zeros(len(valid_names))\n",
        "\n",
        "        quantities = quantities_norm / col_norms\n",
        "        fitted = A @ quantities\n",
        "        ss_res = np.sum((target - fitted) ** 2)\n",
        "        ss_tot = np.sum((target - np.mean(target)) ** 2)\n",
        "        r2 = 1 - ss_res / ss_tot if ss_tot > 1e-10 else 0.0\n",
        "        return r2, quantities\n",
        "\n",
        "    def optimize(self, target, wl, priority_categories=None, verbose=False):\n",
        "        component_spectra = self._compute_component_spectra(wl)\n",
        "        all_names = list(component_spectra.keys())\n",
        "\n",
        "        if not all_names:\n",
        "            return {'components': [], 'quantities': [], 'fitted': np.zeros_like(target), 'r_squared': 0.0, 'n_components': 0}\n",
        "\n",
        "        # Build priority pool\n",
        "        category_components = {}\n",
        "        for cat, comp_list in COMPONENT_CATEGORIES.items():\n",
        "            category_components[cat] = [n for n in comp_list if n in all_names]\n",
        "\n",
        "        priority_pool = []\n",
        "        if priority_categories:\n",
        "            for cat in priority_categories:\n",
        "                priority_pool.extend(category_components.get(cat, []))\n",
        "        priority_pool = list(dict.fromkeys(priority_pool))\n",
        "        other_pool = [n for n in all_names if n not in priority_pool]\n",
        "        search_pool = priority_pool + other_pool\n",
        "\n",
        "        # Greedy forward selection\n",
        "        best_components = []\n",
        "        best_r2 = 0.0\n",
        "        remaining = search_pool.copy()\n",
        "\n",
        "        for _ in range(min(self.max_components, len(search_pool))):\n",
        "            best_add = None\n",
        "            best_add_r2 = best_r2\n",
        "\n",
        "            for comp in remaining[:30]:\n",
        "                test_components = best_components + [comp]\n",
        "                r2, _ = self._fit_with_components(target, component_spectra, test_components)\n",
        "                if r2 > best_add_r2 + 0.001:\n",
        "                    best_add = comp\n",
        "                    best_add_r2 = r2\n",
        "\n",
        "            if best_add is None:\n",
        "                break\n",
        "            best_components.append(best_add)\n",
        "            remaining.remove(best_add)\n",
        "            best_r2 = best_add_r2\n",
        "\n",
        "        # Swap refinement\n",
        "        improved = True\n",
        "        n_swaps = 0\n",
        "        while improved and n_swaps < 10:\n",
        "            improved = False\n",
        "            for i, old_comp in enumerate(best_components):\n",
        "                for new_comp in other_pool[:20]:\n",
        "                    if new_comp in best_components:\n",
        "                        continue\n",
        "                    test_components = best_components.copy()\n",
        "                    test_components[i] = new_comp\n",
        "                    r2, _ = self._fit_with_components(target, component_spectra, test_components)\n",
        "                    if r2 > best_r2 + 0.002:\n",
        "                        best_components[i] = new_comp\n",
        "                        best_r2 = r2\n",
        "                        improved = True\n",
        "                        n_swaps += 1\n",
        "                        break\n",
        "                if improved:\n",
        "                    break\n",
        "\n",
        "        # Final fit\n",
        "        r2, quantities = self._fit_with_components(target, component_spectra, best_components)\n",
        "        valid_components = [n for n in best_components if n in component_spectra]\n",
        "        if valid_components:\n",
        "            A = np.column_stack([component_spectra[n] for n in valid_components])\n",
        "            fitted = A @ quantities\n",
        "        else:\n",
        "            fitted = np.zeros_like(target)\n",
        "\n",
        "        return {\n",
        "            'components': best_components,\n",
        "            'quantities': quantities.tolist() if len(quantities) > 0 else [],\n",
        "            'fitted': fitted,\n",
        "            'residual': target - fitted,\n",
        "            'r_squared': r2,\n",
        "            'n_components': len(best_components),\n",
        "        }\n",
        "\n",
        "print(\"OptimizedComponentFitter class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run optimized component fitting\n",
        "print(\"\\nOPTIMIZED COMPONENT SELECTION\")\n",
        "print(\"=\" * 120)\n",
        "\n",
        "for d in datasets:\n",
        "    wl, median = d['wl'], d['median']\n",
        "    window = min(21, len(wl) // 10 * 2 + 1) | 1\n",
        "    target = scipy_signal.savgol_filter(median, window, 2)\n",
        "    \n",
        "    preprocessing = 'first_derivative' if d['is_derivative'] else None\n",
        "    optimizer = OptimizedComponentFitter(max_components=12, preprocessing=preprocessing)\n",
        "    result = optimizer.optimize(target, wl, priority_categories=d['categories'])\n",
        "    d['optimized_component_result'] = result\n",
        "    \n",
        "    status = \"\u2713\" if result['r_squared'] > 0.7 else \"\u25b3\" if result['r_squared'] > 0.3 else \"\u2717\"\n",
        "    print(f\"{status} {d['name'][:40]:<40} R\u00b2={result['r_squared']:.4f}  Components: {result['components'][:4]}\")\n",
        "\n",
        "print(\"=\" * 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Combined Fitting (Components + Bands)\n",
        "\n",
        "Combine component fitting with band refinement for residuals.\n",
        "This creates 4 combinations:\n",
        "1. **Components + Pure Bands** - Constrained components, then pure band residual fitting\n",
        "2. **Optimized + Pure Bands** - Optimized components, then pure band residual fitting\n",
        "3. **Components + Real Bands** - Constrained components, then real band residual fitting\n",
        "4. **Optimized + Real Bands** - Optimized components, then real band residual fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_residual_with_bands(residual, wl, fitter_class, **kwargs):\n",
        "    \"\"\"Fit residual spectrum with a band fitter.\"\"\"\n",
        "    fitter = fitter_class(**kwargs)\n",
        "    return fitter.fit(residual, wl)\n",
        "\n",
        "def combine_fits(component_result, band_result, target):\n",
        "    \"\"\"Combine component and band fits.\"\"\"\n",
        "    if component_result is None or band_result is None:\n",
        "        return None\n",
        "    \n",
        "    comp_fitted = component_result.get('fitted', np.zeros_like(target))\n",
        "    band_fitted = band_result.get('fitted', np.zeros_like(target))\n",
        "    combined = comp_fitted + band_fitted\n",
        "    \n",
        "    # Recalculate R\u00b2\n",
        "    ss_res = np.sum((target - combined) ** 2)\n",
        "    ss_tot = np.sum((target - np.mean(target)) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot if ss_tot > 1e-10 else 0.0\n",
        "    \n",
        "    return {\n",
        "        'combined_fitted': combined,\n",
        "        'component_fitted': comp_fitted,\n",
        "        'band_fitted': band_fitted,\n",
        "        'r_squared': r2,\n",
        "        'component_r2': component_result.get('r_squared', 0),\n",
        "        'band_r2': band_result.get('r_squared', 0),\n",
        "    }\n",
        "\n",
        "# Perform all 4 combined fittings\n",
        "print(\"\\nCOMBINED FITTING (4 Approaches)\")\n",
        "print(\"=\" * 140)\n",
        "\n",
        "for d in datasets:\n",
        "    wl, median = d['wl'], d['median']\n",
        "    window = min(21, len(wl) // 10 * 2 + 1) | 1\n",
        "    target = scipy_signal.savgol_filter(median, window, 2)\n",
        "    \n",
        "    # Get component residuals\n",
        "    comp_result = d.get('component_result')\n",
        "    optim_result = d.get('optimized_component_result')\n",
        "    \n",
        "    comp_residual = comp_result['residual'] if comp_result else target\n",
        "    optim_residual = optim_result['residual'] if optim_result else target\n",
        "    \n",
        "    # 1. Components + Pure Bands\n",
        "    pure_fitter = PureBandFitter(max_bands=30, target_r2=0.95)\n",
        "    comp_pure_band = pure_fitter.fit(comp_residual, wl)\n",
        "    d['comp_pure_combined'] = combine_fits(comp_result, comp_pure_band, target)\n",
        "    \n",
        "    # 2. Optimized + Pure Bands\n",
        "    optim_pure_band = pure_fitter.fit(optim_residual, wl)\n",
        "    d['optim_pure_combined'] = combine_fits(optim_result, optim_pure_band, target)\n",
        "    \n",
        "    # 3. Components + Real Bands\n",
        "    real_fitter = RealBandFitter(target_r2=0.95)\n",
        "    comp_real_band = real_fitter.fit(comp_residual, wl)\n",
        "    d['comp_real_combined'] = combine_fits(comp_result, comp_real_band, target)\n",
        "    \n",
        "    # 4. Optimized + Real Bands\n",
        "    optim_real_band = real_fitter.fit(optim_residual, wl)\n",
        "    d['optim_real_combined'] = combine_fits(optim_result, optim_real_band, target)\n",
        "    \n",
        "    print(f\"{d['name'][:35]:<35}\", end=\" | \")\n",
        "    print(f\"Comp+Pure: {d['comp_pure_combined']['r_squared']:.3f}\", end=\" | \")\n",
        "    print(f\"Optim+Pure: {d['optim_pure_combined']['r_squared']:.3f}\", end=\" | \")\n",
        "    print(f\"Comp+Real: {d['comp_real_combined']['r_squared']:.3f}\", end=\" | \")\n",
        "    print(f\"Optim+Real: {d['optim_real_combined']['r_squared']:.3f}\")\n",
        "\n",
        "print(\"=\" * 140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Display All Fitting Results\n",
        "\n",
        "Compare all 8 fitting approaches:\n",
        "1. Pure Bands\n",
        "2. Real Bands\n",
        "3. Components (Constrained)\n",
        "4. Components (Optimized)\n",
        "5. Components + Pure Bands\n",
        "6. Optimized + Pure Bands\n",
        "7. Components + Real Bands\n",
        "8. Optimized + Real Bands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all R\u00b2 values\n",
        "all_approaches = [\n",
        "    ('pure_bands', 'Pure Bands'),\n",
        "    ('real_bands', 'Real Bands'),\n",
        "    ('components', 'Components'),\n",
        "    ('optimized', 'Optimized'),\n",
        "    ('comp_pure', 'Comp+Pure'),\n",
        "    ('optim_pure', 'Optim+Pure'),\n",
        "    ('comp_real', 'Comp+Real'),\n",
        "    ('optim_real', 'Optim+Real'),\n",
        "]\n",
        "\n",
        "results_matrix = []\n",
        "for d in datasets:\n",
        "    row = {'dataset': d['name'][:30]}\n",
        "    row['pure_bands'] = d['pure_band_result']['r_squared']\n",
        "    row['real_bands'] = d['real_band_result']['r_squared']\n",
        "    row['components'] = d['component_result']['r_squared']\n",
        "    row['optimized'] = d['optimized_component_result']['r_squared']\n",
        "    row['comp_pure'] = d['comp_pure_combined']['r_squared']\n",
        "    row['optim_pure'] = d['optim_pure_combined']['r_squared']\n",
        "    row['comp_real'] = d['comp_real_combined']['r_squared']\n",
        "    row['optim_real'] = d['optim_real_combined']['r_squared']\n",
        "    results_matrix.append(row)\n",
        "\n",
        "# Display as table\n",
        "print(\"\\nR\u00b2 COMPARISON (All 8 Approaches)\")\n",
        "print(\"=\" * 140)\n",
        "header = f\"{'Dataset':<30}\"\n",
        "for key, label in all_approaches:\n",
        "    header += f\" {label:>10}\"\n",
        "print(header)\n",
        "print(\"-\" * 140)\n",
        "\n",
        "for row in results_matrix:\n",
        "    line = f\"{row['dataset']:<30}\"\n",
        "    for key, _ in all_approaches:\n",
        "        r2 = row[key]\n",
        "        line += f\" {r2:>10.4f}\"\n",
        "    print(line)\n",
        "\n",
        "# Summary stats\n",
        "print(\"-\" * 140)\n",
        "means_line = f\"{'MEAN':<30}\"\n",
        "for key, _ in all_approaches:\n",
        "    mean_r2 = np.mean([r[key] for r in results_matrix])\n",
        "    means_line += f\" {mean_r2:>10.4f}\"\n",
        "print(means_line)\n",
        "print(\"=\" * 140)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: R\u00b2 heatmap\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Build matrix\n",
        "n_datasets = len(results_matrix)\n",
        "n_approaches = len(all_approaches)\n",
        "r2_matrix = np.zeros((n_datasets, n_approaches))\n",
        "\n",
        "for i, row in enumerate(results_matrix):\n",
        "    for j, (key, _) in enumerate(all_approaches):\n",
        "        r2_matrix[i, j] = row[key]\n",
        "\n",
        "# Plot heatmap\n",
        "im = ax.imshow(r2_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "# Labels\n",
        "ax.set_xticks(range(n_approaches))\n",
        "ax.set_xticklabels([label for _, label in all_approaches], rotation=45, ha='right')\n",
        "ax.set_yticks(range(n_datasets))\n",
        "ax.set_yticklabels([row['dataset'] for row in results_matrix])\n",
        "\n",
        "# Annotate\n",
        "for i in range(n_datasets):\n",
        "    for j in range(n_approaches):\n",
        "        val = r2_matrix[i, j]\n",
        "        color = 'white' if val < 0.5 else 'black'\n",
        "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "plt.colorbar(im, ax=ax, label='R\u00b2')\n",
        "ax.set_title('Fitting Quality: R\u00b2 by Approach', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Bar chart of mean R\u00b2 per approach\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "means = [np.mean([r[key] for r in results_matrix]) for key, _ in all_approaches]\n",
        "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(means)))\n",
        "bars = ax.bar(range(len(means)), means, color=colors)\n",
        "ax.set_xticks(range(len(means)))\n",
        "ax.set_xticklabels([label for _, label in all_approaches], rotation=45, ha='right')\n",
        "ax.set_ylabel('Mean R\u00b2')\n",
        "ax.set_title('Mean Fitting Quality by Approach')\n",
        "ax.set_ylim(0, 1)\n",
        "for bar, mean in zip(bars, means):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{mean:.3f}', \n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Noise Evaluation\n",
        "\n",
        "Evaluate the randomness/variance in real datasets to understand what synthetic \n",
        "generators need to reproduce. We analyze:\n",
        "- Sample-to-sample variation\n",
        "- High-frequency noise levels\n",
        "- Baseline drift patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_noise_characteristics(X_real, wl):\n",
        "    \"\"\"\n",
        "    Analyze noise characteristics of real dataset.\n",
        "    \"\"\"\n",
        "    n_samples, n_wl = X_real.shape\n",
        "    \n",
        "    # High-frequency noise (from 2nd derivative variance)\n",
        "    window = min(11, n_wl // 20 * 2 + 1) | 1\n",
        "    deriv2 = scipy_signal.savgol_filter(X_real, window, 3, deriv=2, axis=1)\n",
        "    hf_noise = np.median(np.std(deriv2, axis=1))\n",
        "    \n",
        "    # Sample-to-sample variation\n",
        "    mean_spectrum = X_real.mean(axis=0)\n",
        "    sample_variations = np.std(X_real - mean_spectrum, axis=1)\n",
        "    \n",
        "    # Baseline variation (low-frequency)\n",
        "    wl_norm = (wl - wl.mean()) / (wl.max() - wl.min())\n",
        "    baseline_coeffs = []\n",
        "    for i in range(n_samples):\n",
        "        coeffs = np.polyfit(wl_norm, X_real[i], 3)\n",
        "        baseline_coeffs.append(coeffs)\n",
        "    baseline_coeffs = np.array(baseline_coeffs)\n",
        "    \n",
        "    # Scale variation\n",
        "    sample_maxes = X_real.max(axis=1)\n",
        "    sample_mins = X_real.min(axis=1)\n",
        "    \n",
        "    return {\n",
        "        'hf_noise_level': hf_noise,\n",
        "        'sample_variation_mean': np.mean(sample_variations),\n",
        "        'sample_variation_std': np.std(sample_variations),\n",
        "        'baseline_offset_std': np.std(baseline_coeffs[:, -1]),  # constant term\n",
        "        'baseline_slope_std': np.std(baseline_coeffs[:, -2]),   # linear term\n",
        "        'scale_variation': np.std(sample_maxes - sample_mins),\n",
        "        'max_range': (np.min(sample_mins), np.max(sample_maxes)),\n",
        "    }\n",
        "\n",
        "# Evaluate noise for all datasets\n",
        "print(\"\\nNOISE CHARACTERISTICS\")\n",
        "print(\"=\" * 120)\n",
        "print(f\"{'Dataset':<35} {'HF Noise':>10} {'Sample Var':>12} {'Baseline \u03c3':>12} {'Scale Var':>10}\")\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for d in datasets:\n",
        "    noise = evaluate_noise_characteristics(d['X'], d['wl'])\n",
        "    d['noise_characteristics'] = noise\n",
        "    \n",
        "    print(f\"{d['name'][:35]:<35} {noise['hf_noise_level']:>10.6f} \"\n",
        "          f\"{noise['sample_variation_mean']:>12.6f} {noise['baseline_offset_std']:>12.6f} \"\n",
        "          f\"{noise['scale_variation']:>10.4f}\")\n",
        "\n",
        "print(\"=\" * 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Variance Fitting\n",
        "\n",
        "Fit variance parameters using two approaches:\n",
        "1. **Operator-based** - Fit noise, scatter, baseline operators\n",
        "2. **PCA-based** - Capture variance structure via principal components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VarianceFitter:\n",
        "    \"\"\"\n",
        "    Fit variance parameters using operator-based and PCA-based approaches.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def fit_operator_variance(X_real, wl):\n",
        "        n_samples, n_wl = X_real.shape\n",
        "        \n",
        "        # High-frequency noise\n",
        "        window = min(11, n_wl // 20 * 2 + 1) | 1\n",
        "        deriv2 = scipy_signal.savgol_filter(X_real, window, 3, deriv=2, axis=1)\n",
        "        noise_level = np.median(np.std(deriv2, axis=1))\n",
        "        \n",
        "        # Baseline variation\n",
        "        wl_norm = (wl - wl.mean()) / (wl.max() - wl.min())\n",
        "        baseline_coeffs = []\n",
        "        for i in range(n_samples):\n",
        "            coeffs = np.polyfit(wl_norm, X_real[i], 3)\n",
        "            baseline_coeffs.append(coeffs)\n",
        "        baseline_coeffs = np.array(baseline_coeffs)\n",
        "        \n",
        "        # Scattering\n",
        "        sample_scales = X_real.max(axis=1) / (X_real.mean(axis=1) + 1e-10)\n",
        "        mult_scatter_std = np.std(sample_scales)\n",
        "        \n",
        "        # Offset and slope\n",
        "        offset_std = np.std(X_real.mean(axis=1))\n",
        "        slopes = np.array([np.polyfit(wl_norm, X_real[i], 1)[0] for i in range(n_samples)])\n",
        "        slope_std = np.std(slopes)\n",
        "        \n",
        "        return {\n",
        "            'noise_std': noise_level,\n",
        "            'offset_std': offset_std,\n",
        "            'slope_std': slope_std,\n",
        "            'curvature_std': np.std(baseline_coeffs[:, 0]),\n",
        "            'mult_scatter_std': mult_scatter_std,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def fit_pca_variance(X_real, n_components=15):\n",
        "        n_samples = X_real.shape[0]\n",
        "        n_components = min(n_components, n_samples - 1, X_real.shape[1])\n",
        "        \n",
        "        pca = PCA(n_components=n_components)\n",
        "        scores = pca.fit_transform(X_real)\n",
        "        \n",
        "        score_means = scores.mean(axis=0)\n",
        "        score_stds = scores.std(axis=0)\n",
        "        \n",
        "        return {\n",
        "            'pca_model': pca,\n",
        "            'loadings': pca.components_,\n",
        "            'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "            'score_means': score_means,\n",
        "            'score_stds': score_stds,\n",
        "            'mean_spectrum': X_real.mean(axis=0),\n",
        "            'n_components': n_components,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_with_operator_variance(base_spectra, wl, var_params, n_samples, random_state=None):\n",
        "        rng = np.random.default_rng(random_state)\n",
        "        \n",
        "        if base_spectra.ndim == 1:\n",
        "            X = np.tile(base_spectra, (n_samples, 1))\n",
        "        else:\n",
        "            idx = rng.choice(len(base_spectra), n_samples, replace=True)\n",
        "            X = base_spectra[idx].copy()\n",
        "        \n",
        "        wl_norm = (wl - wl.mean()) / (wl.max() - wl.min())\n",
        "        \n",
        "        for i in range(n_samples):\n",
        "            X[i] += rng.normal(0, var_params['offset_std'])\n",
        "            X[i] += rng.normal(0, var_params['slope_std']) * wl_norm\n",
        "            X[i] += rng.normal(0, var_params['curvature_std']) * wl_norm**2\n",
        "            X[i] = X[i] * (1 + rng.normal(0, var_params['mult_scatter_std']))\n",
        "            X[i] += rng.normal(0, var_params['noise_std'], len(wl))\n",
        "        \n",
        "        return X\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_with_pca_variance(pca_params, n_samples, random_state=None):\n",
        "        rng = np.random.default_rng(random_state)\n",
        "        \n",
        "        pca = pca_params['pca_model']\n",
        "        n_components = pca_params['n_components']\n",
        "        score_stds = pca_params['score_stds']\n",
        "        \n",
        "        scores = np.zeros((n_samples, n_components))\n",
        "        for i in range(n_components):\n",
        "            scores[:, i] = rng.normal(0, score_stds[i], n_samples)\n",
        "        \n",
        "        X = pca.inverse_transform(scores)\n",
        "        return X\n",
        "\n",
        "# Fit variance for all datasets\n",
        "print(\"\\nVARIANCE FITTING\")\n",
        "print(\"=\" * 120)\n",
        "\n",
        "for d in datasets:\n",
        "    X_real, wl = d['X'], d['wl']\n",
        "    \n",
        "    operator_var = VarianceFitter.fit_operator_variance(X_real, wl)\n",
        "    d['operator_variance'] = operator_var\n",
        "    \n",
        "    pca_var = VarianceFitter.fit_pca_variance(X_real, n_components=15)\n",
        "    d['pca_variance'] = pca_var\n",
        "    \n",
        "    print(f\"{d['name'][:40]:<40}\")\n",
        "    print(f\"  Operator: noise={operator_var['noise_std']:.6f}, offset={operator_var['offset_std']:.4f}, \"\n",
        "          f\"scatter={operator_var['mult_scatter_std']:.4f}\")\n",
        "    print(f\"  PCA: {pca_var['n_components']} PCs, explained={sum(pca_var['explained_variance_ratio'][:3]):.3f} (top 3)\")\n",
        "\n",
        "print(\"=\" * 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 11. Synthetic Data Generation with Variance\n",
        "\n",
        "Generate synthetic data for all 8 fitting approaches, using both variance methods:\n",
        "- Operator-based variance (noise, scatter, baseline)\n",
        "- PCA-based variance (score distributions)\n",
        "\n",
        "This creates 16 synthetic variants per dataset (8 approaches \u00d7 2 variance methods)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_fitted_spectrum(d, approach):\n",
        "    \"\"\"Get the best fitted spectrum for a given approach.\"\"\"\n",
        "    if approach == 'pure_bands':\n",
        "        return d['pure_band_result']['fitted']\n",
        "    elif approach == 'real_bands':\n",
        "        return d['real_band_result']['fitted']\n",
        "    elif approach == 'components':\n",
        "        return d['component_result']['fitted']\n",
        "    elif approach == 'optimized':\n",
        "        return d['optimized_component_result']['fitted']\n",
        "    elif approach == 'comp_pure':\n",
        "        return d['comp_pure_combined']['combined_fitted']\n",
        "    elif approach == 'optim_pure':\n",
        "        return d['optim_pure_combined']['combined_fitted']\n",
        "    elif approach == 'comp_real':\n",
        "        return d['comp_real_combined']['combined_fitted']\n",
        "    elif approach == 'optim_real':\n",
        "        return d['optim_real_combined']['combined_fitted']\n",
        "    return None\n",
        "\n",
        "# Generate synthetic data for all approaches and variance methods\n",
        "print(\"\\nGENERATING SYNTHETIC DATA (8 Approaches \u00d7 2 Variance Methods)\")\n",
        "print(\"=\" * 120)\n",
        "\n",
        "approach_keys = ['pure_bands', 'real_bands', 'components', 'optimized',\n",
        "                 'comp_pure', 'optim_pure', 'comp_real', 'optim_real']\n",
        "\n",
        "for d in datasets:\n",
        "    wl = d['wl']\n",
        "    n_synth = min(d['X'].shape[0], 500)\n",
        "    \n",
        "    d['synthetic'] = {}\n",
        "    \n",
        "    for approach in approach_keys:\n",
        "        base_spectrum = get_best_fitted_spectrum(d, approach)\n",
        "        if base_spectrum is None:\n",
        "            continue\n",
        "        \n",
        "        # Operator-based variance\n",
        "        X_op = VarianceFitter.generate_with_operator_variance(\n",
        "            base_spectrum, wl, d['operator_variance'], n_synth, random_state=42\n",
        "        )\n",
        "        d['synthetic'][f'{approach}_op'] = X_op\n",
        "        \n",
        "        # PCA-based variance\n",
        "        X_pca = VarianceFitter.generate_with_pca_variance(\n",
        "            d['pca_variance'], n_synth, random_state=42\n",
        "        )\n",
        "        d['synthetic'][f'{approach}_pca'] = X_pca\n",
        "    \n",
        "    print(f\"{d['name'][:40]:<40} Generated {len(d['synthetic'])} variants ({n_synth} samples each)\")\n",
        "\n",
        "print(\"=\" * 120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate synthetic data quality\n",
        "def evaluate_synthetic_quality(X_real, X_synth):\n",
        "    \"\"\"Compare real and synthetic distributions.\"\"\"\n",
        "    if X_synth is None or len(X_synth) == 0:\n",
        "        return {'ks_stat': np.nan, 'wasserstein': np.nan}\n",
        "    \n",
        "    # Use first PC for distribution comparison\n",
        "    n_comp = min(5, X_real.shape[0] - 1, X_synth.shape[0] - 1)\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    \n",
        "    try:\n",
        "        scores_real = pca.fit_transform(X_real)\n",
        "        scores_synth = pca.transform(X_synth)\n",
        "        \n",
        "        # KS test on first PC\n",
        "        ks_stat, ks_pval = ks_2samp(scores_real[:, 0], scores_synth[:, 0])\n",
        "        \n",
        "        # Wasserstein distance\n",
        "        wd = wasserstein_distance(scores_real[:, 0], scores_synth[:, 0])\n",
        "        \n",
        "        # Mean spectrum difference\n",
        "        mean_diff = np.mean(np.abs(X_real.mean(axis=0) - X_synth.mean(axis=0)))\n",
        "        \n",
        "        return {\n",
        "            'ks_stat': ks_stat,\n",
        "            'ks_pval': ks_pval,\n",
        "            'wasserstein': wd,\n",
        "            'mean_diff': mean_diff,\n",
        "        }\n",
        "    except:\n",
        "        return {'ks_stat': np.nan, 'wasserstein': np.nan, 'mean_diff': np.nan}\n",
        "\n",
        "# Evaluate quality for a subset of approaches\n",
        "print(\"\\nSYNTHETIC QUALITY EVALUATION\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Dataset':<30} {'Approach':<15} {'Variance':<8} {'KS Stat':>10} {'Wasserstein':>12}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for d in datasets[:3]:  # Show first 3 datasets\n",
        "    for approach in ['optim_pure', 'optim_real']:\n",
        "        for var_method in ['op', 'pca']:\n",
        "            key = f'{approach}_{var_method}'\n",
        "            if key in d['synthetic']:\n",
        "                quality = evaluate_synthetic_quality(d['X'], d['synthetic'][key])\n",
        "                d[f'quality_{key}'] = quality\n",
        "                print(f\"{d['name'][:30]:<30} {approach:<15} {var_method:<8} \"\n",
        "                      f\"{quality['ks_stat']:>10.4f} {quality['wasserstein']:>12.4f}\")\n",
        "\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 12. Discriminator Training\n",
        "\n",
        "Train a classifier to distinguish real from synthetic data.\n",
        "**Lower accuracy = better synthetic data** (harder to tell apart).\n",
        "\n",
        "We test all 16 variants (8 approaches \u00d7 2 variance methods)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_discriminator(X_real, X_synth, n_estimators=100, n_iterations=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Train classifier to distinguish real from synthetic.\n",
        "    Returns mean accuracy (lower = better synthetic quality).\n",
        "    \"\"\"\n",
        "    if X_synth is None or len(X_synth) == 0:\n",
        "        return np.nan, np.nan\n",
        "    \n",
        "    n = min(len(X_real), len(X_synth))\n",
        "    if n < 10:\n",
        "        return np.nan, np.nan\n",
        "    \n",
        "    rng = np.random.default_rng(random_state)\n",
        "    accuracies = []\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        idx_r = rng.choice(len(X_real), n, replace=False)\n",
        "        idx_s = rng.choice(len(X_synth), n, replace=False)\n",
        "        \n",
        "        X = np.vstack([X_real[idx_r], X_synth[idx_s]])\n",
        "        y = np.array([0] * n + [1] * n)\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=random_state + i\n",
        "        )\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        clf = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=10,\n",
        "            random_state=random_state + i,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        clf.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        y_pred = clf.predict(X_test_scaled)\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "    \n",
        "    return np.mean(accuracies), np.std(accuracies)\n",
        "\n",
        "# Run discriminator on all variants\n",
        "print(\"\\nDISCRIMINATOR TEST (All Variants)\")\n",
        "print(\"=\" * 140)\n",
        "print(\"Target: Accuracy close to 0.50 means synthetic is indistinguishable from real\")\n",
        "print(\"-\" * 140)\n",
        "\n",
        "discriminator_results = []\n",
        "\n",
        "for d in datasets:\n",
        "    dataset_results = {'name': d['name'][:30]}\n",
        "    \n",
        "    for approach in approach_keys:\n",
        "        for var_method in ['op', 'pca']:\n",
        "            key = f'{approach}_{var_method}'\n",
        "            if key in d['synthetic']:\n",
        "                acc_mean, acc_std = test_discriminator(d['X'], d['synthetic'][key])\n",
        "                dataset_results[key] = acc_mean\n",
        "            else:\n",
        "                dataset_results[key] = np.nan\n",
        "    \n",
        "    discriminator_results.append(dataset_results)\n",
        "    \n",
        "    # Print best for this dataset\n",
        "    valid_results = {k: v for k, v in dataset_results.items() if k != 'name' and not np.isnan(v)}\n",
        "    if valid_results:\n",
        "        best_key = min(valid_results, key=valid_results.get)\n",
        "        best_acc = valid_results[best_key]\n",
        "        print(f\"{d['name'][:40]:<40} Best: {best_key:<20} Acc={best_acc:.3f}\")\n",
        "\n",
        "print(\"=\" * 140)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discriminator summary table\n",
        "print(\"\\nDISCRIMINATOR ACCURACY SUMMARY (Lower = Better)\")\n",
        "print(\"=\" * 160)\n",
        "\n",
        "# Header\n",
        "header = f\"{'Dataset':<30}\"\n",
        "for approach in ['optim_pure', 'optim_real', 'comp_pure', 'comp_real']:\n",
        "    header += f\" {approach+'_op':>14} {approach+'_pca':>14}\"\n",
        "print(header)\n",
        "print(\"-\" * 160)\n",
        "\n",
        "for result in discriminator_results:\n",
        "    line = f\"{result['name']:<30}\"\n",
        "    for approach in ['optim_pure', 'optim_real', 'comp_pure', 'comp_real']:\n",
        "        for var in ['op', 'pca']:\n",
        "            key = f'{approach}_{var}'\n",
        "            val = result.get(key, np.nan)\n",
        "            if np.isnan(val):\n",
        "                line += f\" {'N/A':>14}\"\n",
        "            else:\n",
        "                # Color code: lower is better\n",
        "                marker = \"\u2713\" if val < 0.6 else \"\u25b3\" if val < 0.75 else \"\u2717\"\n",
        "                line += f\" {marker}{val:>13.3f}\"\n",
        "    print(line)\n",
        "\n",
        "# Compute means\n",
        "print(\"-\" * 160)\n",
        "means_line = f\"{'MEAN':<30}\"\n",
        "for approach in ['optim_pure', 'optim_real', 'comp_pure', 'comp_real']:\n",
        "    for var in ['op', 'pca']:\n",
        "        key = f'{approach}_{var}'\n",
        "        vals = [r.get(key, np.nan) for r in discriminator_results]\n",
        "        vals = [v for v in vals if not np.isnan(v)]\n",
        "        mean_val = np.mean(vals) if vals else np.nan\n",
        "        if np.isnan(mean_val):\n",
        "            means_line += f\" {'N/A':>14}\"\n",
        "        else:\n",
        "            means_line += f\" {mean_val:>14.3f}\"\n",
        "print(means_line)\n",
        "print(\"=\" * 160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discriminator visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Heatmap of discriminator accuracy\n",
        "ax = axes[0]\n",
        "selected_approaches = ['pure_bands', 'real_bands', 'components', 'optimized', \n",
        "                       'comp_pure', 'optim_pure', 'comp_real', 'optim_real']\n",
        "n_approaches = len(selected_approaches)\n",
        "n_datasets = len(discriminator_results)\n",
        "\n",
        "# Build matrix for operator variance\n",
        "acc_matrix = np.zeros((n_datasets, n_approaches))\n",
        "for i, result in enumerate(discriminator_results):\n",
        "    for j, approach in enumerate(selected_approaches):\n",
        "        key = f'{approach}_op'\n",
        "        acc_matrix[i, j] = result.get(key, np.nan)\n",
        "\n",
        "# Replace NaN for visualization\n",
        "acc_matrix_display = np.nan_to_num(acc_matrix, nan=0.5)\n",
        "\n",
        "im = ax.imshow(acc_matrix_display, cmap='RdYlGn_r', aspect='auto', vmin=0.5, vmax=1.0)\n",
        "ax.set_xticks(range(n_approaches))\n",
        "ax.set_xticklabels(selected_approaches, rotation=45, ha='right', fontsize=8)\n",
        "ax.set_yticks(range(n_datasets))\n",
        "ax.set_yticklabels([r['name'] for r in discriminator_results], fontsize=8)\n",
        "ax.set_title('Discriminator Accuracy (Operator Variance)\\nLower = Better', fontsize=11)\n",
        "plt.colorbar(im, ax=ax, label='Accuracy')\n",
        "\n",
        "# Bar chart of mean accuracy per approach\n",
        "ax = axes[1]\n",
        "means_op = []\n",
        "means_pca = []\n",
        "for approach in selected_approaches:\n",
        "    vals_op = [r.get(f'{approach}_op', np.nan) for r in discriminator_results]\n",
        "    vals_pca = [r.get(f'{approach}_pca', np.nan) for r in discriminator_results]\n",
        "    means_op.append(np.nanmean(vals_op))\n",
        "    means_pca.append(np.nanmean(vals_pca))\n",
        "\n",
        "x = np.arange(len(selected_approaches))\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, means_op, width, label='Operator', color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, means_pca, width, label='PCA', color='coral')\n",
        "\n",
        "ax.set_ylabel('Mean Discriminator Accuracy')\n",
        "ax.set_title('Mean Accuracy by Approach & Variance Method')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(selected_approaches, rotation=45, ha='right', fontsize=8)\n",
        "ax.axhline(y=0.5, color='green', linestyle='--', label='Ideal (0.5)')\n",
        "ax.legend()\n",
        "ax.set_ylim(0.4, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 13. Global Summary\n",
        "\n",
        "Comprehensive summary of all fitting approaches with:\n",
        "- Fitting quality (R\u00b2)\n",
        "- Synthetic data quality (discriminator accuracy)\n",
        "- Component and band statistics\n",
        "- Recommendations per dataset type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build comprehensive summary\n",
        "print(\"=\" * 180)\n",
        "print(\"GLOBAL SUMMARY: SYNTHETIC GENERATOR EVALUATION\")\n",
        "print(\"=\" * 180)\n",
        "\n",
        "summary_table = []\n",
        "for i, d in enumerate(datasets):\n",
        "    row = {\n",
        "        'name': d['name'],\n",
        "        'n_samples': d['n_samples'],\n",
        "        'n_wavelengths': d['n_wavelengths'],\n",
        "        'type': d['preprocessing_type'],\n",
        "    }\n",
        "    \n",
        "    # Best fitting approach\n",
        "    r2_values = {\n",
        "        'pure_bands': d['pure_band_result']['r_squared'],\n",
        "        'real_bands': d['real_band_result']['r_squared'],\n",
        "        'components': d['component_result']['r_squared'],\n",
        "        'optimized': d['optimized_component_result']['r_squared'],\n",
        "        'comp_pure': d['comp_pure_combined']['r_squared'],\n",
        "        'optim_pure': d['optim_pure_combined']['r_squared'],\n",
        "        'comp_real': d['comp_real_combined']['r_squared'],\n",
        "        'optim_real': d['optim_real_combined']['r_squared'],\n",
        "    }\n",
        "    row['best_fitting'] = max(r2_values, key=r2_values.get)\n",
        "    row['best_r2'] = r2_values[row['best_fitting']]\n",
        "    \n",
        "    # Best discriminator (lowest accuracy)\n",
        "    disc_values = {}\n",
        "    for approach in approach_keys:\n",
        "        for var in ['op', 'pca']:\n",
        "            key = f'{approach}_{var}'\n",
        "            if key in d['synthetic']:\n",
        "                acc, _ = test_discriminator(d['X'], d['synthetic'][key], n_iterations=3)\n",
        "                disc_values[key] = acc\n",
        "    \n",
        "    if disc_values:\n",
        "        row['best_synth'] = min(disc_values, key=lambda k: disc_values.get(k, 1.0))\n",
        "        row['best_disc_acc'] = disc_values[row['best_synth']]\n",
        "    else:\n",
        "        row['best_synth'] = 'N/A'\n",
        "        row['best_disc_acc'] = np.nan\n",
        "    \n",
        "    # Components used\n",
        "    row['n_components'] = d['optimized_component_result']['n_components']\n",
        "    row['top_components'] = ', '.join(d['optimized_component_result']['components'][:3])\n",
        "    \n",
        "    summary_table.append(row)\n",
        "\n",
        "# Print summary\n",
        "print(f\"{'Dataset':<35} {'Samples':>8} {'Best Fit':>12} {'R\u00b2':>8} {'Best Synth':>20} {'Disc Acc':>10}\")\n",
        "print(\"-\" * 180)\n",
        "\n",
        "for row in summary_table:\n",
        "    print(f\"{row['name'][:35]:<35} {row['n_samples']:>8} {row['best_fitting']:>12} \"\n",
        "          f\"{row['best_r2']:>8.4f} {row['best_synth']:>20} {row['best_disc_acc']:>10.3f}\")\n",
        "\n",
        "print(\"=\" * 180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"\\nSUMMARY STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Fitting quality\n",
        "print(\"\\nFITTING QUALITY (Mean R\u00b2 per approach):\")\n",
        "for approach in approach_keys:\n",
        "    r2_key_map = {\n",
        "        'pure_bands': lambda d: d['pure_band_result']['r_squared'],\n",
        "        'real_bands': lambda d: d['real_band_result']['r_squared'],\n",
        "        'components': lambda d: d['component_result']['r_squared'],\n",
        "        'optimized': lambda d: d['optimized_component_result']['r_squared'],\n",
        "        'comp_pure': lambda d: d['comp_pure_combined']['r_squared'],\n",
        "        'optim_pure': lambda d: d['optim_pure_combined']['r_squared'],\n",
        "        'comp_real': lambda d: d['comp_real_combined']['r_squared'],\n",
        "        'optim_real': lambda d: d['optim_real_combined']['r_squared'],\n",
        "    }\n",
        "    mean_r2 = np.mean([r2_key_map[approach](d) for d in datasets])\n",
        "    print(f\"  {approach:<15}: {mean_r2:.4f}\")\n",
        "\n",
        "# Best approaches by dataset type\n",
        "print(\"\\nBEST APPROACH BY DATASET TYPE:\")\n",
        "type_counts = {}\n",
        "for row in summary_table:\n",
        "    dtype = row['type']\n",
        "    best = row['best_fitting']\n",
        "    if dtype not in type_counts:\n",
        "        type_counts[dtype] = {}\n",
        "    type_counts[dtype][best] = type_counts[dtype].get(best, 0) + 1\n",
        "\n",
        "for dtype, counts in type_counts.items():\n",
        "    best_approach = max(counts, key=counts.get)\n",
        "    print(f\"  {dtype:<20}: {best_approach}\")\n",
        "\n",
        "# Component usage summary\n",
        "print(\"\\nMOST COMMON COMPONENTS:\")\n",
        "all_components = []\n",
        "for d in datasets:\n",
        "    all_components.extend(d['optimized_component_result']['components'])\n",
        "\n",
        "from collections import Counter\n",
        "comp_counts = Counter(all_components)\n",
        "for comp, count in comp_counts.most_common(10):\n",
        "    print(f\"  {comp:<25}: {count} datasets\")\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final visualization: Summary dashboard\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# 1. R\u00b2 comparison across approaches\n",
        "ax1 = fig.add_subplot(2, 2, 1)\n",
        "approach_labels = ['Pure\\nBands', 'Real\\nBands', 'Comp', 'Optim', \n",
        "                   'Comp+\\nPure', 'Optim+\\nPure', 'Comp+\\nReal', 'Optim+\\nReal']\n",
        "r2_means = []\n",
        "r2_stds = []\n",
        "for approach in approach_keys:\n",
        "    r2_key_map = {\n",
        "        'pure_bands': lambda d: d['pure_band_result']['r_squared'],\n",
        "        'real_bands': lambda d: d['real_band_result']['r_squared'],\n",
        "        'components': lambda d: d['component_result']['r_squared'],\n",
        "        'optimized': lambda d: d['optimized_component_result']['r_squared'],\n",
        "        'comp_pure': lambda d: d['comp_pure_combined']['r_squared'],\n",
        "        'optim_pure': lambda d: d['optim_pure_combined']['r_squared'],\n",
        "        'comp_real': lambda d: d['comp_real_combined']['r_squared'],\n",
        "        'optim_real': lambda d: d['optim_real_combined']['r_squared'],\n",
        "    }\n",
        "    vals = [r2_key_map[approach](d) for d in datasets]\n",
        "    r2_means.append(np.mean(vals))\n",
        "    r2_stds.append(np.std(vals))\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(r2_means)))\n",
        "bars = ax1.bar(range(len(r2_means)), r2_means, yerr=r2_stds, color=colors, capsize=3)\n",
        "ax1.set_xticks(range(len(approach_labels)))\n",
        "ax1.set_xticklabels(approach_labels, fontsize=8)\n",
        "ax1.set_ylabel('Mean R\u00b2')\n",
        "ax1.set_title('Fitting Quality by Approach', fontsize=12)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='Good (0.9)')\n",
        "\n",
        "# 2. Discriminator accuracy comparison\n",
        "ax2 = fig.add_subplot(2, 2, 2)\n",
        "disc_means_op = []\n",
        "disc_means_pca = []\n",
        "for approach in approach_keys:\n",
        "    vals_op = [discriminator_results[i].get(f'{approach}_op', np.nan) for i in range(len(datasets))]\n",
        "    vals_pca = [discriminator_results[i].get(f'{approach}_pca', np.nan) for i in range(len(datasets))]\n",
        "    disc_means_op.append(np.nanmean(vals_op))\n",
        "    disc_means_pca.append(np.nanmean(vals_pca))\n",
        "\n",
        "x = np.arange(len(approach_keys))\n",
        "width = 0.35\n",
        "ax2.bar(x - width/2, disc_means_op, width, label='Operator Var', color='steelblue')\n",
        "ax2.bar(x + width/2, disc_means_pca, width, label='PCA Var', color='coral')\n",
        "ax2.axhline(y=0.5, color='green', linestyle='--', label='Ideal (0.5)')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(approach_labels, fontsize=8)\n",
        "ax2.set_ylabel('Mean Discriminator Accuracy')\n",
        "ax2.set_title('Synthetic Quality (Lower = Better)', fontsize=12)\n",
        "ax2.legend(fontsize=8)\n",
        "ax2.set_ylim(0.4, 1.0)\n",
        "\n",
        "# 3. Per-dataset best approach\n",
        "ax3 = fig.add_subplot(2, 2, 3)\n",
        "best_approaches = [row['best_fitting'] for row in summary_table]\n",
        "approach_counts = Counter(best_approaches)\n",
        "labels = list(approach_counts.keys())\n",
        "sizes = list(approach_counts.values())\n",
        "ax3.pie(sizes, labels=labels, autopct='%1.0f%%', startangle=90)\n",
        "ax3.set_title('Best Fitting Approach Distribution', fontsize=12)\n",
        "\n",
        "# 4. Component usage heatmap\n",
        "ax4 = fig.add_subplot(2, 2, 4)\n",
        "top_comps = [c for c, _ in comp_counts.most_common(8)]\n",
        "comp_matrix = np.zeros((len(datasets), len(top_comps)))\n",
        "for i, d in enumerate(datasets):\n",
        "    for j, comp in enumerate(top_comps):\n",
        "        if comp in d['optimized_component_result']['components']:\n",
        "            comp_matrix[i, j] = 1\n",
        "\n",
        "im = ax4.imshow(comp_matrix, cmap='YlGn', aspect='auto')\n",
        "ax4.set_xticks(range(len(top_comps)))\n",
        "ax4.set_xticklabels(top_comps, rotation=45, ha='right', fontsize=8)\n",
        "ax4.set_yticks(range(len(datasets)))\n",
        "ax4.set_yticklabels([d['name'][:20] for d in datasets], fontsize=8)\n",
        "ax4.set_title('Component Usage Matrix', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Synthetic NIRS Generator - Final Evaluation Summary', y=1.02, fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 14. Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Fitting Quality**: Combined approaches (Components + Bands) generally achieve the best R\u00b2 values,\n",
        "   with `optim_pure` and `optim_real` being the top performers.\n",
        "\n",
        "2. **Synthetic Data Quality**: Operator-based variance tends to produce more realistic synthetic data\n",
        "   (lower discriminator accuracy) compared to PCA-based variance for most datasets.\n",
        "\n",
        "3. **Component Selection**: The optimized greedy search with swap refinement outperforms \n",
        "   simple category-based selection, finding better component combinations.\n",
        "\n",
        "4. **Band Fitting**: Pure bands (unconstrained) achieve higher R\u00b2 than real bands (constrained),\n",
        "   but real bands provide more interpretable spectral decomposition.\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "- For **maximum fitting quality**: Use `optim_pure` (optimized components + pure bands)\n",
        "- For **interpretability**: Use `optim_real` (optimized components + real bands)  \n",
        "- For **variance generation**: Prefer operator-based variance for most applications\n",
        "- For **derivative spectra**: Enable preprocessing in component fitting\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Integrate best approaches into `nirs4all.generate()` API\n",
        "2. Add automatic approach selection based on dataset characteristics\n",
        "3. Implement hybrid variance (operator + PCA) for improved realism"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}