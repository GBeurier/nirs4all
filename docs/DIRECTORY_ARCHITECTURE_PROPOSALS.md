# Directory Architecture Proposals for nirs4all

## Current Situation Analysis

**Problem Statement:**
The current system has multiple file storage patterns that are confusing for users:
- Content-addressed artifacts in `results/artifacts/objects/XX/hash.ext` (unreadable hashes)
- Pipeline configs in `results/pipelines/<UUID>/pipeline.json` + `manifest.yaml` (UUID is unreadable)
- Dataset configs in `results/datasets/<name>/config_name/` folders
- Human-readable outputs in `results/outputs/<dataset>_<pipeline>/`
- Prediction CSVs scattered in various locations
- Full UUID identifiers everywhere (c40ca32c-43d7-4e1e-80c4-7f52de42b7a5) making navigation impossible

**User Pain Points:**
1. âŒ UUIDs are completely unreadable - users can't find their work
2. âŒ Hash-based artifact names prevent browsing
3. âŒ Multiple metadata files (pipeline.json, metadata.json, manifest.yaml) create confusion
4. âŒ No clear hierarchy between datasets, pipelines, and results
5. âŒ Files scattered across many directories
6. âš ï¸ However, users DO appreciate knowing where their files are

**What Works:**
- âœ… Content-addressed storage prevents duplication (technical efficiency)
- âœ… Metadata tracking is comprehensive
- âœ… Recent outputs directory is user-friendly
- âœ… Users like having organized folders, not a flat structure

---

## Proposal 1: Project-Centric with Human-Readable IDs

### Philosophy
**"Projects are the unit of work"** - Users think in terms of experiments/projects, not individual pipeline runs. Use shortened readable IDs (6-8 chars) instead of full UUIDs. **All artifacts stay WITH the run for easy sharing.**

### Complete Directory Structure
```
results/
â”œâ”€â”€ projects/
â”‚   â”œâ”€â”€ wheat_quality_2024/                    # User-named project folder
â”‚   â”‚   â”œâ”€â”€ project.yaml                       # Project metadata, notes, objectives
â”‚   â”‚   â”œâ”€â”€ runs/
â”‚   â”‚   â”‚   â”œâ”€â”€ run_pb8yky/                    # PLS-17_components run [pb8yky]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ summary.yaml               # âœ¨ ALL metadata in ONE file
â”‚   â”‚   â”‚   â”‚   â”‚                              #    - Pipeline config
â”‚   â”‚   â”‚   â”‚   â”‚                              #    - Dataset info
â”‚   â”‚   â”‚   â”‚   â”‚                              #    - Scores (RMSE, RÂ², etc.)
â”‚   â”‚   â”‚   â”‚   â”‚                              #    - Timestamps
â”‚   â”‚   â”‚   â”‚   â”‚                              #    - Fold results
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.yaml              # ğŸ“‹ Standalone pipeline definition
â”‚   â”‚   â”‚   â”‚   â”‚                              #    (can copy to reuse elsewhere)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ predictions/               # ğŸ“Š All prediction files
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ best_prediction.csv    #    Main: y_true, y_pred, residuals
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fold_0_predictions.csv #    Individual fold results
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fold_1_predictions.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fold_2_predictions.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ train_predictions.csv  #    Training set predictions
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ charts/                    # ğŸ“ˆ All visualizations
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2D_Chart.png           #    Spectra visualization (2D)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 3D_Chart_src0.png      #    Spectra 3D (if applicable)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Y_distribution_train_test.png  # Y distribution
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fold_visualization_3folds_train.png  # CV folds
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fold_visualization_3folds_test.png
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ predictions_scatter.png         # Pred vs True
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ residuals_plot.png              # Residuals analysis
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ feature_importance.png          # If model supports it
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scores/                    # ğŸ“Š Detailed scoring files
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cross_validation_scores.csv     # All folds scores
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ train_scores.yaml      #    Train metrics
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ test_scores.yaml       #    Test metrics
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ summary_table.txt      #    Pretty table (console output)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ models/                    # ğŸ”§ All artifacts (SELF-CONTAINED)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pls_17comp_pb8yky.pkl  #    Trained model
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scaler_minmax_pb8yky.pkl     # Preprocessing artifacts
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ transformer_1stder_pb8yky.pkl
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ transformer_haar_pb8yky.pkl
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ MANIFEST.yaml          #    List of all artifacts with metadata
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ logs/                      # ğŸ“ Execution logs
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ console_output.log     #    Full console output
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ warnings.log           #    Warnings/errors
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ timing.yaml            #    Step execution times
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ run_5grd70/                    # PLS-15_components run [5grd70]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ summary.yaml               # âœ¨ Best model: RMSE 6.8466
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.yaml              # ğŸ“‹ Slightly different config
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ predictions/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ best_prediction.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ charts/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2D_Chart.png
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scores/
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ models/                    # ğŸ”§ Complete artifact set
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pls_15comp_5grd70.pkl
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scaler_minmax_5grd70.pkl
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ logs/
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ run_2e3nwn/                    # Classification run
â”‚   â”‚   â”‚       â”œâ”€â”€ summary.yaml               # Different metrics (accuracy, f1, etc.)
â”‚   â”‚   â”‚       â”œâ”€â”€ pipeline.yaml
â”‚   â”‚   â”‚       â”œâ”€â”€ predictions/
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ best_prediction.csv    # class labels, probabilities
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ confusion_matrix.csv
â”‚   â”‚   â”‚       â”œâ”€â”€ charts/
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ class_distribution.png
â”‚   â”‚   â”‚       â”œâ”€â”€ scores/
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ classification_report.txt
â”‚   â”‚   â”‚       â””â”€â”€ models/
â”‚   â”‚   â”‚           â””â”€â”€ random_forest_2e3nwn.pkl
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ datasets/                          # ğŸ“¦ Dataset configs
â”‚   â”‚   â”‚   â”œâ”€â”€ regression/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.yaml                # Dataset configuration
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ metadata.yaml              # Features, samples, statistics
â”‚   â”‚   â”‚   â””â”€â”€ classification_Xtrain/
â”‚   â”‚   â”‚       â””â”€â”€ config.yaml
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ comparisons/                       # ğŸ“Š Cross-run comparisons
â”‚   â”‚       â”œâ”€â”€ all_runs_comparison.csv        # Compare all runs in project
â”‚   â”‚       â””â”€â”€ best_models_report.html        # Visual comparison
â”‚   â”‚
â”‚   â”œâ”€â”€ protein_prediction/                    # Another project
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ default/                               # Auto-created if no project specified
â”‚       â””â”€â”€ runs/
â”‚           â””â”€â”€ run_xxxxxx/
â”‚               â””â”€â”€ ...
â”‚
â””â”€â”€ archive/                                   # Old runs (auto-archived after 90 days)
    â””â”€â”€ 2024-07_wheat_quality/
        â””â”€â”€ runs/
            â””â”€â”€ run_old123/
                â””â”€â”€ ... (complete structure preserved)
```

### Real Example - Complete Run Folder

**Example: `results/projects/wheat_quality_2024/runs/run_pb8yky/`**

This is the **winning model** from Q1 regression example:
- Model: PLS-17_components
- Test RMSE: 13.3989
- Val RMSE: 6.9658

```
run_pb8yky/
â”œâ”€â”€ summary.yaml                    # 4.2 KB - All metadata
â”‚   â”œâ”€â”€ run_id: "pb8yky"
â”‚   â”œâ”€â”€ pipeline_name: "Q1_c20f9b"
â”‚   â”œâ”€â”€ dataset: "regression"
â”‚   â”œâ”€â”€ model: "PLS-17_components"
â”‚   â”œâ”€â”€ created_at: "2024-10-14T00:22:31"
â”‚   â”œâ”€â”€ scores:
â”‚   â”‚   â”œâ”€â”€ test: {rmse: 13.3989, r2: 0.546, mae: 9.763}
â”‚   â”‚   â”œâ”€â”€ val: {rmse: 6.9658, r2: 0.878, mae: 5.389}
â”‚   â”‚   â””â”€â”€ train: {rmse: 4.904, r2: 0.957, mae: 3.129}
â”‚   â”œâ”€â”€ fold_results: [...]
â”‚   â””â”€â”€ processing_steps: [...]
â”‚
â”œâ”€â”€ pipeline.yaml                   # 2.1 KB - Portable pipeline config
â”‚   â”œâ”€â”€ steps:
â”‚   â”‚   â”œâ”€â”€ MinMax scaler
â”‚   â”‚   â”œâ”€â”€ 1st Derivative
â”‚   â”‚   â”œâ”€â”€ Haar wavelet
â”‚   â”‚   â”œâ”€â”€ MinMax scaler (again)
â”‚   â”‚   â””â”€â”€ PLS (n_components: 17)
â”‚   â””â”€â”€ ... (complete, can copy/paste to new project)
â”‚
â”œâ”€â”€ predictions/
â”‚   â”œâ”€â”€ best_prediction.csv         # 4.8 KB - Main results
â”‚   â”‚   # Columns: index, y_true, y_pred, residual, fold
â”‚   â”‚   # 59 rows (test set)
â”‚   â”‚
â”‚   â”œâ”€â”€ fold_0_predictions.csv      # Individual fold predictions
â”‚   â”œâ”€â”€ fold_1_predictions.csv
â”‚   â”œâ”€â”€ fold_2_predictions.csv
â”‚   â”œâ”€â”€ avg_predictions.csv         # Average across folds
â”‚   â”œâ”€â”€ w_avg_predictions.csv       # Weighted average
â”‚   â””â”€â”€ train_predictions.csv       # Training set (130 samples)
â”‚
â”œâ”€â”€ charts/
â”‚   â”œâ”€â”€ 2D_Chart.png                # 156 KB - All spectra overlaid
â”‚   â”œâ”€â”€ Y_distribution_train_test.png  # 89 KB - Y value distributions
â”‚   â”œâ”€â”€ fold_visualization_3folds_train.png  # 124 KB - CV visualization
â”‚   â”œâ”€â”€ predictions_scatter.png     # 142 KB - Predicted vs True
â”‚   â””â”€â”€ residuals_plot.png          # 98 KB - Residual analysis
â”‚
â”œâ”€â”€ scores/
â”‚   â”œâ”€â”€ cross_validation_scores.csv # All metrics for each fold
â”‚   â”‚   # Columns: fold, rmse, r2, mae, sep, rpd, bias, consistency
â”‚   â”‚
â”‚   â”œâ”€â”€ train_scores.yaml           # Detailed train metrics
â”‚   â”œâ”€â”€ test_scores.yaml            # Detailed test metrics
â”‚   â””â”€â”€ summary_table.txt           # Pretty ASCII table (console output)
â”‚       # The table you see in console with all metrics
â”‚
â”œâ”€â”€ models/                         # ğŸ¯ COMPLETE ARTIFACT SET
â”‚   â”œâ”€â”€ pls_model_pb8yky.pkl        # 2.4 MB - Trained PLS model
â”‚   â”œâ”€â”€ scaler_minmax_1_pb8yky.pkl  # 12 KB - First MinMax scaler
â”‚   â”œâ”€â”€ scaler_minmax_2_pb8yky.pkl  # 12 KB - Second MinMax scaler
â”‚   â”œâ”€â”€ transformer_1stder_pb8yky.pkl  # 8 KB - 1st Derivative transformer
â”‚   â”œâ”€â”€ transformer_haar_pb8yky.pkl    # 24 KB - Haar wavelet transformer
â”‚   â””â”€â”€ MANIFEST.yaml               # Index of all artifacts
â”‚       # Lists each artifact with:
â”‚       # - filename
â”‚       # - type (model/scaler/transformer)
â”‚       # - step_number
â”‚       # - sha256 hash (for verification)
â”‚       # - size
â”‚
â””â”€â”€ logs/
    â”œâ”€â”€ console_output.log          # Full console output (ANSI colors stripped)
    â”œâ”€â”€ warnings.log                # Any warnings during execution
    â””â”€â”€ timing.yaml                 # Execution time per step
```

### Sharing the Pipeline with a Friend ğŸ

**Problem Addressed:** How to share a working pipeline including all artifacts?

**Solution: Self-Contained Run Folder**

```bash
# Option 1: Share the entire run folder
zip -r wheat_model_pb8yky.zip results/projects/wheat_quality_2024/runs/run_pb8yky/
# Send to friend: wheat_model_pb8yky.zip (contains EVERYTHING)

# Friend extracts and can:
# 1. View summary.yaml to understand the model
# 2. Copy pipeline.yaml to create new runs
# 3. Use models/*.pkl for predictions
# 4. See all charts and scores
```

**What the friend gets:**
- âœ… Complete pipeline definition (`pipeline.yaml`)
- âœ… All trained models and transformers (`models/`)
- âœ… Performance metrics (`scores/`, `summary.yaml`)
- âœ… Visual validation (`charts/`)
- âœ… Example predictions (`predictions/`)
- âœ… **NO dependency on a shared artifact cache!**

**Using the shared pipeline:**

```python
# Friend loads your pipeline
from nirs4all import PipelineRunner

# Option 1: Use your exact pipeline config
runner = PipelineRunner()
predictions = runner.predict_from_pipeline(
    pipeline_path="run_pb8yky/pipeline.yaml",
    model_artifacts="run_pb8yky/models/",
    new_data="my_samples.csv"
)

# Option 2: Copy pipeline.yaml to their project and run
# results/projects/my_own_study/pipelines/wheat_method.yaml
runner.run(
    pipeline="wheat_method.yaml",  # Your pipeline
    dataset="my_dataset"           # Their data
)
```

### Naming Convention
- **Projects**: User-defined names (e.g., `wheat_quality_2024`)
- **Runs**: `run_<6-char-id>` (e.g., `run_pb8yky` from console output `[pb8yky]`)
- **Artifacts**: `<type>_<descriptor>_<id>.pkl` (e.g., `pls_17comp_pb8yky.pkl`)
- **Predictions**: Descriptive (e.g., `best_prediction.csv`, `fold_0_predictions.csv`)
- **Charts**: Descriptive (e.g., `2D_Chart.png`, `Y_distribution_train_test.png`)

### Rationale

**Pros:**
- âœ… **Complete isolation** - Each run is self-contained
- âœ… **Easy sharing** - Zip one folder, includes everything
- âœ… **No broken links** - All artifacts physically present
- âœ… **Project organization** - Related runs grouped together
- âœ… **Short memorable IDs** - `pb8yky` visible in console and folder name
- âœ… **Portable pipelines** - `pipeline.yaml` can be copied anywhere
- âœ… **Clear structure** - predictions/, charts/, scores/, models/ separate
- âœ… **One metadata file** - `summary.yaml` instead of 3 JSONs

**Cons:**
- âš ï¸ **Disk space** - Models duplicated across runs (but realistic for ML)
- âš ï¸ **Project naming** - Requires user to think about organization
- âš ï¸ **More files** - But organized into clear subdirectories

**Why No Shared Artifacts Cache?**

In Proposal 1, we **prioritize portability over deduplication**:

1. **ML Reality:** Trained models are unique to each run's data/params
2. **Sharing:** Users want to share complete working pipelines
3. **Backup:** Each run is independently archivable
4. **Simplicity:** No symlinks or references to resolve
5. **Typical Size:** A complete run with all artifacts is ~5-20 MB (reasonable)

**When deduplication matters:** If you run 1000s of experiments and disk space is critical, see Proposal 3's artifact library approach.

### Implementation Notes
- Projects created explicitly: `runner.run(..., project="wheat_quality")`
- If no project specified, use `projects/default/`
- Short hash: Use existing console output ID (`[pb8yky]`)
- All artifacts saved directly to `models/` subfolder
- `MANIFEST.yaml` tracks artifact relationships
- Auto-archive moves runs to `archive/<date>_<project>/` after 90 days

---

## Proposal 2: Flat Timeline with Smart Naming

### Philosophy
**"Chronological browsing"** - Users want to see recent work first. Use timestamp + descriptive names + short IDs.

### Directory Structure
```
results/
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ 2024-10-14_00h22_regression_PLS-17comp_pb8yky/    # Date + Dataset + Model + ShortID
â”‚   â”‚   â”œâ”€â”€ run.yaml                     # All metadata in ONE file
â”‚   â”‚   â”œâ”€â”€ predictions.csv
â”‚   â”‚   â”œâ”€â”€ 2D_Chart.png
â”‚   â”‚   â”œâ”€â”€ 3D_Chart.png
â”‚   â”‚   â”œâ”€â”€ Y_distribution.png
â”‚   â”‚   â”œâ”€â”€ fold_visualization.png
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ pls_model.pkl
â”‚   â”‚       â””â”€â”€ minmax_scaler.pkl
â”‚   â”œâ”€â”€ 2024-10-14_00h23_classification_RFC-30depth_2e3nwn/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ 2024-10-14_00h24_regression_PLS-15comp_5grd70/    # Best from Q1
â”‚       â””â”€â”€ ...
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ regression.yaml                  # Dataset config (NOT in separate folder)
â”‚   â”œâ”€â”€ regression.nirs4all              # Dataset data archive
â”‚   â”œâ”€â”€ regression_2.yaml
â”‚   â””â”€â”€ classification_Xtrain.yaml
â””â”€â”€ cache/                               # Hidden cache for deduplication
    â””â”€â”€ objects/
        â””â”€â”€ ab/abc123.pkl
```

### Naming Convention
- **Runs**: `YYYY-MM-DD_HHhMMmSSs_<dataset>_<model>_<shortid>`
  - Example: `2024-10-14_00h22_regression_PLS-17comp_pb8yky`
- **Short ID**: Last 6 chars from pipeline UID
- **Model Name**: Simplified (e.g., `PLS-17comp` instead of full class name)

### Rationale
**Pros:**
- âœ… Chronological sorting - latest runs at top when sorted by name
- âœ… ALL information in folder name (dataset, model, ID)
- âœ… Zero navigation depth - all runs at same level
- âœ… No sub-folders to dig through
- âœ… Single `run.yaml` file instead of multiple JSONs
- âœ… Very fast browsing (ls/dir shows everything)
- âœ… Short IDs for reference (`pb8yky`)

**Cons:**
- âš ï¸ Long folder names (but descriptive)
- âš ï¸ Flat structure can become crowded (100+ runs)
- âš ï¸ Need to clean old runs manually
- âš ï¸ Models duplicated (no deduplication)

### Implementation Notes
- Generate folder name from: `f"{timestamp}_{dataset.name}_{model_name}_{short_id}"`
- Model name extracted from pipeline step config
- All outputs directly in run folder (no subdirectories)
- Optional: Add `archive/` command to move old runs

---

## Proposal 3: Hybrid - Date-Organized with Named Experiments

### Philosophy
**"Best of both worlds"** - Organize by date for freshness, but allow grouping by experiment name. Use semantic IDs.

### Directory Structure
```
results/
â”œâ”€â”€ 2024-10/                             # Month folders
â”‚   â”œâ”€â”€ wheat_study/                     # User experiment name (optional)
â”‚   â”‚   â”œâ”€â”€ 14_regression_PLS17_pb8yky/  # Day + Dataset + Model + ID
â”‚   â”‚   â”‚   â”œâ”€â”€ summary.yaml             # Single metadata file
â”‚   â”‚   â”‚   â”œâ”€â”€ predictions.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ charts/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2D_Chart.png
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Y_distribution.png
â”‚   â”‚   â”‚   â””â”€â”€ artifacts/
â”‚   â”‚   â”‚       â””â”€â”€ pls_model_pb8yky.pkl # ID in filename
â”‚   â”‚   â”œâ”€â”€ 14_regression_PLS15_5grd70/
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ experiment.yaml              # Experiment-level notes/config
â”‚   â”œâ”€â”€ protein_pred/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ quick_tests/                     # Default experiment name
â”‚       â””â”€â”€ 14_classification_RFC_2e3nwn/
â”‚           â””â”€â”€ ...
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ regression/
â”‚   â”‚   â”œâ”€â”€ config.yaml                  # Single config file
â”‚   â”‚   â””â”€â”€ data.nirs4all                # Optional: bundled data
â”‚   â””â”€â”€ classification_Xtrain/
â”‚       â””â”€â”€ config.yaml
â””â”€â”€ library/                             # Reusable artifacts catalog
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ pls_17comp_pb8yky.pkl        # Named with ID
    â”‚   â””â”€â”€ pls_15comp_5grd70.pkl
    â”œâ”€â”€ scalers/
    â”‚   â””â”€â”€ minmax_regression_pb8yky.pkl
    â””â”€â”€ catalog.yaml                     # Index of all artifacts
```

### Naming Convention
- **Months**: `YYYY-MM/` (e.g., `2024-10/`)
- **Experiments**: User-defined or `quick_tests` default
- **Runs**: `DD_<dataset>_<model>_<shortid>` (e.g., `14_regression_PLS17_pb8yky`)
- **Artifacts**: `<type>_<context>_<id>.pkl` (e.g., `pls_17comp_pb8yky.pkl`)

### Rationale
**Pros:**
- âœ… Natural organization by date (recent work easy to find)
- âœ… Optional experiment grouping (flexibility)
- âœ… Artifact library with semantic names + IDs
- âœ… Shorter folder names (day only, not full timestamp)
- âœ… Can browse by month when cleaning old runs
- âœ… Single metadata file per run
- âœ… Clear separation of outputs (charts) and artifacts (models)
- âœ… Catalog system makes artifacts discoverable

**Cons:**
- âš ï¸ Two-level hierarchy (month + experiment)
- âš ï¸ Need to specify experiment name (or use default)
- âš ï¸ Artifact library needs maintenance
- âš ï¸ More complex than flat structure

### Implementation Notes
- Experiment name: `runner.run(..., experiment="wheat_study")` or auto-default
- Month folder auto-created from current date
- Short ID: Use existing console output ID (`[pb8yky]`)
- Catalog updated automatically when artifacts saved
- `library/` allows browsing all saved models across experiments

---

## Summary Comparison Table

| Feature | Proposal 1 (Project-Centric) | Proposal 2 (Flat Timeline) | Proposal 3 (Hybrid Date-Experiment) |
|---------|------------------------------|----------------------------|-------------------------------------|
| **Readability** | â­â­â­â­ Project names | â­â­â­â­â­ Full context in name | â­â­â­â­ Month + experiment |
| **Simplicity** | â­â­â­ Two levels | â­â­â­â­â­ One level | â­â­â­ Two-three levels |
| **Latest First** | â­â­ Need to check project | â­â­â­â­â­ Chronological names | â­â­â­â­ Month folder sorting |
| **Deduplication** | â­â­â­â­â­ Hidden shared artifacts | â­â­ No dedup | â­â­â­â­ Artifact library |
| **Discoverability** | â­â­â­â­ By project | â­â­â­â­ By folder name | â­â­â­â­â­ By month + catalog |
| **Cleaning Old Runs** | â­â­â­â­â­ Auto-archive | â­â­ Manual | â­â­â­â­ By month |
| **File Count** | â­â­â­ Multiple per run | â­â­â­â­ All in one folder | â­â­â­ Balanced |
| **ID Length** | â­â­â­â­â­ 6 chars | â­â­â­â­â­ 6 chars | â­â­â­â­â­ 6 chars |
| **Flexibility** | â­â­â­ Need project | â­â­â­â­ No structure needed | â­â­â­â­â­ Optional experiment |

---

## Recommended Choice

### ğŸ† **Proposal 3 (Hybrid Date-Experiment)** is recommended because:

1. **Best for users at all skill levels:**
   - Beginners: Default `quick_tests/` experiment, easy chronological browsing
   - Advanced: Custom experiment names for organization
   - Administrators: Month folders make cleanup natural

2. **Balances technical needs:**
   - Artifact library maintains deduplication benefits
   - Short IDs (6 chars) are user-friendly and console-visible
   - Single `summary.yaml` per run reduces clutter

3. **Future-proof:**
   - Can add more experiments without restructuring
   - Month archiving prevents results/ from growing unbounded
   - Catalog system enables advanced queries later

4. **Migration path:**
   - Current UUID-based structure can be converted
   - Generate short IDs from existing UUIDs (first 6 chars or hash)
   - Build catalog from existing artifacts directory

### Next Steps:
1. Confirm chosen proposal
2. Design migration strategy from current structure
3. Implement short ID generation (use existing console IDs like `[pb8yky]`)
4. Create `summary.yaml` schema (merge pipeline.json + metadata.json)
5. Add experiment name parameter to runner API
