## Cheat-Sheet: NIR Preprocessing by Model

### Classical ML (scikit-learn + libraries)

| Model                                             | Task | Works well                                                                                                                                                  | Avoid / Often harmful                                                                                            |
| ------------------------------------------------- | ---- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| **PLS (PLS-R, PLS-DA)**                           | R/C  | Mean-centering; SNV/MSC/EMSC; mild SG smoothing; **1st derivative**; detrend; wavelength masking of water/noise bands; outlier control; CV for n_components | Over-aggressive **2nd deriv** with short windows; no scatter correction; leaking preprocessing (fit on full set) |
| **OPLS / Kernel PLS / Local PLS (L-PLS, LW-PLS)** | R/C  | Same as PLS; for Local/LW: **distance-aware scaling** (SNV), band selection; for Kernel: global **standardization**                                         | Using raw unscaled spectra for neighbor search; noisy high-order derivatives; too many irrelevant bands          |
| **PCR**                                           | R    | Center + (often) autoscale; SG smoothing; baseline removal before PCA; retain PCs via CV                                                                    | PCA on raw uncorrected scatter; keeping too many PCs; PCA fitted outside CV                                      |
| **Linear / Ridge / Lasso / ElasticNet**           | R/C  | Center + scale; SNV/MSC; mild SG; band selection to reduce collinearity                                                                                     | No scaling; strong noise amplification via derivatives; many collinear bands without regularization              |
| **LDA / QDA**                                     | C    | Dimension reduction first (PCA/PLS scores); center + scale; SNV/MSC; outlier control                                                                        | Training directly on thousands of wavelengths; uncorrected batch/scatter effects                                 |
| **k-NN**                                          | R/C  | **Per-feature scaling** or SNV; baseline/scatter correction; smoothed **1st deriv**; band selection or PCA/PLS scores                                       | Raw unscaled spectra; high-order noisy derivatives; very high-D without reduction                                |
| **SVM / SVR (RBF/linear/poly)**                   | R/C  | **Standardization**; SNV/MSC; SG + **1st deriv**; band selection or PCA/PLS scores; outlier control                                                         | No scaling; feeding entire noisy spectrum; aggressive 2nd deriv without smoothing                                |
| **Decision Tree**                                 | R/C  | SG smoothing; SNV/MSC if strong scatter; **band/bin selection** to cut redundancy                                                                           | Per-feature standardization; high-order noisy derivatives; raw outliers                                          |
| **Random Forest**                                 | R/C  | SG smoothing; SNV/MSC helpful; **band/bin selection**; remove obvious noise regions                                                                         | Standardization per wavelength; over-derivation amplifying noise; feeding many redundant bands                   |
| **Gradient Boosting (sklearn)**                   | R/C  | As RF; plus outlier trimming; modest feature reduction; early stopping                                                                                      | Per-feature standardization; noisy 2nd deriv; no denoising                                                       |
| **XGBoost / LightGBM / CatBoost**                 | R/C  | As boosting: SG smoothing; SNV/MSC; band/bin selection; remove artifacts; tune regularization                                                               | Standardization per wavelength; noisy derivatives; leaving many useless bands                                    |
| **TabPFN**                                        | R/C  | Minimal scaling needed (internal z-score); **mask noisy/irrelevant bands**; optional band/bin reduction; remove outliers                                    | Manual re-scaling expecting effect; feeding artifact regions; extreme dimensionality with tiny N                 |

### Neural Networks

| Model                                                                   | Task | Works well                                                                                                                                       | Avoid / Often harmful                                                                  |
| ----------------------------------------------------------------------- | ---- | ------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------- |
| **MLP (fully-connected)**                                               | R/C  | **Standardization** or min-max; mean-centering/SNV; baseline removal; SG smoothing; band/PCA/PLS scores to cut input size; dropout/weight-decay  | Raw unscaled spectra; high-D collinearity with small N; noisy high-order derivatives   |
| **1D CNN (incl. ResNet-1D, Xception-1D)**                               | R/C  | Input scaling to stable range; SNV/MSC; SG smoothing; **1st deriv** optional; data augmentation (jitter, shift, intensity scale); mask bad bands | No normalization; over-smoothed spectra losing peaks; pure 2nd deriv without smoothing |
| **RNN (LSTM/GRU)**                                                      | R/C  | Standardization; mean-centering; baseline removal; moderate smoothing; optional **downsampling/binning** to shorten sequence                     | Very long raw sequences with noise; unscaled inputs that saturate gates                |
| **Transformers (seq or ViT-style patches)**                             | R/C  | Standardization; positional encoding; SNV/MSC; denoising or **patch/bin tokens**; mask artifact regions                                          | Raw baselines/scatter; very long token sequences without reduction; no normalization   |
| **Vision backbones via transfer (ResNet/Xception/ViT on 2D encodings)** | C/R  | Match pretrained **input normalization**; consistent encoding (e.g., spectrum→1×N image); SNV/MSC before encoding; mild smoothing                | Mismatch of expected scale; noisy encodings; ignoring positional order                 |

### Quick rules of thumb

* **Scatter/baseline present?** Use **SNV/MSC/EMSC + detrend** before almost any model.
* **Noisy spectra?** Apply **SG smoothing**; keep **1st derivative** conservative.
* **High-D with small N?** Prefer **band/bin selection** or **PCA/PLS scores** for k-NN, SVM, linear, MLP; trees/boosters still benefit.
* **Models needing scaling:** SVM/SVR, k-NN, linear/regularized, MLP, RNN, Transformers.
* **Models disliking per-feature standardization:** Trees, RF, Boosting.
* **Derivatives:** 1st deriv helps PLS/SVM/k-NN/CNN; **2nd deriv** only with adequate smoothing and SNR.
* **Always fit preprocessing inside CV/pipeline** to avoid leakage.
