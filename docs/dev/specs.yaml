# Core Data Structures

SpectraDataset:
  attributes:
    - X: Dict[str, np.ndarray]  # multi-source data {source_id: features_matrix}
    - indices: pl.DataFrame     # multiindex tracking
    - targets: pl.DataFrame     # target values per sample
    - results: pl.DataFrame     # predictions accumulator
    - samples_metadata: pl.DataFrame  # per-sample metadata (age, batch, etc.)
    - metadata: Dict[str, Any]  # dataset-level metadata
  methods:
    - get_view(context: PipelineContext) -> DatasetView
    - clone_structure() -> SpectraDataset
    - merge_sources(source_keys: List[str]) -> None
    - add_source(key: str, data: np.ndarray) -> None
    - get_active_X(context: PipelineContext) -> np.ndarray  # merged or specific source

DatasetView:
  attributes:
    - X: np.ndarray            # view of active data
    - indices: pl.DataFrame    # filtered indices
    - targets: pl.DataFrame    # filtered targets
    - samples_metadata: pl.DataFrame  # filtered metadata
    - parent: SpectraDataset
  methods:
    - update_parent_indices(new_indices: pl.DataFrame) -> None
    - get_partition_mask(partition: str) -> np.ndarray
    - get_cluster_centroids() -> np.ndarray

PipelineContext:
  attributes:
    - current_partition: Optional[str]
    - current_branch: int
    - current_seed: int
    - current_processing: str
    - use_centroids: bool
    - active_sources: List[str]  # which sources to use
    - filters: Dict[str, Any]
    - pipeline_depth: int        # for recursive pipelines
  methods:
    - clone() -> PipelineContext
    - with_branch(branch: int) -> PipelineContext
    - with_partition(partition: str) -> PipelineContext
    - with_sources(sources: List[str]) -> PipelineContext
    - push_depth() -> PipelineContext  # for sub-pipelines
    - pop_depth() -> PipelineContext

# Base Operation Classes

PipelineOperation:
  methods:
    - execute(dataset: SpectraDataset, context: PipelineContext) -> None
    - can_execute(dataset: SpectraDataset, context: PipelineContext) -> bool
    - get_name() -> str

TransformationOperation(PipelineOperation):
  attributes:
    - transformer: Any
    - is_fitted: bool
    - fit_partition: str = "train"
  methods:
    - fit(X: np.ndarray, y: np.ndarray = None) -> None
    - transform(X: np.ndarray) -> np.ndarray
    - fit_transform(X: np.ndarray, y: np.ndarray = None) -> np.ndarray
    - _update_dataset_X(dataset: SpectraDataset, X_new: np.ndarray, context: PipelineContext) -> None

AugmentationOperation(PipelineOperation):
  attributes:
    - augmentations: List[Any]
    - target_partition: str = "train"
  methods:
    - augment_samples(dataset: SpectraDataset, context: PipelineContext) -> None  # creates new sample IDs
    - augment_features(dataset: SpectraDataset, context: PipelineContext) -> None  # keeps sample IDs
    - _create_augmented_indices(base_indices: pl.DataFrame, aug_type: str) -> pl.DataFrame

SplitOperation(PipelineOperation):
  attributes:
    - splitter: BaseEstimator
    - target_column: str = "target"
  methods:
    - generate_splits(X: np.ndarray, y: np.ndarray, groups: np.ndarray = None) -> Iterator[Tuple]
    - update_partition_indices(dataset: SpectraDataset, splits: Iterator, context: PipelineContext) -> None

ClusteringOperation(PipelineOperation):
  attributes:
    - clusterer: BaseEstimator
    - centroids_: Optional[np.ndarray]
    - labels_: Optional[np.ndarray]
    - cluster_metadata_: Dict[str, Any]
  methods:
    - fit_clusters(X: np.ndarray) -> None
    - predict_clusters(X: np.ndarray) -> np.ndarray
    - create_centroids_samples(dataset: SpectraDataset, context: PipelineContext) -> None
    - update_cluster_indices(dataset: SpectraDataset, labels: np.ndarray, context: PipelineContext) -> None

ModelingOperation(PipelineOperation):
  attributes:
    - model: BaseEstimator
    - y_pipeline: List[TransformationOperation]
    - is_fitted: bool
    - feature_importance_: Optional[np.ndarray]
  methods:
    - process_targets(y: np.ndarray) -> np.ndarray
    - fit_model(X: np.ndarray, y: np.ndarray) -> None
    - predict(X: np.ndarray) -> np.ndarray
    - predict_proba(X: np.ndarray) -> np.ndarray
    - get_feature_importance() -> np.ndarray
    - _update_results(dataset: SpectraDataset, predictions: np.ndarray, context: PipelineContext) -> None

StackingOperation(ModelingOperation):
  attributes:
    - meta_model: BaseEstimator
    - base_learners: List[Dict[str, Any]]
    - fitted_base_models: List[ModelingOperation]
    - base_predictions_: Optional[np.ndarray]
  methods:
    - fit_base_learners(X: np.ndarray, y: np.ndarray) -> None
    - get_base_predictions(X: np.ndarray) -> np.ndarray
    - fit_meta_model(base_preds: np.ndarray, y: np.ndarray) -> None

OptimizationOperation(PipelineOperation):
  attributes:
    - base_operation: ModelingOperation
    - param_space: Dict[str, Any]
    - optimization_metric: str
    - n_trials: int
    - best_params_: Optional[Dict]
    - best_score_: Optional[float]
  methods:
    - optimize(dataset: SpectraDataset, context: PipelineContext) -> Dict[str, Any]
    - objective_function(trial, dataset: SpectraDataset, context: PipelineContext) -> float
    - update_operation_params(params: Dict[str, Any]) -> None

DispatchOperation(PipelineOperation):
  attributes:
    - branches: List[List[PipelineOperation]]  # parsed sub-pipelines
    - parallel: bool
    - branch_results: List[Dict]
  methods:
    - execute_branch(branch_ops: List[PipelineOperation], dataset: SpectraDataset, context: PipelineContext) -> None
    - execute_parallel(dataset: SpectraDataset, context: PipelineContext) -> None
    - execute_sequential(dataset: SpectraDataset, context: PipelineContext) -> None
    - merge_branch_results(dataset: SpectraDataset) -> None

RecursivePipelineOperation(PipelineOperation):  # for sub-pipelines
  attributes:
    - sub_pipeline: List[PipelineOperation]
    - max_depth: int = 10
  methods:
    - execute_recursive(dataset: SpectraDataset, context: PipelineContext) -> None
    - check_recursion_depth(context: PipelineContext) -> bool

VisualizationOperation(PipelineOperation):
  attributes:
    - plot_type: str
    - save_path: Optional[str]
    - plot_config: Dict[str, Any]
  methods:
    - generate_plot(view: DatasetView, context: PipelineContext) -> None
    - save_plot(fig: Any, context: PipelineContext) -> None

# Specific Operations

MergeSourcesOperation(PipelineOperation):
  attributes:
    - merge_strategy: str = "concatenate"  # concatenate, average, weighted
    - axis: int = 1  # feature axis
  methods:
    - merge_horizontal(X_dict: Dict[str, np.ndarray]) -> np.ndarray
    - merge_average(X_dict: Dict[str, np.ndarray]) -> np.ndarray
    - update_feature_names(dataset: SpectraDataset) -> None

UnclusterOperation(PipelineOperation):
  methods:
    - remove_centroid_samples(dataset: SpectraDataset, context: PipelineContext) -> None
    - preserve_cluster_labels(dataset: SpectraDataset) -> None

SklearnTransformer(TransformationOperation):
  attributes:
    - sklearn_transformer: BaseEstimator

SampleAugmentation(AugmentationOperation):
  attributes:
    - augmentation_methods: List[Callable]  # RT, noise, etc.
  methods:
    - apply_rt_augmentation(X: np.ndarray, **params) -> np.ndarray
    - apply_noise_augmentation(X: np.ndarray, **params) -> np.ndarray

FeatureAugmentation(AugmentationOperation):
  attributes:
    - preprocessing_methods: List[Any]  # SNV, SG, etc.

# Pipeline Management

Pipeline:
  attributes:
    - config: Dict[str, Any]
    - operations: List[PipelineOperation]
    - dataset: Optional[SpectraDataset]
    - context: PipelineContext
    - execution_log: List[Dict[str, Any]]
  methods:
    - build_from_config() -> None
    - execute() -> SpectraDataset
    - execute_step(step_idx: int) -> None
    - validate_pipeline() -> bool
    - get_execution_summary() -> Dict[str, Any]
    - _log_step_execution(operation: PipelineOperation, success: bool, error: Optional[str]) -> None

OperationFactory:
  methods:
    - create_operation(config_item: Union[str, Dict, Any]) -> PipelineOperation
    - parse_sklearn_object(obj: Any) -> PipelineOperation
    - parse_dict_config(config: Dict) -> PipelineOperation
    - parse_string_operation(op_name: str) -> PipelineOperation
    - build_sub_pipeline(pipeline_config: List) -> List[PipelineOperation]  # for recursive

ConfigValidator:
  methods:
    - validate_config(config: Dict) -> bool
    - validate_operation_sequence(operations: List) -> bool
    - check_data_flow_compatibility(op1: PipelineOperation, op2: PipelineOperation) -> bool

# Utilities

DatasetOptimizer:
  methods:
    - optimize_memory_layout(X: np.ndarray) -> np.ndarray
    - create_efficient_views(dataset: SpectraDataset, context: PipelineContext) -> DatasetView
    - batch_process_large_arrays(X: np.ndarray, operation: Callable, batch_size: int) -> np.ndarray

IndexManager:
  methods:
    - create_indices_from_sources(sources: Dict[str, np.ndarray]) -> pl.DataFrame
    - filter_indices(indices: pl.DataFrame, filters: Dict) -> pl.DataFrame
    - update_indices_batch(indices: pl.DataFrame, updates: Dict) -> pl.DataFrame
    - merge_indices(indices_list: List[pl.DataFrame]) -> pl.DataFrame

ResultsCollector:
  attributes:
    - results: pl.DataFrame
  methods:
    - add_predictions(sample_ids: np.ndarray, predictions: np.ndarray, context: PipelineContext) -> None
    - add_metrics(metrics: Dict[str, float], context: PipelineContext) -> None
    - get_results_by_branch(branch_id: int) -> pl.DataFrame
    - export_results(format: str = "csv") -> None

ErrorHandler:
  methods:
    - handle_operation_error(operation: PipelineOperation, error: Exception, context: PipelineContext) -> None
    - log_error(error: Exception, context: Dict) -> None
    - should_continue_on_error(error: Exception) -> bool