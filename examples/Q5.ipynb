{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from nirs4all.operators.transformations import Gaussian, SavitzkyGolay, StandardNormalVariate, Haar\n",
    "from nirs4all.pipeline.config import PipelineConfigs\n",
    "from nirs4all.dataset.dataset_config import DatasetConfigs\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from nirs4all.dataset.prediction_visualizer import PredictionVisualizer\n",
    "\n",
    "# # Clear old results to ensure fresh training with metadata\n",
    "# results_path = Path(\"./results\")\n",
    "# if results_path.exists():\n",
    "#     shutil.rmtree(results_path)\n",
    "#     print(\"üßπ Cleared old results to ensure fresh training\")\n",
    "\n",
    "# pipeline = [\n",
    "#     # Normalize the spectra reflectance\n",
    "#     MinMaxScaler(),\n",
    "#     {\"y_processing\": MinMaxScaler},\n",
    "\n",
    "#     # Generate 5 version of feature augmentation combinations (3 elements with size 1 to 2, ie. [SG, [SNV, GS], Haar])\n",
    "#     {\n",
    "#         \"feature_augmentation\": {\n",
    "#             Gaussian, StandardNormalVariate, SavitzkyGolay, Haar,\n",
    "#         }\n",
    "#     },\n",
    "#     # Split the dataset in train and validation\n",
    "#     ShuffleSplit(n_splits=3, test_size=.25),\n",
    "#     # Normalize the y values\n",
    "#     {\"model\": PLSRegression(10)},\n",
    "# ]\n",
    "\n",
    "# p_configs = PipelineConfigs(pipeline)\n",
    "\n",
    "# path = ['../../sample_data/regression', '../../sample_data/classification', '../../sample_data/binary']\n",
    "path = '../../sample_data/multi'\n",
    "d_configs = DatasetConfigs(path)\n",
    "\n",
    "# # Train with explicit settings to ensure metadata is saved\n",
    "# runner = PipelineRunner(save_files=True, verbose=0)  # Set verbose=0 to reduce output\n",
    "# predictions, results = runner.run(p_configs, d_configs)\n",
    "\n",
    "# print(f\"\\n=== TRAINING METADATA CHECK ===\")\n",
    "# print(f\"Step binaries tracked: {len(runner.step_binaries)} steps\")\n",
    "# print(f\"Sample step binaries: {dict(list(runner.step_binaries.items())[:3])}\")\n",
    "\n",
    "# visualizer = PredictionVisualizer(predictions, dataset_name_override=\"dataset\")\n",
    "# top_5 = visualizer.get_top_k(5, 'rmse') ##TODO get_top_1\n",
    "\n",
    "# print(f\"\\n=== TOP 5 RESULTS ===\")\n",
    "# for i, model in enumerate(top_5, 1):\n",
    "#     print(f\"{i}. {model['path']} - RMSE: {model['rmse']:.6f}, R¬≤: {model['r2']:.6f}, MAE: {model['mae']:.6f} {'‚úÖ' if has_metadata else '‚ùå'}\")\n",
    "\n",
    "# print(f\"\\n=== TESTING PREDICTION ===\")\n",
    "# best_path = top_5[0]['path']\n",
    "# print(f\"Using best model from: {best_path}\")\n",
    "\n",
    "# try:\n",
    "#     predictions = PipelineRunner.predict(\n",
    "#         path=best_path,\n",
    "#         dataset=d_configs,\n",
    "#         # model=my_model,##TODO\n",
    "#         best_model=False,##TODO quand on veut pr√©dire sur tous les mod√®les\n",
    "#         verbose=1\n",
    "#     )\n",
    "#     print(\"‚úÖ Prediction successful!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Prediction failed: {e}\")\n",
    "#     # Show which step failed\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391b4294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "üìä Multiple train_x files found for ../../sample_data/multi: 3 sources detected.\n",
      "üìä Multiple test_x files found for ../../sample_data/multi: 3 sources detected.\n",
      "‚úÖ Loaded dataset 'multi' with 3 training and 3 test samples.\n",
      "[<nirs4all.dataset.dataset.SpectroDataset object at 0x000002499C05D8A0>]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from nirs4all.operators.transformations import MultiplicativeScatterCorrection\n",
    "from nirs4all.dataset.dataset_config import DatasetConfigs\n",
    "path = '../../sample_data/multi'\n",
    "d_configs = DatasetConfigs(path)\n",
    "# datasets = d_configs.get_datasets()\n",
    "print(d_configs.get_datasets())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pipeline = [\n",
    "#     MinMaxScaler(feature_range=(0.1, 0.8)),\n",
    "#     MultiplicativeScatterCorrection,\n",
    "#     ShuffleSplit(n_splits=3),\n",
    "#     {\"y_processing\": MinMaxScaler},\n",
    "#     {\"model\": PLSRegression(15)},\n",
    "# ]\n",
    "# p_configs = PipelineConfigs(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "195a8e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded dataset 'multi' with 3 training and 3 test samples.\n",
      "‚úÖ Multi-source dataset loading works with simplified loader!\n",
      "Dataset name: multi\n",
      "Number of sources: 3\n",
      "Is multi-source: True\n",
      "\n",
      "Dataset details:\n",
      "üìä Dataset: multi\n",
      "Features (samples=189, sources=3):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "- Source 1: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "- Source 2: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\n"
     ]
    }
   ],
   "source": [
    "# Test that the simplified multi-source dataset works correctly\n",
    "dataset = d_configs.get_datasets()[0]\n",
    "\n",
    "print(f\"‚úÖ Multi-source dataset loading works with simplified loader!\")\n",
    "print(f\"Dataset name: {dataset.name}\")\n",
    "print(f\"Number of sources: {dataset.features_sources()}\")\n",
    "print(f\"Is multi-source: {dataset.is_multi_source()}\")\n",
    "print(\"\\nDataset details:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee30585",
   "metadata": {},
   "source": [
    "## ‚úÖ Multi-Source Dataset Loading Fixed!\n",
    "\n",
    "The loader has been successfully simplified and cleaned up:\n",
    "\n",
    "**Key Changes Made:**\n",
    "1. **Removed all report handling** - No more complex report dictionaries and error tracking\n",
    "2. **Simplified error handling** - Just raise ValueError with clear messages instead of complex report management\n",
    "3. **Removed code duplication** - The multi-source logic now simply calls `load_XY` multiple times\n",
    "4. **Fixed multi-source handling** - In `handle_data`, when `x_path` is a list, it loops through each path\n",
    "5. **Cleaner Y handling** - For additional X sources (after the first), Y is handled as empty array\n",
    "\n",
    "**How Multi-Source Works Now:**\n",
    "- When `train_x` or `test_x` is a list of paths (detected by folder parser)\n",
    "- `handle_data` calls `load_XY` for each X source\n",
    "- First source extracts Y data, subsequent sources get empty Y arrays\n",
    "- Returns list of X arrays and single Y array\n",
    "\n",
    "**Result:** 3 sources, 189 samples, 2151 features per source, working perfectly! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
