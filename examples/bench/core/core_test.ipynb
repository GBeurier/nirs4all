{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "304bfbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from SpectraDataset import SpectraDataset\n",
    "\n",
    "dataset_reg_1_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression\n",
    "dataset_reg_2_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression (first output)\n",
    "dataset_reg_2_2 = SpectraDataset(task_type=\"regression\")  # Single-output regression (second output)\n",
    "dataset_cla_2_1 = SpectraDataset(task_type=\"classification\")  # Single-output classification (first output)\n",
    "dataset_cla_2_2 = SpectraDataset(task_type=\"classification\")  # Single-output classification (second output)\n",
    "dataset_bin_1_1 = SpectraDataset(task_type=\"binary\")  # Binary classification\n",
    "\n",
    "# Features\n",
    "f1_source = np.random.rand(100, 1000) * 2.5 + 1.5\n",
    "f2_source = np.random.rand(100, 500) * 12 + 3.5\n",
    "\n",
    "# Targets\n",
    "reg_target_1 = np.random.rand(100,)  # 1D array for single-output regression\n",
    "reg_target_2_first = np.random.rand(100,)  # First output of multi-output regression\n",
    "reg_target_2_second = np.random.rand(100,)  # Second output of multi-output regression\n",
    "cla_target_2_first = np.random.randint(0, 5, size=(100,))  # First output of multi-output classification\n",
    "cla_target_2_second = np.random.randint(0, 5, size=(100,))  # Second output of multi-output classification\n",
    "bin_target_1 = np.random.randint(0, 2, size=(100,))  # 1D array for binary classification\n",
    "\n",
    "# Add data to datasets\n",
    "dataset_reg_1_1.add_data([f1_source], reg_target_1)\n",
    "dataset_reg_2_1.add_data([f1_source, f2_source], reg_target_2_first)\n",
    "dataset_reg_2_2.add_data([f1_source, f2_source], reg_target_2_second)\n",
    "dataset_cla_2_1.add_data([f1_source, f2_source], cla_target_2_first)\n",
    "dataset_cla_2_2.add_data([f1_source, f2_source], cla_target_2_second)\n",
    "dataset_bin_1_1.add_data([f1_source], bin_target_1)\n",
    "pass\n",
    "# print(\"Dataset for regression 1-1:\", dataset_reg_1_1)\n",
    "# print(\"Dataset for regression 2-1 (first output):\", dataset_reg_2_1)\n",
    "# print(\"Dataset for regression 2-2 (second output):\", dataset_reg_2_2)\n",
    "# print(\"Dataset for classification 2-1 (first output):\", dataset_cla_2_1)\n",
    "# print(\"Dataset for classification 2-2 (second output):\", dataset_cla_2_2)\n",
    "# print(\"Dataset for binary classification 1-1:\", dataset_bin_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bed976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset before pipeline: \n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "üöÄ Starting Pipeline Runner\n",
      "üîπ Step 1: Dict with 3 keys\n",
      "üîπ Current context: PipelineContext(filters={}, branch=0, processing_level=0, scope_depth=0)\n",
      "üîπ Step config: {'instance': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}\n",
      "  ‚öôÔ∏è Executing: Transform(MinMaxScaler)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step 1 completed: Dict with 3 keys\n",
      "Dataset state after step 1:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "‚úÖ Pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PipelineRunner import PipelineRunner\n",
    "\n",
    "config = { \"pipeline\": [ MinMaxScaler(feature_range=(0.2,0.8)) ] }\n",
    "\n",
    "runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "\n",
    "print(\"Dataset before pipeline:\", dataset_reg_1_1)\n",
    "print(\"-\"*200)\n",
    "dataset_res_json, fitted_json, history_json, tree_json = runner.run(config, dataset_reg_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7232a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRANSFORMATION VERIFICATION ===\n",
      "\n",
      "Original features shape: (100, 1000)\n",
      "Original mean: 2.7512\n",
      "Original std: 0.7206\n",
      "Original min: 1.5000\n",
      "Original max: 4.0000\n",
      "\n",
      "Transformed features shape: (100, 1000)\n",
      "Transformed mean: 2.7512\n",
      "Transformed std: 0.7206\n",
      "Transformed min: 1.5000\n",
      "Transformed max: 4.0000\n",
      "\n",
      "Expected range: [0.2, 0.8]\n",
      "Actual range: [1.5000, 4.0000]\n",
      "‚ùå MinMaxScaler transformation did NOT work correctly!\n",
      "   The features should be scaled to range [0.2, 0.8]\n"
     ]
    }
   ],
   "source": [
    "# Check if transformation worked - compare mean and std before and after\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== TRANSFORMATION VERIFICATION ===\")\n",
    "print()\n",
    "\n",
    "# Get features from original dataset using select() to get all data\n",
    "original_view = dataset_reg_1_1.select()\n",
    "original_features = original_view.get_features()\n",
    "print(f\"Original features shape: {original_features.shape}\")\n",
    "print(f\"Original mean: {np.mean(original_features):.4f}\")\n",
    "print(f\"Original std: {np.std(original_features):.4f}\")\n",
    "print(f\"Original min: {np.min(original_features):.4f}\")\n",
    "print(f\"Original max: {np.max(original_features):.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Get features from transformed dataset\n",
    "transformed_view = dataset_res_json.select()\n",
    "transformed_features = transformed_view.get_features()\n",
    "print(f\"Transformed features shape: {transformed_features.shape}\")\n",
    "print(f\"Transformed mean: {np.mean(transformed_features):.4f}\")\n",
    "print(f\"Transformed std: {np.std(transformed_features):.4f}\")\n",
    "print(f\"Transformed min: {np.min(transformed_features):.4f}\")\n",
    "print(f\"Transformed max: {np.max(transformed_features):.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check if transformation is as expected (MinMaxScaler with range 0.2-0.8)\n",
    "expected_min = 0.2\n",
    "expected_max = 0.8\n",
    "actual_min = np.min(transformed_features)\n",
    "actual_max = np.max(transformed_features)\n",
    "\n",
    "print(f\"Expected range: [{expected_min}, {expected_max}]\")\n",
    "print(f\"Actual range: [{actual_min:.4f}, {actual_max:.4f}]\")\n",
    "\n",
    "if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "    print(\"‚úÖ MinMaxScaler transformation worked correctly!\")\n",
    "else:\n",
    "    print(\"‚ùå MinMaxScaler transformation did NOT work correctly!\")\n",
    "    print(\"   The features should be scaled to range [0.2, 0.8]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f700c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING OPERATIONTRANSFORMATION ===\n",
      "‚úÖ OperationTransformation imported successfully\n",
      "‚úÖ OperationTransformation created: <operations.OperationTranformation.OperationTransformation object at 0x0000028BC640B730>\n",
      "   Operation name: Transform(MinMaxScaler)\n",
      "   Is scaler a TransformerMixin? True\n",
      "\n",
      "=== DEBUGGING PIPELINE BUILDER ===\n",
      "PipelineBuilder created operation: <class 'operations.OperationTranformation.OperationTransformation'>\n",
      "Operation name: Transform(MinMaxScaler)\n",
      "\n",
      "PipelineBuilder.build_operation created: <class 'operations.OperationTranformation.OperationTransformation'>\n",
      "Operation name: Transform(MinMaxScaler)\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check if OperationTransformation can be imported and used\n",
    "print(\"=== DEBUGGING OPERATIONTRANSFORMATION ===\")\n",
    "\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    print(\"‚úÖ OperationTransformation imported successfully\")\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "\n",
    "    # Create an OperationTransformation directly\n",
    "    operation = OperationTransformation(transformer=scaler)\n",
    "    print(f\"‚úÖ OperationTransformation created: {operation}\")\n",
    "    print(f\"   Operation name: {operation.get_name()}\")\n",
    "\n",
    "    # Test if it's a sklearn transformer\n",
    "    from sklearn.base import TransformerMixin\n",
    "    print(f\"   Is scaler a TransformerMixin? {isinstance(scaler, TransformerMixin)}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import OperationTransformation: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating OperationTransformation: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check what the PipelineBuilder is actually creating\n",
    "from PipelineBuilder import PipelineBuilder\n",
    "builder = PipelineBuilder()\n",
    "\n",
    "print(\"=== DEBUGGING PIPELINE BUILDER ===\")\n",
    "try:\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    operation = builder._wrap_operator(scaler)\n",
    "    print(f\"PipelineBuilder created operation: {type(operation)}\")\n",
    "    print(f\"Operation name: {operation.get_name()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in PipelineBuilder._wrap_operator: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check what build_operation creates for our config\n",
    "try:\n",
    "    operation = builder.build_operation(scaler)\n",
    "    print(f\"PipelineBuilder.build_operation created: {type(operation)}\")\n",
    "    print(f\"Operation name: {operation.get_name()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in PipelineBuilder.build_operation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3721558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMPLE DEBUGGING ===\n",
      "No get_partition_names method or error getting partitions\n",
      "‚úÖ OperationTransformation imported successfully\n",
      "‚úÖ Is MinMaxScaler a TransformerMixin? True\n",
      "\n",
      "Let's check the fitted pipeline history to see what actually executed:\n",
      "Error accessing history: 'PipelineHistory' object has no attribute 'steps'\n"
     ]
    }
   ],
   "source": [
    "# Simple debugging - check dataset partitions and pipeline operations\n",
    "print(\"=== SIMPLE DEBUGGING ===\")\n",
    "\n",
    "# Check what partitions exist in our dataset\n",
    "try:\n",
    "    partitions = dataset_reg_1_1.get_partition_names()\n",
    "    print(f\"Available partitions: {partitions}\")\n",
    "except:\n",
    "    print(\"No get_partition_names method or error getting partitions\")\n",
    "\n",
    "# Check if OperationTransformation can be imported\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    print(\"‚úÖ OperationTransformation imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "\n",
    "# Check sklearn TransformerMixin\n",
    "try:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.base import TransformerMixin\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    print(f\"‚úÖ Is MinMaxScaler a TransformerMixin? {isinstance(scaler, TransformerMixin)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå sklearn error: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"Let's check the fitted pipeline history to see what actually executed:\")\n",
    "try:\n",
    "    print(f\"History steps: {len(history_json.steps)}\")\n",
    "    for i, step in enumerate(history_json.steps):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing history: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84748cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INVESTIGATING PARTITIONS ===\n",
      "Available dataset methods: []\n",
      "Total samples in dataset: 100\n",
      "\n",
      "Train partition has 100 samples\n",
      "All data (no partition filter) has 100 samples\n",
      "\n",
      "=== TRYING MANUAL OPERATIONTRANSFORMATION ===\n",
      "Let's try to create and execute OperationTransformation manually...\n",
      "Executing OperationTransformation manually...\n",
      "üîÑ Executing Transform(MinMaxScaler)\n",
      "‚ùå Manual execution failed: No data found in partition 'None' for fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\U108-N257\\AppData\\Local\\Temp\\ipykernel_17444\\1393911186.py\", line 60, in <module>\n",
      "    operation.execute(dataset_reg_1_1, context)\n",
      "  File \"c:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\operations\\OperationTranformation.py\", line 52, in execute\n",
      "    raise ValueError(f\"No data found in partition '{self.fit_partition}' for fitting\")\n",
      "ValueError: No data found in partition 'None' for fitting\n"
     ]
    }
   ],
   "source": [
    "# Check and fix partition issue\n",
    "print(\"=== INVESTIGATING PARTITIONS ===\")\n",
    "\n",
    "# Check all available methods on the dataset\n",
    "dataset_methods = [method for method in dir(dataset_reg_1_1) if not method.startswith('_')]\n",
    "print(f\"Available dataset methods: {[m for m in dataset_methods if 'partition' in m.lower()]}\")\n",
    "\n",
    "# Check if there's a way to see what data exists\n",
    "try:\n",
    "    view = dataset_reg_1_1.select()\n",
    "    print(f\"Total samples in dataset: {len(view)}\")\n",
    "\n",
    "    # Try to see sample info\n",
    "    if hasattr(view, 'sample_ids'):\n",
    "        print(f\"Sample IDs: {view.sample_ids[:5]}...\")  # First 5\n",
    "    if hasattr(view, 'partitions'):\n",
    "        print(f\"Partitions in view: {view.partitions}\")\n",
    "    if hasattr(view, 'row_indices'):\n",
    "        print(f\"Row indices: {view.row_indices[:5]}...\")  # First 5\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking dataset view: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Try to select with specific partition\n",
    "try:\n",
    "    train_view = dataset_reg_1_1.select(partition=\"train\")\n",
    "    print(f\"Train partition has {len(train_view)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"No 'train' partition found: {e}\")\n",
    "\n",
    "try:\n",
    "    all_view = dataset_reg_1_1.select()\n",
    "    print(f\"All data (no partition filter) has {len(all_view)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting all data: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"=== TRYING MANUAL OPERATIONTRANSFORMATION ===\")\n",
    "print(\"Let's try to create and execute OperationTransformation manually...\")\n",
    "\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from PipelineContext import PipelineContext\n",
    "\n",
    "    # Create scaler and operation\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    operation = OperationTransformation(\n",
    "        transformer=scaler,\n",
    "        fit_partition=None,  # Try with None first\n",
    "        transform_partitions=None\n",
    "    )\n",
    "\n",
    "    # Create context\n",
    "    context = PipelineContext()\n",
    "\n",
    "    print(\"Executing OperationTransformation manually...\")\n",
    "    operation.execute(dataset_reg_1_1, context)\n",
    "    print(\"‚úÖ Manual execution completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Manual execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55afa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXED MANUAL OPERATIONTRANSFORMATION TEST ===\n",
      "Before transformation:\n",
      "  Min: 1.5000, Max: 4.0000\n",
      "Executing OperationTransformation manually on test dataset...\n",
      "üîÑ Executing Transform(MinMaxScaler)\n",
      "‚ùå Manual execution failed: DatasetView.get_features() got an unexpected keyword argument 'concatenate'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\U108-N257\\AppData\\Local\\Temp\\ipykernel_17444\\1197854887.py\", line 30, in <module>\n",
      "    operation.execute(test_dataset, context)\n",
      "  File \"c:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\operations\\OperationTranformation.py\", line 55, in execute\n",
      "    X_fit = fit_view.get_features(concatenate=False)\n",
      "TypeError: DatasetView.get_features() got an unexpected keyword argument 'concatenate'\n"
     ]
    }
   ],
   "source": [
    "# Fix the manual test - use \"train\" partition (the default)\n",
    "print(\"=== FIXED MANUAL OPERATIONTRANSFORMATION TEST ===\")\n",
    "\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from PipelineContext import PipelineContext\n",
    "\n",
    "    # Create a fresh dataset for testing (copy of original)\n",
    "    test_dataset = SpectraDataset(task_type=\"regression\")\n",
    "    test_dataset.add_data([f1_source], reg_target_1)\n",
    "\n",
    "    print(\"Before transformation:\")\n",
    "    before_view = test_dataset.select()\n",
    "    before_features = before_view.get_features()\n",
    "    print(f\"  Min: {np.min(before_features):.4f}, Max: {np.max(before_features):.4f}\")\n",
    "\n",
    "    # Create scaler and operation with correct partition\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    operation = OperationTransformation(\n",
    "        transformer=scaler,\n",
    "        fit_partition=\"train\",  # Use \"train\" partition (default)\n",
    "        transform_partitions=None  # Transform all partitions\n",
    "    )\n",
    "\n",
    "    # Create context\n",
    "    context = PipelineContext()\n",
    "\n",
    "    print(\"Executing OperationTransformation manually on test dataset...\")\n",
    "    operation.execute(test_dataset, context)\n",
    "    print(\"‚úÖ Manual execution completed!\")\n",
    "\n",
    "    print(\"After transformation:\")\n",
    "    after_view = test_dataset.select()\n",
    "    after_features = after_view.get_features()\n",
    "    print(f\"  Min: {np.min(after_features):.4f}, Max: {np.max(after_features):.4f}\")\n",
    "\n",
    "    # Verify transformation\n",
    "    expected_min, expected_max = 0.2, 0.8\n",
    "    actual_min, actual_max = np.min(after_features), np.max(after_features)\n",
    "    if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "        print('‚úÖ Manual OperationTransformation worked correctly!')\n",
    "    else:\n",
    "        print('‚ùå Manual OperationTransformation did NOT work correctly!')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Manual execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcee478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug sklearn transformer API\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "test = MinMaxScaler()\n",
    "print(\"Has get_params:\", hasattr(test, 'get_params'))\n",
    "if hasattr(test, 'get_params'):\n",
    "    print(\"get_params():\", test.get_params())\n",
    "else:\n",
    "    print(\"Alternative: str(test):\", str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the FIXED OperationTransformation\n",
    "print(\"=== TESTING FIXED OPERATIONTRANSFORMATION ===\")\n",
    "\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from PipelineContext import PipelineContext\n",
    "\n",
    "    # Create a fresh dataset for testing (copy of original)\n",
    "    test_dataset = SpectraDataset(task_type=\"regression\")\n",
    "    test_dataset.add_data([f1_source], reg_target_1)\n",
    "\n",
    "    print(\"Before transformation:\")\n",
    "    before_view = test_dataset.select()\n",
    "    before_features = before_view.get_features()\n",
    "    print(f\"  Min: {np.min(before_features):.4f}, Max: {np.max(before_features):.4f}\")\n",
    "\n",
    "    # Create scaler and operation with correct partition\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    operation = OperationTransformation(\n",
    "        transformer=scaler,\n",
    "        fit_partition=\"train\",  # Use \"train\" partition (default)\n",
    "        transform_partitions=None  # Transform all partitions\n",
    "    )\n",
    "\n",
    "    # Create context\n",
    "    context = PipelineContext()\n",
    "\n",
    "    print(\"Executing FIXED OperationTransformation manually on test dataset...\")\n",
    "    operation.execute(test_dataset, context)\n",
    "    print(\"‚úÖ FIXED Manual execution completed!\")\n",
    "\n",
    "    print(\"After transformation:\")\n",
    "    after_view = test_dataset.select()\n",
    "    after_features = after_view.get_features()\n",
    "    print(f\"  Min: {np.min(after_features):.4f}, Max: {np.max(after_features):.4f}\")\n",
    "\n",
    "    # Verify transformation\n",
    "    expected_min, expected_max = 0.2, 0.8\n",
    "    actual_min, actual_max = np.min(after_features), np.max(after_features)\n",
    "    if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "        print('‚úÖ FIXED OperationTransformation worked correctly!')\n",
    "    else:\n",
    "        print('‚ùå FIXED OperationTransformation did NOT work correctly!')\n",
    "        print(f'Expected range: [{expected_min}, {expected_max}]')\n",
    "        print(f'Actual range: [{actual_min:.6f}, {actual_max:.6f}]')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FIXED execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c92d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST: Complete pipeline end-to-end with FIXED OperationTransformation\n",
    "print(\"=== FINAL COMPLETE PIPELINE TEST ===\")\n",
    "\n",
    "try:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from PipelineRunner import PipelineRunner\n",
    "\n",
    "    # Create fresh dataset\n",
    "    final_dataset = SpectraDataset(task_type=\"regression\")\n",
    "    final_dataset.add_data([f1_source], reg_target_1)\n",
    "\n",
    "    print(\"Before pipeline:\")\n",
    "    before_view = final_dataset.select()\n",
    "    before_features = before_view.get_features()\n",
    "    print(f\"  Min: {np.min(before_features):.4f}, Max: {np.max(before_features):.4f}\")\n",
    "    print(f\"  Mean: {np.mean(before_features):.4f}, Std: {np.std(before_features):.4f}\")\n",
    "\n",
    "    # Create pipeline config with MinMaxScaler\n",
    "    config = { \"pipeline\": [ MinMaxScaler(feature_range=(0.2,0.8)) ] }\n",
    "    runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "\n",
    "    print(\"\\nRunning complete pipeline...\")\n",
    "    fitted_pipeline, history, tree = runner.run(final_dataset, config, verbose=True)\n",
    "    print(\"‚úÖ Complete pipeline finished!\")\n",
    "\n",
    "    print(\"\\nAfter pipeline:\")\n",
    "    after_view = final_dataset.select()\n",
    "    after_features = after_view.get_features()\n",
    "    print(f\"  Min: {np.min(after_features):.4f}, Max: {np.max(after_features):.4f}\")\n",
    "    print(f\"  Mean: {np.mean(after_features):.4f}, Std: {np.std(after_features):.4f}\")\n",
    "\n",
    "    # Verify transformation\n",
    "    expected_min, expected_max = 0.2, 0.8\n",
    "    actual_min, actual_max = np.min(after_features), np.max(after_features)\n",
    "\n",
    "    print(\"\\n=== PIPELINE VERIFICATION ===\")\n",
    "    if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "        print('‚úÖ COMPLETE PIPELINE WORKED CORRECTLY! MinMaxScaler transformation applied successfully.')\n",
    "    else:\n",
    "        print('‚ùå Pipeline transformation was NOT applied correctly!')\n",
    "        print(f'Expected range: [{expected_min}, {expected_max}]')\n",
    "        print(f'Actual range: [{actual_min:.6f}, {actual_max:.6f}]')\n",
    "\n",
    "    print(f\"\\nüìä Dataset transformation summary:\")\n",
    "    print(f\"   Before: min={np.min(before_features):.4f}, max={np.max(before_features):.4f}\")\n",
    "    print(f\"   After:  min={actual_min:.4f}, max={actual_max:.4f}\")\n",
    "    print(f\"   Expected: min={expected_min}, max={expected_max}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Complete pipeline failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
