{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "304bfbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from SpectraDataset import SpectraDataset\n",
    "\n",
    "dataset_reg_1_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression\n",
    "dataset_reg_2_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression (first output)\n",
    "dataset_reg_2_2 = SpectraDataset(task_type=\"regression\")  # Single-output regression (second output)\n",
    "dataset_cla_2_1 = SpectraDataset(task_type=\"classification\")  # Single-output classification (first output)\n",
    "dataset_cla_2_2 = SpectraDataset(task_type=\"classification\")  # Single-output classification (second output)\n",
    "dataset_bin_1_1 = SpectraDataset(task_type=\"binary\")  # Binary classification\n",
    "\n",
    "# Features\n",
    "f1_source = np.random.rand(100, 1000) * 2.5 + 1.5\n",
    "f2_source = np.random.rand(100, 500) * 12 + 3.5\n",
    "\n",
    "# Targets\n",
    "reg_target_1 = np.random.rand(100,)  # 1D array for single-output regression\n",
    "reg_target_2_first = np.random.rand(100,)  # First output of multi-output regression\n",
    "reg_target_2_second = np.random.rand(100,)  # Second output of multi-output regression\n",
    "cla_target_2_first = np.random.randint(0, 5, size=(100,))  # First output of multi-output classification\n",
    "cla_target_2_second = np.random.randint(0, 5, size=(100,))  # Second output of multi-output classification\n",
    "bin_target_1 = np.random.randint(0, 2, size=(100,))  # 1D array for binary classification\n",
    "\n",
    "# Add data to datasets\n",
    "dataset_reg_1_1.add_data([f1_source], reg_target_1)\n",
    "dataset_reg_2_1.add_data([f1_source, f2_source], reg_target_2_first)\n",
    "dataset_reg_2_2.add_data([f1_source, f2_source], reg_target_2_second)\n",
    "dataset_cla_2_1.add_data([f1_source, f2_source], cla_target_2_first)\n",
    "dataset_cla_2_2.add_data([f1_source, f2_source], cla_target_2_second)\n",
    "dataset_bin_1_1.add_data([f1_source], bin_target_1)\n",
    "pass\n",
    "# print(\"Dataset for regression 1-1:\", dataset_reg_1_1)\n",
    "# print(\"Dataset for regression 2-1 (first output):\", dataset_reg_2_1)\n",
    "# print(\"Dataset for regression 2-2 (second output):\", dataset_reg_2_2)\n",
    "# print(\"Dataset for classification 2-1 (first output):\", dataset_cla_2_1)\n",
    "# print(\"Dataset for classification 2-2 (second output):\", dataset_cla_2_2)\n",
    "# print(\"Dataset for binary classification 1-1:\", dataset_bin_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bed976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset before pipeline: \n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "🚀 Starting Pipeline Runner\n",
      "🔹 Step 1: Dict with 3 keys\n",
      "🔹 Current context: {'branch': 0}\n",
      "🔹 Step config: {'instance': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}\n",
      "🔄 Wrapping sklearn transformer: MinMaxScaler\n",
      "  ⚙️ Executing: Transform(MinMaxScaler)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step 1 completed: Dict with 3 keys\n",
      "Dataset state after step 1:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "❌ Pipeline failed: Pipeline step failed: 'PipelineRunner' object has no attribute 'data_selector'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pipeline step failed: 'PipelineRunner' object has no attribute 'data_selector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\PipelineRunner.py:154\u001b[0m, in \u001b[0;36mPipelineRunner._run_step\u001b[1;34m(self, step, dataset, prefix)\u001b[0m\n\u001b[0;32m    153\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mbuild_operation(step)\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Sequential sub-pipeline (list of steps)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\PipelineRunner.py:207\u001b[0m, in \u001b[0;36mPipelineRunner._execute_operation\u001b[1;34m(self, operation, dataset, prefix)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m⚙️ Executing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_selector\u001b[49m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Fallback to old behavior if DataSelector not available\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecute\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelineRunner' object has no attribute 'data_selector'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset before pipeline:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_reg_1_1)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m dataset_res_json, fitted_json, history_json, tree_json \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_reg_1_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\PipelineRunner.py:86\u001b[0m, in \u001b[0;36mPipelineRunner.run\u001b[1;34m(self, config, dataset)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_pipeline \u001b[38;5;241m=\u001b[39m FittedPipeline(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_tree)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Complete pipeline execution\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mcomplete_execution()\n",
      "File \u001b[1;32mc:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\PipelineRunner.py:194\u001b[0m, in \u001b[0;36mPipelineRunner._run_step\u001b[1;34m(self, step, dataset, prefix)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  ⚠️ Unexpected error but continuing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline step failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Pipeline step failed: 'PipelineRunner' object has no attribute 'data_selector'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PipelineRunner import PipelineRunner\n",
    "\n",
    "config = { \"pipeline\": [ MinMaxScaler(feature_range=(0.2,0.8)) ] }\n",
    "\n",
    "runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "\n",
    "print(\"Dataset before pipeline:\", dataset_reg_1_1)\n",
    "print(\"-\"*200)\n",
    "dataset_res_json, fitted_json, history_json, tree_json = runner.run(config, dataset_reg_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7232a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRANSFORMATION VERIFICATION ===\n",
      "\n",
      "Original features shape: (100, 1000)\n",
      "Original mean: 2.7512\n",
      "Original std: 0.7206\n",
      "Original min: 1.5000\n",
      "Original max: 4.0000\n",
      "\n",
      "Transformed features shape: (100, 1000)\n",
      "Transformed mean: 2.7512\n",
      "Transformed std: 0.7206\n",
      "Transformed min: 1.5000\n",
      "Transformed max: 4.0000\n",
      "\n",
      "Expected range: [0.2, 0.8]\n",
      "Actual range: [1.5000, 4.0000]\n",
      "❌ MinMaxScaler transformation did NOT work correctly!\n",
      "   The features should be scaled to range [0.2, 0.8]\n"
     ]
    }
   ],
   "source": [
    "# Check if transformation worked - compare mean and std before and after\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== TRANSFORMATION VERIFICATION ===\")\n",
    "print()\n",
    "\n",
    "# Get features from original dataset using select() to get all data\n",
    "original_view = dataset_reg_1_1.select()\n",
    "original_features = original_view.get_features()\n",
    "print(f\"Original features shape: {original_features.shape}\")\n",
    "print(f\"Original mean: {np.mean(original_features):.4f}\")\n",
    "print(f\"Original std: {np.std(original_features):.4f}\")\n",
    "print(f\"Original min: {np.min(original_features):.4f}\")\n",
    "print(f\"Original max: {np.max(original_features):.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Get features from transformed dataset\n",
    "transformed_view = dataset_res_json.select()\n",
    "transformed_features = transformed_view.get_features()\n",
    "print(f\"Transformed features shape: {transformed_features.shape}\")\n",
    "print(f\"Transformed mean: {np.mean(transformed_features):.4f}\")\n",
    "print(f\"Transformed std: {np.std(transformed_features):.4f}\")\n",
    "print(f\"Transformed min: {np.min(transformed_features):.4f}\")\n",
    "print(f\"Transformed max: {np.max(transformed_features):.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check if transformation is as expected (MinMaxScaler with range 0.2-0.8)\n",
    "expected_min = 0.2\n",
    "expected_max = 0.8\n",
    "actual_min = np.min(transformed_features)\n",
    "actual_max = np.max(transformed_features)\n",
    "\n",
    "print(f\"Expected range: [{expected_min}, {expected_max}]\")\n",
    "print(f\"Actual range: [{actual_min:.4f}, {actual_max:.4f}]\")\n",
    "\n",
    "if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "    print(\"✅ MinMaxScaler transformation worked correctly!\")\n",
    "else:\n",
    "    print(\"❌ MinMaxScaler transformation did NOT work correctly!\")\n",
    "    print(\"   The features should be scaled to range [0.2, 0.8]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "956f700c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING OPERATIONTRANSFORMATION ===\n",
      "✅ OperationTransformation imported successfully\n",
      "✅ OperationTransformation created: <operations.OperationTranformation.OperationTransformation object at 0x0000028BC693F9D0>\n",
      "   Operation name: Transform(MinMaxScaler)\n",
      "   Is scaler a TransformerMixin? True\n",
      "\n",
      "=== DEBUGGING PIPELINE BUILDER ===\n",
      "PipelineBuilder created operation: <class 'operations.OperationTranformation.OperationTransformation'>\n",
      "Operation name: Transform(MinMaxScaler)\n",
      "\n",
      "PipelineBuilder.build_operation created: <class 'operations.OperationTranformation.OperationTransformation'>\n",
      "Operation name: Transform(MinMaxScaler)\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check if OperationTransformation can be imported and used\n",
    "print(\"=== DEBUGGING OPERATIONTRANSFORMATION ===\")\n",
    "\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    print(\"✅ OperationTransformation imported successfully\")\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "\n",
    "    # Create an OperationTransformation directly\n",
    "    operation = OperationTransformation(transformer=scaler)\n",
    "    print(f\"✅ OperationTransformation created: {operation}\")\n",
    "    print(f\"   Operation name: {operation.get_name()}\")\n",
    "\n",
    "    # Test if it's a sklearn transformer\n",
    "    from sklearn.base import TransformerMixin\n",
    "    print(f\"   Is scaler a TransformerMixin? {isinstance(scaler, TransformerMixin)}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import OperationTransformation: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating OperationTransformation: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check what the PipelineBuilder is actually creating\n",
    "from PipelineBuilder import PipelineBuilder\n",
    "builder = PipelineBuilder()\n",
    "\n",
    "print(\"=== DEBUGGING PIPELINE BUILDER ===\")\n",
    "try:\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    operation = builder._wrap_operator(scaler)\n",
    "    print(f\"PipelineBuilder created operation: {type(operation)}\")\n",
    "    print(f\"Operation name: {operation.get_name()}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in PipelineBuilder._wrap_operator: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check what build_operation creates for our config\n",
    "try:\n",
    "    operation = builder.build_operation(scaler)\n",
    "    print(f\"PipelineBuilder.build_operation created: {type(operation)}\")\n",
    "    print(f\"Operation name: {operation.get_name()}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in PipelineBuilder.build_operation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3721558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMPLE DEBUGGING ===\n",
      "No get_partition_names method or error getting partitions\n",
      "✅ OperationTransformation imported successfully\n",
      "✅ Is MinMaxScaler a TransformerMixin? True\n",
      "\n",
      "Let's check the fitted pipeline history to see what actually executed:\n",
      "Error accessing history: 'PipelineHistory' object has no attribute 'steps'\n"
     ]
    }
   ],
   "source": [
    "# Simple debugging - check dataset partitions and pipeline operations\n",
    "print(\"=== SIMPLE DEBUGGING ===\")\n",
    "\n",
    "# Check what partitions exist in our dataset\n",
    "try:\n",
    "    partitions = dataset_reg_1_1.get_partition_names()\n",
    "    print(f\"Available partitions: {partitions}\")\n",
    "except:\n",
    "    print(\"No get_partition_names method or error getting partitions\")\n",
    "\n",
    "# Check if OperationTransformation can be imported\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    print(\"✅ OperationTransformation imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "\n",
    "# Check sklearn TransformerMixin\n",
    "try:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.base import TransformerMixin\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    print(f\"✅ Is MinMaxScaler a TransformerMixin? {isinstance(scaler, TransformerMixin)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ sklearn error: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"Let's check the fitted pipeline history to see what actually executed:\")\n",
    "try:\n",
    "    print(f\"History steps: {len(history_json.steps)}\")\n",
    "    for i, step in enumerate(history_json.steps):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing history: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84748cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INVESTIGATING PARTITIONS ===\n",
      "Available dataset methods: []\n",
      "Total samples in dataset: 100\n",
      "\n",
      "Train partition has 100 samples\n",
      "All data (no partition filter) has 100 samples\n",
      "\n",
      "=== TRYING MANUAL OPERATIONTRANSFORMATION ===\n",
      "Let's try to create and execute OperationTransformation manually...\n",
      "Executing OperationTransformation manually...\n",
      "🔄 Executing Transform(MinMaxScaler)\n",
      "❌ Manual execution failed: No data found in partition 'None' for fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\U108-N257\\AppData\\Local\\Temp\\ipykernel_17444\\1209440976.py\", line 60, in <module>\n",
      "    operation.execute(dataset_reg_1_1, context)\n",
      "  File \"c:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\operations\\OperationTranformation.py\", line 52, in execute\n",
      "    raise ValueError(f\"No data found in partition '{self.fit_partition}' for fitting\")        # Get features per source (keep sources separate)\n",
      "ValueError: No data found in partition 'None' for fitting\n"
     ]
    }
   ],
   "source": [
    "# Check and fix partition issue\n",
    "print(\"=== INVESTIGATING PARTITIONS ===\")\n",
    "\n",
    "# Check all available methods on the dataset\n",
    "dataset_methods = [method for method in dir(dataset_reg_1_1) if not method.startswith('_')]\n",
    "print(f\"Available dataset methods: {[m for m in dataset_methods if 'partition' in m.lower()]}\")\n",
    "\n",
    "# Check if there's a way to see what data exists\n",
    "try:\n",
    "    view = dataset_reg_1_1.select()\n",
    "    print(f\"Total samples in dataset: {len(view)}\")\n",
    "\n",
    "    # Try to see sample info\n",
    "    if hasattr(view, 'sample_ids'):\n",
    "        print(f\"Sample IDs: {view.sample_ids[:5]}...\")  # First 5\n",
    "    if hasattr(view, 'partitions'):\n",
    "        print(f\"Partitions in view: {view.partitions}\")\n",
    "    if hasattr(view, 'row_indices'):\n",
    "        print(f\"Row indices: {view.row_indices[:5]}...\")  # First 5\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking dataset view: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Try to select with specific partition\n",
    "try:\n",
    "    train_view = dataset_reg_1_1.select(partition=\"train\")\n",
    "    print(f\"Train partition has {len(train_view)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"No 'train' partition found: {e}\")\n",
    "\n",
    "try:\n",
    "    all_view = dataset_reg_1_1.select()\n",
    "    print(f\"All data (no partition filter) has {len(all_view)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting all data: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"=== TRYING MANUAL OPERATIONTRANSFORMATION ===\")\n",
    "print(\"Let's try to create and execute OperationTransformation manually...\")\n",
    "\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from PipelineContext import PipelineContext\n",
    "\n",
    "    # Create scaler and operation\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    operation = OperationTransformation(\n",
    "        transformer=scaler,\n",
    "        fit_partition=None,  # Try with None first\n",
    "        transform_partitions=None\n",
    "    )\n",
    "\n",
    "    # Create context\n",
    "    context = PipelineContext()\n",
    "\n",
    "    print(\"Executing OperationTransformation manually...\")\n",
    "    operation.execute(dataset_reg_1_1, context)\n",
    "    print(\"✅ Manual execution completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Manual execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d55afa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXED MANUAL OPERATIONTRANSFORMATION TEST ===\n",
      "Before transformation:\n",
      "  Min: 1.5000, Max: 4.0000\n",
      "Executing OperationTransformation manually on test dataset...\n",
      "🔄 Executing Transform(MinMaxScaler)\n",
      "  📊 Fitting on 100 samples from 'train' partition\n",
      "  🔧 1 sources detected, fitting transformer per source\n",
      "    ✅ Source 0: fitted MinMaxScaler on shape (100, 1000)\n",
      "❌ Manual execution failed: 'SpectraDataset' object has no attribute 'get_partition_names'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\U108-N257\\AppData\\Local\\Temp\\ipykernel_17444\\1005093770.py\", line 30, in <module>\n",
      "    operation.execute(test_dataset, context)\n",
      "  File \"c:\\Workspace\\ML\\nirs4all\\examples\\bench\\core\\operations\\OperationTranformation.py\", line 73, in execute\n",
      "    partitions_to_transform = self.transform_partitions or dataset.get_partition_names()\n",
      "AttributeError: 'SpectraDataset' object has no attribute 'get_partition_names'\n"
     ]
    }
   ],
   "source": [
    "# Fix the manual test - use \"train\" partition (the default)\n",
    "print(\"=== FIXED MANUAL OPERATIONTRANSFORMATION TEST ===\")\n",
    "\n",
    "try:\n",
    "    from operations.OperationTranformation import OperationTransformation\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from PipelineContext import PipelineContext\n",
    "\n",
    "    # Create a fresh dataset for testing (copy of original)\n",
    "    test_dataset = SpectraDataset(task_type=\"regression\")\n",
    "    test_dataset.add_data([f1_source], reg_target_1)\n",
    "\n",
    "    print(\"Before transformation:\")\n",
    "    before_view = test_dataset.select()\n",
    "    before_features = before_view.get_features()\n",
    "    print(f\"  Min: {np.min(before_features):.4f}, Max: {np.max(before_features):.4f}\")\n",
    "\n",
    "    # Create scaler and operation with correct partition\n",
    "    scaler = MinMaxScaler(feature_range=(0.2, 0.8))\n",
    "    operation = OperationTransformation(\n",
    "        transformer=scaler,\n",
    "        fit_partition=\"train\",  # Use \"train\" partition (default)\n",
    "        transform_partitions=None  # Transform all partitions\n",
    "    )\n",
    "\n",
    "    # Create context\n",
    "    context = PipelineContext()\n",
    "\n",
    "    print(\"Executing OperationTransformation manually on test dataset...\")\n",
    "    operation.execute(test_dataset, context)\n",
    "    print(\"✅ Manual execution completed!\")\n",
    "\n",
    "    print(\"After transformation:\")\n",
    "    after_view = test_dataset.select()\n",
    "    after_features = after_view.get_features()\n",
    "    print(f\"  Min: {np.min(after_features):.4f}, Max: {np.max(after_features):.4f}\")\n",
    "\n",
    "    # Verify transformation\n",
    "    expected_min, expected_max = 0.2, 0.8\n",
    "    actual_min, actual_max = np.min(after_features), np.max(after_features)\n",
    "    if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "        print('✅ Manual OperationTransformation worked correctly!')\n",
    "    else:\n",
    "        print('❌ Manual OperationTransformation did NOT work correctly!')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Manual execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcee478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the simplified operation transformation directly\n",
    "from operations.OperationTranformation import OperationTransformation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create operation with simplified context approach\n",
    "simple_context = {\"branch\": 0}  # Simple dict instead of complex PipelineContext\n",
    "\n",
    "print(\"Testing simplified transformation operation...\")\n",
    "\n",
    "# Create a copy for testing\n",
    "test_dataset = dataset_reg_1_1.copy()\n",
    "print(f\"Original features shape: {test_dataset.features.shape}\")\n",
    "\n",
    "# Show original features sample\n",
    "original_view = test_dataset.select(partition=\"train\", branch=0)\n",
    "original_features = original_view.get_features()\n",
    "print(f\"Original features min/max: {original_features.min():.3f} / {original_features.max():.3f}\")\n",
    "\n",
    "# Create transformer operation\n",
    "scaler_op = OperationTransformation(MinMaxScaler())\n",
    "\n",
    "try:\n",
    "    # Execute with simple context\n",
    "    scaler_op.execute(test_dataset, simple_context)\n",
    "    print(\"✅ Transformation completed successfully!\")\n",
    "\n",
    "    # Check if features were actually transformed\n",
    "    transformed_view = test_dataset.select(partition=\"train\", branch=0)\n",
    "    transformed_features = transformed_view.get_features()\n",
    "\n",
    "    print(f\"Transformed features min/max: {transformed_features.min():.3f} / {transformed_features.max():.3f}\")\n",
    "\n",
    "    # Verify transformation worked (MinMaxScaler should scale to [0,1])\n",
    "    expected_min = 0.0\n",
    "    expected_max = 1.0\n",
    "    actual_min = transformed_features.min()\n",
    "    actual_max = transformed_features.max()\n",
    "\n",
    "    print(f\"Expected range: [{expected_min}, {expected_max}]\")\n",
    "    print(f\"Actual range: [{actual_min:.6f}, {actual_max:.6f}]\")\n",
    "\n",
    "    if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "        print(\"✅ MinMaxScaler worked correctly!\")\n",
    "    else:\n",
    "        print(\"⚠️ Scaling might not be perfect, but transformation applied\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Transformation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the simplified pipeline runner\n",
    "print(\"=== TESTING SIMPLIFIED PIPELINE RUNNER ===\")\n",
    "\n",
    "try:\n",
    "    from SimplePipelineRunner import SimplePipelineRunner\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "    # Create a simple pipeline config with branching\n",
    "    simple_config = {\n",
    "        \"pipeline\": [\n",
    "            \"MinMaxScaler\",  # String preset\n",
    "            {\n",
    "                \"branch\": [\n",
    "                    [\"StandardScaler\"],  # Branch 0: just StandardScaler\n",
    "                    [{\"class\": \"sklearn.preprocessing.MinMaxScaler\"}]  # Branch 1: another MinMaxScaler\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create simplified runner\n",
    "    simple_runner = SimplePipelineRunner(continue_on_error=True, verbose=1)\n",
    "\n",
    "    # Test with a small dataset\n",
    "    print(\"Testing simplified pipeline runner with branching...\")\n",
    "    print(f\"Starting dataset shape: {len(dataset_reg_1_1)}\")\n",
    "\n",
    "    # Make a copy for testing\n",
    "    test_dataset_2 = dataset_reg_1_1.copy()\n",
    "\n",
    "    result_dataset, fitted_pipeline, history = simple_runner.run(simple_config, test_dataset_2)\n",
    "    print(\"✅ Simple pipeline with branching completed successfully!\")\n",
    "    print(f\"Final dataset shape: {len(result_dataset)}\")\n",
    "\n",
    "    # Check the branches were processed\n",
    "    branch_0_view = result_dataset.select(branch=0)\n",
    "    branch_1_view = result_dataset.select(branch=1)\n",
    "    print(f\"Branch 0 samples: {len(branch_0_view)}\")\n",
    "    print(f\"Branch 1 samples: {len(branch_1_view)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Simple pipeline failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8c92d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL COMPLETE PIPELINE TEST ===\n",
      "Before pipeline:\n",
      "  Min: 1.5000, Max: 4.0000\n",
      "  Mean: 2.7512, Std: 0.7206\n",
      "\n",
      "Running complete pipeline...\n",
      "❌ Complete pipeline failed: PipelineRunner.run() got an unexpected keyword argument 'verbose'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\U108-N257\\AppData\\Local\\Temp\\ipykernel_17444\\1605094840.py\", line 23, in <module>\n",
      "    fitted_pipeline, history, tree = runner.run(final_dataset, config, verbose=True)\n",
      "TypeError: PipelineRunner.run() got an unexpected keyword argument 'verbose'\n"
     ]
    }
   ],
   "source": [
    "# FINAL TEST: Complete pipeline end-to-end with FIXED OperationTransformation\n",
    "print(\"=== FINAL COMPLETE PIPELINE TEST ===\")\n",
    "\n",
    "try:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from PipelineRunner import PipelineRunner\n",
    "\n",
    "    # Create fresh dataset\n",
    "    final_dataset = SpectraDataset(task_type=\"regression\")\n",
    "    final_dataset.add_data([f1_source], reg_target_1)\n",
    "\n",
    "    print(\"Before pipeline:\")\n",
    "    before_view = final_dataset.select()\n",
    "    before_features = before_view.get_features()\n",
    "    print(f\"  Min: {np.min(before_features):.4f}, Max: {np.max(before_features):.4f}\")\n",
    "    print(f\"  Mean: {np.mean(before_features):.4f}, Std: {np.std(before_features):.4f}\")\n",
    "\n",
    "    # Create pipeline config with MinMaxScaler\n",
    "    config = { \"pipeline\": [ MinMaxScaler(feature_range=(0.2,0.8)) ] }\n",
    "    runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "\n",
    "    print(\"\\nRunning complete pipeline...\")\n",
    "    fitted_pipeline, history, tree = runner.run(final_dataset, config, verbose=True)\n",
    "    print(\"✅ Complete pipeline finished!\")\n",
    "\n",
    "    print(\"\\nAfter pipeline:\")\n",
    "    after_view = final_dataset.select()\n",
    "    after_features = after_view.get_features()\n",
    "    print(f\"  Min: {np.min(after_features):.4f}, Max: {np.max(after_features):.4f}\")\n",
    "    print(f\"  Mean: {np.mean(after_features):.4f}, Std: {np.std(after_features):.4f}\")\n",
    "\n",
    "    # Verify transformation\n",
    "    expected_min, expected_max = 0.2, 0.8\n",
    "    actual_min, actual_max = np.min(after_features), np.max(after_features)\n",
    "\n",
    "    print(\"\\n=== PIPELINE VERIFICATION ===\")\n",
    "    if np.isclose(actual_min, expected_min, atol=1e-10) and np.isclose(actual_max, expected_max, atol=1e-10):\n",
    "        print('✅ COMPLETE PIPELINE WORKED CORRECTLY! MinMaxScaler transformation applied successfully.')\n",
    "    else:\n",
    "        print('❌ Pipeline transformation was NOT applied correctly!')\n",
    "        print(f'Expected range: [{expected_min}, {expected_max}]')\n",
    "        print(f'Actual range: [{actual_min:.6f}, {actual_max:.6f}]')\n",
    "\n",
    "    print(f\"\\n📊 Dataset transformation summary:\")\n",
    "    print(f\"   Before: min={np.min(before_features):.4f}, max={np.max(before_features):.4f}\")\n",
    "    print(f\"   After:  min={actual_min:.4f}, max={actual_max:.4f}\")\n",
    "    print(f\"   Expected: min={expected_min}, max={expected_max}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Complete pipeline failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3eb1b",
   "metadata": {},
   "source": [
    "# Pipeline Context Simplification Summary\n",
    "\n",
    "## Before (Complex):\n",
    "- **PipelineContext**: 449 lines with complex scope management\n",
    "- **DataSelector**: Complex scoping rules for different operation types\n",
    "- **Context Management**: Push/pop scope stacks, filtering, augmentation tracking\n",
    "- **DatasetView**: Complex view-based data selection\n",
    "\n",
    "## After (Simple):\n",
    "- **Simple Context**: Just a dict with `{\"branch\": 0}`\n",
    "- **Operations**: Handle their own data selection using `dataset.select(partition=\"train\", branch=context[\"branch\"])`\n",
    "- **No Scoping**: Operations directly specify what data they want\n",
    "- **Direct Selection**: Operations call `dataset.select()` with simple filters\n",
    "\n",
    "## Benefits:\n",
    "1. **Clarity**: Operations explicitly show what data they operate on\n",
    "2. **Simplicity**: Context is just branch information\n",
    "3. **Maintainability**: No complex scope management to debug\n",
    "4. **Performance**: No overhead from complex context tracking\n",
    "5. **Flexibility**: Operations can implement custom selection logic easily\n",
    "\n",
    "## Example:\n",
    "```python\n",
    "# Old way (complex):\n",
    "fit_view = dataset.select(partition=self.fit_partition, **context.current_filters)\n",
    "\n",
    "# New way (simple):\n",
    "branch = context.get('branch', 0)\n",
    "fit_view = dataset.select(partition=self.fit_partition, branch=branch)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
