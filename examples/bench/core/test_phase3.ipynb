{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ae5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "================================================================================\n",
      "TESTING ENHANCED DATASET OPERATIONS\n",
      "================================================================================\n",
      "Initial Dataset: \n",
      "Source 0: 100x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 100x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "\n",
      "1. SAMPLE AUGMENTATION\n",
      "----------------------------------------\n",
      "Created 100 new samples\n",
      "After Sample Augmentation: \n",
      "Source 0: 200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 200 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw', 'sample_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "\n",
      "2. FEATURE AUGMENTATION\n",
      "----------------------------------------\n",
      "Rows: 200 -> 400 (ratio: 2.0)\n",
      "After Feature Augmentation: \n",
      "Source 0: 400x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 400x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 400, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 400 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw', 'sample_aug', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "\n",
      "3. BRANCHING\n",
      "----------------------------------------\n",
      "Train rows: 400 -> 1200 (ratio: 3.0)\n",
      "After Branching: \n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['raw', 'sample_aug', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "\n",
      "4. FEATURE EXTRACTION\n",
      "----------------------------------------\n",
      "Branch 0, raw processing: (100, 1500)\n",
      "All branches, all processing: (1200, 1500)\n",
      "3D features (branch 1, feature_aug): (200, 1500, 1)\n",
      "\n",
      "================================================================================\n",
      "ENHANCED OPERATIONS TEST COMPLETED!\n",
      "================================================================================\n",
      "\n",
      "Running simple pipeline with enhanced dataset operations...\n",
      "🚀 Starting Pipeline Runner\n",
      "🔹 Step 1: Dict with 2 keys\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['sample_aug', 'raw', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "  ⚙️ Executing: Generic(MinMaxScaler)\n",
      "🔹 Step 2: feature_augmentation\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['sample_aug', 'feature_aug', 'raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "  🔄 Feature augmentation with 2 augmenters\n",
      "    📊 Base train set: 1200 samples\n",
      "    🔀 Running 2 augmenters in parallel (max_workers=-1)\n",
      "    📌 Augmenter 1/2\n",
      "      ⚙️ Executing: Generic(NoneType)\n",
      "    ✅ Augmenter 1 completed\n",
      "    📌 Augmenter 2/2\n",
      "      ⚙️ Executing: Generic(SavitzkyGolay)\n",
      "    ✅ Augmenter 2 completed\n",
      "🔹 Step 3: sample_augmentation\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['feature_aug', 'raw', 'sample_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "  📊 Sample augmentation with 1 augmenters\n",
      "    📌 Augmenter 1/1\n",
      "      ⚙️ Executing: Generic(Rotate_Translate)\n",
      "🔹 Step 4: Dict with 2 keys\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['sample_aug', 'feature_aug', 'raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "  ⚙️ Executing: Generic(ShuffleSplit)\n",
      "🔹 Step 5: cluster\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['raw', 'sample_aug', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "  🔘 Cluster: KMeans(n_clusters=5, random_state=42)\n",
      "    ⚙️ Executing: Generic(KMeans)\n",
      "🔹 Step 6: Dict with 3 keys\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['feature_aug', 'raw', 'sample_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "  ⚙️ Executing: Generic(RepeatedStratifiedKFold)\n",
      "🔹 Step 7: branch\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['raw', 'sample_aug', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "  🌿 Branch with 2 paths\n",
      "    🔀 Branch 1\n",
      "      🔹 Step 8: Sub-pipeline (2 steps)\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['sample_aug', 'feature_aug', 'raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "        📁 Sub-pipeline with 2 steps\n",
      "          🔹 Step 9: RobustScaler\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['sample_aug', 'raw', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "            ⚙️ Executing: Generic(RobustScaler)\n",
      "          🔹 Step 10: Dict with 2 keys\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['raw', 'sample_aug', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "            🤖 Model: {'model': RandomForestClassifier(max_depth=10, random_state=42), 'y_pipeline': <class 'sklearn.preprocessing._data.StandardScaler'>}\n",
      "              ⚙️ Executing: Generic(RandomForestClassifier)\n",
      "    🔀 Branch 2\n",
      "      🔹 Step 11: Dict with 2 keys\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['feature_aug', 'sample_aug', 'raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "        🤖 Model: {'model': <function decon at 0x000002A84684A5F0>, 'y_pipeline': StandardScaler()}\n",
      "          ⚙️ Executing: Generic(function)\n",
      "✅ Pipeline completed successfully\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['sample_aug', 'feature_aug', 'raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "        📁 Sub-pipeline with 2 steps\n",
      "          🔹 Step 9: RobustScaler\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['sample_aug', 'raw', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "            ⚙️ Executing: Generic(RobustScaler)\n",
      "          🔹 Step 10: Dict with 2 keys\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['raw', 'sample_aug', 'feature_aug']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "            🤖 Model: {'model': RandomForestClassifier(max_depth=10, random_state=42), 'y_pipeline': <class 'sklearn.preprocessing._data.StandardScaler'>}\n",
      "              ⚙️ Executing: Generic(RandomForestClassifier)\n",
      "    🔀 Branch 2\n",
      "      🔹 Step 11: Dict with 2 keys\n",
      "\n",
      "Source 0: 1200x1000 Mean: 6.00, Std: 0.12\n",
      "Source 1: 1200x500 Mean: 26.07, Std: 1.14\n",
      "\n",
      "Samples: 200, Rows: 1200, Features: 2\n",
      "Partitions: ['train']\n",
      "  train: 1200 samples\n",
      "Groups: [0] - Branches: [0, 1, 2] - Processing: ['feature_aug', 'sample_aug', 'raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7], 'n_samples': 200}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "        🤖 Model: {'model': <function decon at 0x000002A84684A5F0>, 'y_pipeline': StandardScaler()}\n",
      "          ⚙️ Executing: Generic(function)\n",
      "✅ Pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "from SpectraDataset import SpectraDataset\n",
    "from PipelineRunner import PipelineRunner\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, ShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nirs4all.presets.ref_models import decon\n",
    "from nirs4all.transformations import (\n",
    "    Gaussian as GS,\n",
    "    Rotate_Translate as RT,\n",
    "    SavitzkyGolay as SG,\n",
    "    StandardNormalVariate as SNV,\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING ENHANCED DATASET OPERATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create initial dataset with two sources\n",
    "np.random.seed(42)\n",
    "source_1 = np.random.rand(100, 1000) * 4 + 4\n",
    "source_2 = np.random.rand(100, 500) * 40 + 6\n",
    "targets = np.random.randint(0, 8, size=100)\n",
    "\n",
    "dataset = SpectraDataset(task_type=\"classification\")\n",
    "dataset.add_data(features=[source_1, source_2], targets=targets)\n",
    "print(\"Initial Dataset:\", dataset)\n",
    "\n",
    "# Test 1: Sample Augmentation\n",
    "print(\"\\n1. SAMPLE AUGMENTATION\")\n",
    "print(\"-\" * 40)\n",
    "new_samples = dataset.sample_augmentation(n_copies=1, processing_tag=\"sample_aug\")\n",
    "print(f\"Created {len(new_samples)} new samples\")\n",
    "print(\"After Sample Augmentation:\", dataset)\n",
    "\n",
    "# Test 2: Feature Augmentation\n",
    "print(\"\\n2. FEATURE AUGMENTATION\")\n",
    "print(\"-\" * 40)\n",
    "rows_before = len(dataset.indices)\n",
    "dataset.feature_augmentation(processing_tag=\"feature_aug\")\n",
    "rows_after = len(dataset.indices)\n",
    "print(f\"Rows: {rows_before} -> {rows_after} (ratio: {rows_after/rows_before:.1f})\")\n",
    "print(\"After Feature Augmentation:\", dataset)\n",
    "\n",
    "# Test 3: Branching\n",
    "print(\"\\n3. BRANCHING\")\n",
    "print(\"-\" * 40)\n",
    "train_rows_before = len(dataset.indices.filter(dataset.indices['partition'] == 'train'))\n",
    "dataset.branch_dataset(n_branches=3)\n",
    "train_rows_after = len(dataset.indices.filter(dataset.indices['partition'] == 'train'))\n",
    "print(f\"Train rows: {train_rows_before} -> {train_rows_after} (ratio: {train_rows_after/train_rows_before:.1f})\")\n",
    "print(\"After Branching:\", dataset)\n",
    "\n",
    "# Test 4: Feature extraction in different formats\n",
    "print(\"\\n4. FEATURE EXTRACTION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 2D features for specific branch and processing\n",
    "features_branch0 = dataset.get_features_2d(filters={'branch': 0, 'processing': 'raw'})\n",
    "print(f\"Branch 0, raw processing: {features_branch0.shape}\")\n",
    "\n",
    "features_all_branches = dataset.get_features_2d(filters={'partition': 'train'})\n",
    "print(f\"All branches, all processing: {features_all_branches.shape}\")\n",
    "\n",
    "# 3D features\n",
    "features_3d = dataset.get_features_3d(filters={'branch': 1, 'processing': 'feature_aug'})\n",
    "print(f\"3D features (branch 1, feature_aug): {features_3d.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED OPERATIONS TEST COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Now test with a simple pipeline\n",
    "pipeline_config = {\n",
    "    \"pipeline\": [\n",
    "        MinMaxScaler,\n",
    "        {\"feature_augmentation\": [None, SG]},\n",
    "        {\"sample_augmentation\": [RT]},\n",
    "        ShuffleSplit,\n",
    "        {\"cluster\": KMeans(n_clusters=5, random_state=42)},\n",
    "        RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42),\n",
    "        {\n",
    "            \"branch\": [\n",
    "                [\n",
    "                    RobustScaler(),\n",
    "                    {\n",
    "                        \"model\": RandomForestClassifier(random_state=42, max_depth=10),\n",
    "                        \"y_pipeline\": StandardScaler,\n",
    "                    },\n",
    "                ],\n",
    "                {\n",
    "                    \"model\": decon,\n",
    "                    \"y_pipeline\": StandardScaler(),\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nRunning simple pipeline with enhanced dataset operations...\")\n",
    "runner = PipelineRunner()\n",
    "result_dataset, fitted_pipeline, history, fitted_tree = runner.run(pipeline_config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "MONITORING DATASET OPERATIONS\n",
      "========================================================================================================================\n",
      "Initial Dataset           | Rows:   50 | Samples:  50 | Features: 2 sources       | Partitions: ['train'] | Branches: [0] | Processing: ['raw']\n",
      "Added Test Set            | Rows:   70 | Samples:  70 | Features: 2 sources       | Partitions: ['test', 'train'] | Branches: [0] | Processing: ['raw']\n",
      "Sample Augmentation       | Rows:  120 | Samples: 120 | Features: 2 sources       | Partitions: ['test', 'train'] | Branches: [0] | Processing: ['augmented', 'raw']\n",
      "Feature Augmentation      | Rows:  240 | Samples: 120 | Features: 2 sources       | Partitions: ['train', 'test'] | Branches: [0] | Processing: ['raw', 'feat_aug', 'augmented']\n",
      "Branching (2 branches)    | Rows:  440 | Samples: 120 | Features: 2 sources       | Partitions: ['test', 'train'] | Branches: [0, 1] | Processing: ['augmented', 'feat_aug', 'raw']\n",
      "\n",
      "========================================================================================================================\n",
      "DATASET EVOLUTION SUMMARY\n",
      "========================================================================================================================\n",
      "Initial Dataset           | Rows:   50 | Samples:  50 | Features: 2 sources       | Partitions: ['train'] | Branches: [0] | Processing: ['raw']\n",
      "Added Test Set            | Rows:   70 | Samples:  70 | Features: 2 sources       | Partitions: ['test', 'train'] | Branches: [0] | Processing: ['raw']\n",
      "Sample Augmentation       | Rows:  120 | Samples: 120 | Features: 2 sources       | Partitions: ['test', 'train'] | Branches: [0] | Processing: ['augmented', 'raw']\n",
      "Feature Augmentation      | Rows:  240 | Samples: 120 | Features: 2 sources       | Partitions: ['train', 'test'] | Branches: [0] | Processing: ['raw', 'feat_aug', 'augmented']\n",
      "Branching (2 branches)    | Rows:  440 | Samples: 120 | Features: 2 sources       | Partitions: ['test', 'train'] | Branches: [0, 1] | Processing: ['augmented', 'feat_aug', 'raw']\n"
     ]
    }
   ],
   "source": [
    "class DatasetMonitor:\n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "\n",
    "    def capture(self, dataset, stage_name):\n",
    "        # Updated to work with new SpectraDataset structure\n",
    "        snapshot = {\n",
    "            'stage': stage_name,\n",
    "            'features_shape': f\"{len(dataset.features.sources)} sources\" if dataset.features else \"No features\",\n",
    "            'total_rows': len(dataset.indices),\n",
    "            'samples': dataset._next_sample,\n",
    "            'partitions': dataset.indices['partition'].unique().to_list(),\n",
    "            'branches': dataset.indices['branch'].unique().to_list(),\n",
    "            'processing': dataset.indices['processing'].unique().to_list(),\n",
    "            'target_info': dataset.target_manager.get_info()\n",
    "        }\n",
    "        self.snapshots.append(snapshot)\n",
    "\n",
    "        print(f\"{stage_name:25} | Rows: {snapshot['total_rows']:4d} | Samples: {snapshot['samples']:3d} | \" +\n",
    "              f\"Features: {snapshot['features_shape']:15} | Partitions: {snapshot['partitions']} | \" +\n",
    "              f\"Branches: {snapshot['branches']} | Processing: {snapshot['processing']}\")\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"DATASET EVOLUTION SUMMARY\")\n",
    "        print(\"=\"*120)\n",
    "        for snap in self.snapshots:\n",
    "            print(f\"{snap['stage']:25} | Rows: {snap['total_rows']:4d} | Samples: {snap['samples']:3d} | \" +\n",
    "                  f\"Features: {snap['features_shape']:15} | Partitions: {snap['partitions']} | \" +\n",
    "                  f\"Branches: {snap['branches']} | Processing: {snap['processing']}\")\n",
    "\n",
    "monitor = DatasetMonitor()\n",
    "\n",
    "# Test our enhanced operations step by step\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"MONITORING DATASET OPERATIONS\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Start with a fresh dataset\n",
    "test_dataset = SpectraDataset(task_type=\"classification\")\n",
    "test_dataset.add_data(\n",
    "    features=[np.random.rand(50, 200), np.random.rand(50, 100)],\n",
    "    targets=np.random.randint(0, 3, size=50),\n",
    "    partition=\"train\"\n",
    ")\n",
    "monitor.capture(test_dataset, \"Initial Dataset\")\n",
    "\n",
    "# Add some test data\n",
    "test_dataset.add_data(\n",
    "    features=[np.random.rand(20, 200), np.random.rand(20, 100)],\n",
    "    targets=np.random.randint(0, 3, size=20),\n",
    "    partition=\"test\"\n",
    ")\n",
    "monitor.capture(test_dataset, \"Added Test Set\")\n",
    "\n",
    "# Sample augmentation\n",
    "test_dataset.sample_augmentation(n_copies=1, processing_tag=\"augmented\")\n",
    "monitor.capture(test_dataset, \"Sample Augmentation\")\n",
    "\n",
    "# Feature augmentation\n",
    "test_dataset.feature_augmentation(processing_tag=\"feat_aug\")\n",
    "monitor.capture(test_dataset, \"Feature Augmentation\")\n",
    "\n",
    "# Branching\n",
    "test_dataset.branch_dataset(n_branches=2)\n",
    "monitor.capture(test_dataset, \"Branching (2 branches)\")\n",
    "\n",
    "monitor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133e6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REAL-WORLD SCENARIO SIMULATION\n",
      "================================================================================\n",
      "Initial dataset: 50 samples, 50 rows, 2 feature sources\n",
      "Fresh Dataset             | Rows:   50 | Samples:  50 | Features: 2 sources       | Partitions: ['train'] | Branches: [0] | Processing: ['raw']\n",
      "\n",
      "1. Testing Sample Augmentation...\n",
      "Created 100 new samples\n",
      "After augmentation: 150 samples, 150 rows\n",
      "After Sample Augmentation | Rows:  150 | Samples: 150 | Features: 2 sources       | Partitions: ['train'] | Branches: [0] | Processing: ['raw', 'augmented']\n",
      "\n",
      "2. Testing Feature Augmentation...\n",
      "After feature augmentation: 150 samples, 300 rows\n",
      "After Feature Augmentation | Rows:  300 | Samples: 150 | Features: 2 sources       | Partitions: ['train'] | Branches: [0] | Processing: ['raw', 'variant1', 'augmented']\n",
      "\n",
      "3. Testing Feature Extraction...\n",
      "2D extraction error: SpectraDataset.get_features_2d() got an unexpected keyword argument 'groups'\n",
      "3D extraction error: SpectraDataset.get_features_3d() got an unexpected keyword argument 'groups'\n",
      "\n",
      "Final fresh dataset: 150 samples, 300 rows, 2 feature sources\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Real-world Scenario Simulation with Fresh Dataset\n",
    "print(\"=\"*80)\n",
    "print(\"REAL-WORLD SCENARIO SIMULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_fresh_test_dataset():\n",
    "    \"\"\"Create a fresh dataset for testing\"\"\"\n",
    "    fresh_dataset = SpectraDataset()\n",
    "\n",
    "    # Add some fresh training data\n",
    "    features = [\n",
    "        np.random.randn(50, 100),  # 50 samples, 100 features (source 1)\n",
    "        np.random.randn(50, 80)    # 50 samples, 80 features (source 2)\n",
    "    ]\n",
    "    targets = np.random.randint(0, 3, 50)  # 3-class classification\n",
    "\n",
    "    fresh_dataset.add_data(\n",
    "        features=features,\n",
    "        targets=targets,\n",
    "        partition=\"train\",\n",
    "        group=0,\n",
    "        branch=0,\n",
    "        processing=\"raw\"\n",
    "    )\n",
    "\n",
    "    return fresh_dataset\n",
    "\n",
    "def get_dataset_info(dataset):\n",
    "    \"\"\"Get basic info about a dataset\"\"\"\n",
    "    unique_samples = len(dataset.indices['sample'].unique())\n",
    "    total_rows = len(dataset.indices)\n",
    "    n_features = len(dataset.features.sources) if dataset.features else 0\n",
    "    return unique_samples, total_rows, n_features\n",
    "\n",
    "def test_augmentation_operations():\n",
    "    \"\"\"Test all three augmentation operations on fresh data\"\"\"\n",
    "    # Start with fresh dataset\n",
    "    test_dataset = create_fresh_test_dataset()\n",
    "    samples, rows, features = get_dataset_info(test_dataset)\n",
    "    print(f\"Initial dataset: {samples} samples, {rows} rows, {features} feature sources\")\n",
    "    monitor.capture(test_dataset, \"Fresh Dataset\")\n",
    "\n",
    "    # 1. Sample augmentation\n",
    "    print(\"\\n1. Testing Sample Augmentation...\")\n",
    "    sample_ids = test_dataset.sample_augmentation(\n",
    "        partition='train',\n",
    "        n_copies=2,\n",
    "        processing_tag='augmented'\n",
    "    )\n",
    "    print(f\"Created {len(sample_ids)} new samples\")\n",
    "    samples, rows, features = get_dataset_info(test_dataset)\n",
    "    print(f\"After augmentation: {samples} samples, {rows} rows\")\n",
    "    monitor.capture(test_dataset, \"After Sample Augmentation\")\n",
    "\n",
    "    # 2. Feature augmentation (single processing tag)\n",
    "    print(\"\\n2. Testing Feature Augmentation...\")\n",
    "    test_dataset.feature_augmentation(\n",
    "        processing_tag='variant1'\n",
    "    )\n",
    "    samples, rows, features = get_dataset_info(test_dataset)\n",
    "    print(f\"After feature augmentation: {samples} samples, {rows} rows\")\n",
    "    monitor.capture(test_dataset, \"After Feature Augmentation\")\n",
    "\n",
    "    # 3. Feature extraction\n",
    "    print(\"\\n3. Testing Feature Extraction...\")\n",
    "    try:\n",
    "        features_2d = test_dataset.get_features_2d(\n",
    "            groups=[0],\n",
    "            branches=[0],\n",
    "            processing_tags=['raw']\n",
    "        )\n",
    "        print(f\"2D Features shape: {features_2d.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"2D extraction error: {e}\")\n",
    "\n",
    "    try:\n",
    "        features_3d = test_dataset.get_features_3d(\n",
    "            groups=[0],\n",
    "            branches=[0],\n",
    "            processing_tags=['raw', 'augmented']\n",
    "        )\n",
    "        print(f\"3D Features shape: {features_3d.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"3D extraction error: {e}\")\n",
    "\n",
    "    return test_dataset\n",
    "\n",
    "fresh_dataset = test_augmentation_operations()\n",
    "samples, rows, features = get_dataset_info(fresh_dataset)\n",
    "print(f\"\\nFinal fresh dataset: {samples} samples, {rows} rows, {features} feature sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef38b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PERFORMANCE BENCHMARKING\n",
      "================================================================================\n",
      "Benchmarking with dataset:\n",
      "  Samples: 200\n",
      "  Rows: 1200\n",
      "  Feature sources: 2\n",
      "  Memory usage: ~1173.1 MB\n",
      "\n",
      "Sample Augmentation (2x): 0.240s\n",
      "Feature Augmentation: 0.129s\n",
      "2D extraction error: SpectraDataset.get_features_2d() got an unexpected keyword argument 'partition'\n",
      "3D extraction error: SpectraDataset.get_features_3d() got an unexpected keyword argument 'partition'\n",
      "\n",
      "Total benchmark time: 0.369s\n",
      "Final memory usage: ~1060.4 MB\n",
      "\n",
      "Benchmark Summary:\n",
      "  sample_aug: 0.240s\n",
      "  feature_aug: 0.129s\n",
      "  extract_2d: 0.000s\n",
      "  extract_3d: 0.000s\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Performance Benchmarking\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE BENCHMARKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_memory_usage_mb():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def benchmark_operations():\n",
    "    \"\"\"Benchmark all three operations and feature extraction\"\"\"\n",
    "    import time\n",
    "    import gc\n",
    "\n",
    "    # Create a larger dataset for benchmarking\n",
    "    base_dataset = dataset.copy()\n",
    "    samples, rows, features = get_dataset_info(base_dataset)\n",
    "    memory_mb = get_memory_usage_mb()\n",
    "\n",
    "    print(f\"Benchmarking with dataset:\")\n",
    "    print(f\"  Samples: {samples}\")\n",
    "    print(f\"  Rows: {rows}\")\n",
    "    print(f\"  Feature sources: {features}\")\n",
    "    print(f\"  Memory usage: ~{memory_mb:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "    # Initialize variables\n",
    "    features_2d = None\n",
    "    features_3d = None\n",
    "\n",
    "    # Sample Augmentation Benchmark\n",
    "    start_time = time.time()\n",
    "    sample_ids = base_dataset.sample_augmentation(partition='train', n_copies=2, processing_tag='bench_aug')\n",
    "    sample_time = time.time() - start_time\n",
    "    print(f\"Sample Augmentation (2x): {sample_time:.3f}s\")\n",
    "\n",
    "    # Feature Augmentation Benchmark\n",
    "    start_time = time.time()\n",
    "    base_dataset.feature_augmentation(processing_tag='bench_feat_aug')\n",
    "    feature_time = time.time() - start_time\n",
    "    print(f\"Feature Augmentation: {feature_time:.3f}s\")\n",
    "\n",
    "    # 2D Feature Extraction Benchmark\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        features_2d = base_dataset.get_features_2d(partition='train')\n",
    "        extract_2d_time = time.time() - start_time\n",
    "        print(f\"2D Feature Extraction: {extract_2d_time:.3f}s, Shape: {features_2d.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"2D extraction error: {e}\")\n",
    "        extract_2d_time = 0\n",
    "\n",
    "    # 3D Feature Extraction Benchmark\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        features_3d = base_dataset.get_features_3d(partition='train')\n",
    "        extract_3d_time = time.time() - start_time\n",
    "        print(f\"3D Feature Extraction: {extract_3d_time:.3f}s, Shape: {features_3d.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"3D extraction error: {e}\")\n",
    "        extract_3d_time = 0\n",
    "\n",
    "    # Memory cleanup - safe deletion\n",
    "    if 'features_2d' in locals() and features_2d is not None:\n",
    "        del features_2d\n",
    "    if 'features_3d' in locals() and features_3d is not None:\n",
    "        del features_3d\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        'sample_aug': sample_time,\n",
    "        'feature_aug': feature_time,\n",
    "        'extract_2d': extract_2d_time,\n",
    "        'extract_3d': extract_3d_time\n",
    "    }\n",
    "\n",
    "benchmark_results = benchmark_operations()\n",
    "print(f\"\\nTotal benchmark time: {sum(benchmark_results.values()):.3f}s\")\n",
    "\n",
    "# Show final memory usage\n",
    "final_memory = get_memory_usage_mb()\n",
    "print(f\"Final memory usage: ~{final_memory:.1f} MB\")\n",
    "\n",
    "print(\"\\nBenchmark Summary:\")\n",
    "for operation, time_taken in benchmark_results.items():\n",
    "    print(f\"  {operation}: {time_taken:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PERFORMANCE TESTING\n",
      "================================================================================\n",
      "Testing performance with larger datasets...\n",
      "Initial data loading: 0.020s for 1000 samples\n",
      "Sample augmentation (2x): 0.337s\n",
      "Feature augmentation: 0.209s\n",
      "Branching (3x): 2.212s\n",
      "\n",
      "Final dataset size: 18000 rows\n",
      "Memory usage estimate: ~412.0 MB\n",
      "Feature extraction: 0.019s for (1000, 3000)\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE TESTING COMPLETED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test performance with larger datasets\n",
    "def test_performance():\n",
    "    print(\"Testing performance with larger datasets...\")\n",
    "\n",
    "    # Create a larger dataset\n",
    "    n_samples = 1000\n",
    "    large_dataset = SpectraDataset(task_type=\"classification\")\n",
    "\n",
    "    # Add initial data with timing\n",
    "    start_time = time.time()\n",
    "    large_dataset.add_data(\n",
    "        features=[np.random.rand(n_samples, 2000), np.random.rand(n_samples, 1000)],\n",
    "        targets=np.random.randint(0, 10, size=n_samples),\n",
    "        partition=\"train\"\n",
    "    )\n",
    "    init_time = time.time() - start_time\n",
    "    print(f\"Initial data loading: {init_time:.3f}s for {n_samples} samples\")\n",
    "\n",
    "    # Test sample augmentation performance\n",
    "    start_time = time.time()\n",
    "    large_dataset.sample_augmentation(n_copies=2, processing_tag=\"perf_aug\")\n",
    "    aug_time = time.time() - start_time\n",
    "    print(f\"Sample augmentation (2x): {aug_time:.3f}s\")\n",
    "\n",
    "    # Test feature augmentation performance\n",
    "    start_time = time.time()\n",
    "    large_dataset.feature_augmentation(processing_tag=\"perf_feat\")\n",
    "    feat_time = time.time() - start_time\n",
    "    print(f\"Feature augmentation: {feat_time:.3f}s\")\n",
    "\n",
    "    # Test branching performance\n",
    "    start_time = time.time()\n",
    "    large_dataset.branch_dataset(n_branches=3)\n",
    "    branch_time = time.time() - start_time\n",
    "    print(f\"Branching (3x): {branch_time:.3f}s\")\n",
    "\n",
    "    print(f\"\\nFinal dataset size: {len(large_dataset.indices)} rows\")\n",
    "    print(f\"Memory usage estimate: ~{len(large_dataset.indices) * 3000 * 8 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "    # Test extraction performance\n",
    "    start_time = time.time()\n",
    "    features = large_dataset.get_features_2d(filters={'branch': 0, 'processing': 'raw'})\n",
    "    extract_time = time.time() - start_time\n",
    "    print(f\"Feature extraction: {extract_time:.3f}s for {features.shape}\")\n",
    "\n",
    "    return large_dataset\n",
    "\n",
    "perf_dataset = test_performance()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE TESTING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MEMORY EFFICIENCY TESTING\n",
      "================================================================================\n",
      "Testing memory efficiency and data sharing...\n",
      "Base dataset: 1200 rows\n",
      "Initial samples: 200\n",
      "\n",
      "Testing feature sharing/copying...\n",
      "After sample augmentation: 2400 rows\n",
      "New sample IDs created: 1200\n",
      "Total samples after augmentation: 1400\n",
      "After feature augmentation: 4800 rows (increased by 2400)\n",
      "After branching: 14400 rows (increased by 9600)\n",
      "\n",
      "Testing data extraction...\n",
      "2D features shape: (14400, 1500)\n",
      "3D features shape: (14400, 1500, 1)\n",
      "Branch 0 data shape: (1600, 1500)\n",
      "Raw processing data shape: (900, 1500)\n",
      "\n",
      "================================================================================\n",
      "MEMORY EFFICIENCY TESTING COMPLETED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MEMORY EFFICIENCY TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def test_memory_efficiency():\n",
    "    \"\"\"Test memory efficiency of dataset operations\"\"\"\n",
    "    print(\"Testing memory efficiency and data sharing...\")\n",
    "\n",
    "    # Use a copy of the existing dataset\n",
    "    base_data = dataset.copy()\n",
    "\n",
    "    print(f\"Base dataset: {len(base_data.indices)} rows\")\n",
    "    initial_samples = len(base_data.indices[\"sample\"].unique())\n",
    "    print(f\"Initial samples: {initial_samples}\")\n",
    "\n",
    "    # Check if features are properly shared/copied\n",
    "    print(\"\\nTesting feature sharing/copying...\")\n",
    "\n",
    "    # Sample augmentation\n",
    "    sample_ids = base_data.sample_augmentation(partition='train', n_copies=2)\n",
    "    print(f\"After sample augmentation: {len(base_data.indices)} rows\")\n",
    "    print(f\"New sample IDs created: {len(sample_ids)}\")\n",
    "    augmented_samples = len(base_data.indices[\"sample\"].unique())\n",
    "    print(f\"Total samples after augmentation: {augmented_samples}\")\n",
    "\n",
    "    # Feature augmentation\n",
    "    rows_before = len(base_data.indices)\n",
    "    base_data.feature_augmentation('memory_test')\n",
    "    print(f\"After feature augmentation: {len(base_data.indices)} rows (increased by {len(base_data.indices) - rows_before})\")\n",
    "\n",
    "    # Branch dataset (use integer, not list)\n",
    "    rows_before = len(base_data.indices)\n",
    "    base_data.branch_dataset(3)  # Create 3 branches total (0, 1, 2)\n",
    "    print(f\"After branching: {len(base_data.indices)} rows (increased by {len(base_data.indices) - rows_before})\")\n",
    "\n",
    "    # Test data extraction\n",
    "    print(\"\\nTesting data extraction...\")\n",
    "\n",
    "    # Extract 2D features (using filters)\n",
    "    features_2d = base_data.get_features_2d(filters={'partition': ['train']})\n",
    "    print(f\"2D features shape: {features_2d.shape}\")\n",
    "\n",
    "    # Extract 3D features (using filters)\n",
    "    features_3d = base_data.get_features_3d(filters={'partition': ['train']})\n",
    "    print(f\"3D features shape: {features_3d.shape}\")\n",
    "\n",
    "    # Branch-specific extraction\n",
    "    branch_data = base_data.get_features_2d(filters={'partition': ['train'], 'branch': [0]})\n",
    "    print(f\"Branch 0 data shape: {branch_data.shape}\")\n",
    "\n",
    "    # Test processing-specific extraction\n",
    "    raw_data = base_data.get_features_2d(filters={'partition': ['train'], 'processing': ['raw']})\n",
    "    print(f\"Raw processing data shape: {raw_data.shape}\")\n",
    "\n",
    "    return base_data\n",
    "\n",
    "memory_dataset = test_memory_efficiency()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEMORY EFFICIENCY TESTING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe9792",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clustered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 45\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m SpectraDataset(\n\u001b[0;32m     38\u001b[0m             features\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mfeatures[mask_array],\n\u001b[0;32m     39\u001b[0m             targets\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtargets[mask_array],\n\u001b[0;32m     40\u001b[0m             indices\u001b[38;5;241m=\u001b[39mbranch_indices,\n\u001b[0;32m     41\u001b[0m             feature_sources\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mfeature_sources\n\u001b[0;32m     42\u001b[0m         )\n\u001b[0;32m     44\u001b[0m branch_configs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_rf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_svm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_decon\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 45\u001b[0m branches \u001b[38;5;241m=\u001b[39m BranchManager\u001b[38;5;241m.\u001b[39mcreate_branches(\u001b[43mclustered\u001b[49m, branch_configs)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, branch \u001b[38;5;129;01min\u001b[39;00m branches\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     48\u001b[0m     monitor\u001b[38;5;241m.\u001b[39mcapture(branch, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBranch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clustered' is not defined"
     ]
    }
   ],
   "source": [
    "class BranchManager:\n",
    "    @staticmethod\n",
    "    def create_branches(dataset, branch_configs):\n",
    "        branches = {}\n",
    "\n",
    "        for i, config in enumerate(branch_configs):\n",
    "            branch_name = f\"branch_{i}\"\n",
    "\n",
    "            branch_indices = dataset.indices.with_columns(\n",
    "                pl.lit(branch_name).alias('branch_id')\n",
    "            )\n",
    "\n",
    "            branch_dataset = SpectraDataset(\n",
    "                features=dataset.features.copy(),\n",
    "                targets=dataset.targets.copy(),\n",
    "                indices=branch_indices,\n",
    "                feature_sources=dataset.feature_sources.copy()\n",
    "            )\n",
    "\n",
    "            branches[branch_name] = branch_dataset\n",
    "\n",
    "        return branches\n",
    "\n",
    "    @staticmethod\n",
    "    def dispatch_to_branch(dataset, condition_func, branch_name):\n",
    "        mask = condition_func(dataset)\n",
    "\n",
    "        filtered_indices = dataset.indices.filter(mask)\n",
    "        valid_sample_ids = filtered_indices['sample_id'].to_list()\n",
    "\n",
    "        mask_array = np.isin(range(len(dataset.features)), valid_sample_ids)\n",
    "\n",
    "        branch_indices = filtered_indices.with_columns(\n",
    "            pl.lit(branch_name).alias('branch_id')\n",
    "        )\n",
    "\n",
    "        return SpectraDataset(\n",
    "            features=dataset.features[mask_array],\n",
    "            targets=dataset.targets[mask_array],\n",
    "            indices=branch_indices,\n",
    "            feature_sources=dataset.feature_sources\n",
    "        )\n",
    "\n",
    "branch_configs = ['model_rf', 'model_svm', 'model_decon']\n",
    "branches = BranchManager.create_branches(clustered, branch_configs)\n",
    "\n",
    "for name, branch in branches.items():\n",
    "    monitor.capture(branch, f\"Branch: {name}\")\n",
    "\n",
    "condition = lambda ds: ds.indices['cluster_id'] < 2\n",
    "filtered_branch = BranchManager.dispatch_to_branch(clustered, condition, 'low_cluster')\n",
    "monitor.capture(filtered_branch, \"Filtered Branch\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EDGE CASE TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def test_edge_cases():\n",
    "    \"\"\"Test edge cases and error handling\"\"\"\n",
    "    print(\"Testing edge cases and error handling...\")\n",
    "\n",
    "    # Test with empty dataset\n",
    "    empty_dataset = dataset.copy()\n",
    "    empty_dataset.indices = empty_dataset.indices.filter(pl.col(\"sample\") == -1)  # No matches\n",
    "\n",
    "    print(\"\\nTesting with empty dataset:\")\n",
    "    try:\n",
    "        result = empty_dataset.sample_augmentation(partition='train', n_copies=2)\n",
    "        print(f\"Empty dataset augmentation returned: {len(result)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Empty dataset error: {e}\")\n",
    "\n",
    "    # Test with missing partition\n",
    "    test_dataset = dataset.copy()\n",
    "    print(\"\\nTesting with missing partition:\")\n",
    "    try:\n",
    "        result = test_dataset.sample_augmentation(partition='test', n_copies=2)  # 'test' may not exist\n",
    "        print(f\"Missing partition augmentation returned: {len(result)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Missing partition error: {e}\")\n",
    "\n",
    "    # Test feature augmentation on empty dataset\n",
    "    print(\"\\nTesting feature augmentation on empty dataset:\")\n",
    "    try:\n",
    "        empty_dataset.feature_augmentation('test_empty')\n",
    "        print(\"Feature augmentation on empty dataset completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Feature augmentation error: {e}\")\n",
    "\n",
    "    # Test large copy numbers\n",
    "    print(\"\\nTesting with large copy numbers:\")\n",
    "    try:\n",
    "        small_dataset = dataset.copy()\n",
    "        # Get just first 10 samples\n",
    "        first_samples = small_dataset.indices.head(10)\n",
    "        small_dataset.indices = first_samples\n",
    "        result = small_dataset.sample_augmentation(partition='train', n_copies=5)\n",
    "        print(f\"Large copy number returned: {len(result)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Large copy number error: {e}\")\n",
    "\n",
    "    # Test branch dataset with no train data\n",
    "    print(\"\\nTesting branch dataset edge cases:\")\n",
    "    try:\n",
    "        no_train_dataset = dataset.copy()\n",
    "        no_train_dataset.indices = no_train_dataset.indices.with_columns(\n",
    "            pl.col(\"partition\").str.replace(\"train\", \"test\")\n",
    "        )\n",
    "        no_train_dataset.branch_dataset(2)\n",
    "        print(\"Branch dataset with no train data completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Branch dataset error: {e}\")\n",
    "\n",
    "    print(\"\\nEdge case testing completed!\")\n",
    "\n",
    "test_edge_cases()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EDGE CASE TESTING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE INTEGRATION TEST\n",
      "================================================================================\n",
      "Running comprehensive integration test...\n",
      "Initial Multi-Source      | Rows:  200 | Samples: 200 | Features: 2 sources       | Partitions: ['train', 'test', 'val'] | Branches: [0] | Processing: ['raw']\n",
      "1. Sample Augmentation    | Rows:  340 | Samples: 340 | Features: 2 sources       | Partitions: ['val', 'test', 'train'] | Branches: [0] | Processing: ['balanced', 'raw']\n",
      "2. Feature Augmentation #1 | Rows:  680 | Samples: 340 | Features: 2 sources       | Partitions: ['test', 'val', 'train'] | Branches: [0] | Processing: ['balanced', 'preprocessed_v1', 'raw']\n",
      "3. Feature Augmentation #2 | Rows: 1360 | Samples: 340 | Features: 2 sources       | Partitions: ['train', 'test', 'val'] | Branches: [0] | Processing: ['preprocessed_v1', 'raw', 'balanced', 'preprocessed_v2']\n",
      "4. Branching (4 models)   | Rows: 4720 | Samples: 340 | Features: 2 sources       | Partitions: ['test', 'train', 'val'] | Branches: [0, 1, 2, 3] | Processing: ['preprocessed_v2', 'preprocessed_v1', 'raw', 'balanced']\n",
      "\n",
      "------------------------------------------------------------\n",
      "DATA INTEGRITY VALIDATION\n",
      "------------------------------------------------------------\n",
      "✓ Original train, branch 0 : (140, 1536)\n",
      "✓ Balanced train, branch 1 : (140, 1536)\n",
      "✓ Preprocessed v1, branch 2: (280, 1536)\n",
      "✓ Preprocessed v2, branch 3: (560, 1536)\n",
      "✓ Validation data          : (30, 1536)\n",
      "✓ Test data                : (30, 1536)\n",
      "✓ 3D extraction:             (1120, 1536, 1)\n",
      "\n",
      "------------------------------------------------------------\n",
      "FINAL STATISTICS\n",
      "------------------------------------------------------------\n",
      "Total rows: 4720\n",
      "Unique samples: 340\n",
      "Partitions: ['test', 'val', 'train']\n",
      "Branches: [0, 1, 2, 3]\n",
      "Processing types: ['raw', 'balanced', 'preprocessed_v1', 'preprocessed_v2']\n",
      "\n",
      "========================================================================================================================\n",
      "DATASET EVOLUTION SUMMARY\n",
      "========================================================================================================================\n",
      "Initial Multi-Source      | Rows:  200 | Samples: 200 | Features: 2 sources       | Partitions: ['train', 'test', 'val'] | Branches: [0] | Processing: ['raw']\n",
      "1. Sample Augmentation    | Rows:  340 | Samples: 340 | Features: 2 sources       | Partitions: ['val', 'test', 'train'] | Branches: [0] | Processing: ['balanced', 'raw']\n",
      "2. Feature Augmentation #1 | Rows:  680 | Samples: 340 | Features: 2 sources       | Partitions: ['test', 'val', 'train'] | Branches: [0] | Processing: ['balanced', 'preprocessed_v1', 'raw']\n",
      "3. Feature Augmentation #2 | Rows: 1360 | Samples: 340 | Features: 2 sources       | Partitions: ['train', 'test', 'val'] | Branches: [0] | Processing: ['preprocessed_v1', 'raw', 'balanced', 'preprocessed_v2']\n",
      "4. Branching (4 models)   | Rows: 4720 | Samples: 340 | Features: 2 sources       | Partitions: ['test', 'train', 'val'] | Branches: [0, 1, 2, 3] | Processing: ['preprocessed_v2', 'preprocessed_v1', 'raw', 'balanced']\n",
      "\n",
      "================================================================================\n",
      "🎉 ALL TESTS COMPLETED SUCCESSFULLY!\n",
      "✅ Sample Augmentation: Creates new samples with new IDs, preserves origins\n",
      "✅ Feature Augmentation: Creates new rows with same sample IDs, different processing\n",
      "✅ Branching: Copies train data across branches for ensemble methods\n",
      "✅ Feature Extraction: 2D/3D extraction with flexible filtering\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE INTEGRATION TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def comprehensive_test():\n",
    "    \"\"\"\n",
    "    Test all three operations in sequence to ensure they work together properly.\n",
    "    \"\"\"\n",
    "    print(\"Running comprehensive integration test...\")\n",
    "\n",
    "    # Create a realistic dataset\n",
    "    integration_dataset = SpectraDataset(task_type=\"classification\")\n",
    "\n",
    "    # Add multi-source spectral data\n",
    "    nir_data = np.random.rand(200, 1024) * 0.5 + 1.0\n",
    "    raman_data = np.random.rand(200, 512) * 0.3 + 0.8\n",
    "    labels = np.random.randint(0, 4, size=200)\n",
    "\n",
    "    # Split data\n",
    "    train_nir, train_raman, train_labels = nir_data[:140], raman_data[:140], labels[:140]\n",
    "    val_nir, val_raman, val_labels = nir_data[140:170], raman_data[140:170], labels[140:170]\n",
    "    test_nir, test_raman, test_labels = nir_data[170:], raman_data[170:], labels[170:]\n",
    "\n",
    "    # Add to dataset\n",
    "    integration_dataset.add_data([train_nir, train_raman], train_labels, partition=\"train\")\n",
    "    integration_dataset.add_data([val_nir, val_raman], val_labels, partition=\"val\")\n",
    "    integration_dataset.add_data([test_nir, test_raman], test_labels, partition=\"test\")\n",
    "\n",
    "    monitor = DatasetMonitor()\n",
    "    monitor.capture(integration_dataset, \"Initial Multi-Source\")\n",
    "\n",
    "    # Step 1: Sample augmentation for data balancing\n",
    "    integration_dataset.sample_augmentation(partition=\"train\", n_copies=1, processing_tag=\"balanced\")\n",
    "    monitor.capture(integration_dataset, \"1. Sample Augmentation\")\n",
    "\n",
    "    # Step 2: Feature augmentation for preprocessing variants\n",
    "    integration_dataset.feature_augmentation(processing_tag=\"preprocessed_v1\")\n",
    "    monitor.capture(integration_dataset, \"2. Feature Augmentation #1\")\n",
    "\n",
    "    integration_dataset.feature_augmentation(processing_tag=\"preprocessed_v2\")\n",
    "    monitor.capture(integration_dataset, \"3. Feature Augmentation #2\")\n",
    "\n",
    "    # Step 3: Branching for ensemble methods\n",
    "    integration_dataset.branch_dataset(n_branches=4)\n",
    "    monitor.capture(integration_dataset, \"4. Branching (4 models)\")\n",
    "\n",
    "    # Validate data integrity\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"DATA INTEGRITY VALIDATION\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Check that different extraction patterns work\n",
    "    test_cases = [\n",
    "        (\"Original train, branch 0\", {'partition': 'train', 'branch': 0, 'processing': 'raw'}),\n",
    "        (\"Balanced train, branch 1\", {'partition': 'train', 'branch': 1, 'processing': 'balanced'}),\n",
    "        (\"Preprocessed v1, branch 2\", {'partition': 'train', 'branch': 2, 'processing': 'preprocessed_v1'}),\n",
    "        (\"Preprocessed v2, branch 3\", {'partition': 'train', 'branch': 3, 'processing': 'preprocessed_v2'}),\n",
    "        (\"Validation data\", {'partition': 'val', 'processing': 'raw'}),\n",
    "        (\"Test data\", {'partition': 'test', 'processing': 'raw'}),\n",
    "    ]\n",
    "\n",
    "    for description, filters in test_cases:\n",
    "        try:\n",
    "            data = integration_dataset.get_features_2d(filters=filters)\n",
    "            print(f\"✓ {description:25}: {data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {description:25}: Error - {e}\")\n",
    "\n",
    "    # Test 3D extraction\n",
    "    try:\n",
    "        data_3d = integration_dataset.get_features_3d(filters={'partition': 'train', 'branch': 0})\n",
    "        print(f\"✓ 3D extraction:             {data_3d.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 3D extraction:             Error - {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"FINAL STATISTICS\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"Total rows: {len(integration_dataset.indices)}\")\n",
    "    print(f\"Unique samples: {integration_dataset._next_sample}\")\n",
    "    print(f\"Partitions: {integration_dataset.indices['partition'].unique().to_list()}\")\n",
    "    print(f\"Branches: {integration_dataset.indices['branch'].unique().to_list()}\")\n",
    "    print(f\"Processing types: {integration_dataset.indices['processing'].unique().to_list()}\")\n",
    "\n",
    "    monitor.summary()\n",
    "\n",
    "    return integration_dataset\n",
    "\n",
    "final_dataset = comprehensive_test()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 ALL TESTS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"✅ Sample Augmentation: Creates new samples with new IDs, preserves origins\")\n",
    "print(\"✅ Feature Augmentation: Creates new rows with same sample IDs, different processing\")\n",
    "print(\"✅ Branching: Copies train data across branches for ensemble methods\")\n",
    "print(\"✅ Feature Extraction: 2D/3D extraction with flexible filtering\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
