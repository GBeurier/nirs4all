{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "‚úÖ Loaded pipeline(s) with 1 configuration(s).\n",
      "========================================================================================================================================================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m p_config \u001b[38;5;241m=\u001b[39m PipelineConfigs(pipeline_config)\n\u001b[0;32m     23\u001b[0m runner \u001b[38;5;241m=\u001b[39m PipelineRunner()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Simple transformers pipeline completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# print(f\"Final dataset: {res}\")\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:78\u001b[0m, in \u001b[0;36mPipelineRunner.run\u001b[1;34m(self, pipeline_configs, dataset_configs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d_config \u001b[38;5;129;01min\u001b[39;00m dataset_configs\u001b[38;5;241m.\u001b[39mdata_configs:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_configs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m steps, config_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pipeline_configs\u001b[38;5;241m.\u001b[39msteps, pipeline_configs\u001b[38;5;241m.\u001b[39mnames):\n\u001b[0;32m     80\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single(steps, config_name, dataset)\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\dataset_config.py:54\u001b[0m, in \u001b[0;36mDatasetConfigs.get_dataset\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     51\u001b[0m     x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m get_dataset(config)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m (copy\u001b[38;5;241m.\u001b[39mdeepcopy(x_train), copy\u001b[38;5;241m.\u001b[39mdeepcopy(y_train), copy\u001b[38;5;241m.\u001b[39mdeepcopy(x_test), copy\u001b[38;5;241m.\u001b[39mdeepcopy(y_test))\n\u001b[1;32m---> 54\u001b[0m dataset \u001b[38;5;241m=\u001b[39m SpectroDataset(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfolder_to_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     55\u001b[0m dataset\u001b[38;5;241m.\u001b[39madd_samples(x_train, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     56\u001b[0m dataset\u001b[38;5;241m.\u001b[39madd_samples(x_test, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\dataset_config.py:110\u001b[0m, in \u001b[0;36mDatasetConfigs.folder_to_name\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfolder_to_name\u001b[39m(folder_path):\n\u001b[1;32m--> 110\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(path\u001b[38;5;241m.\u001b[39mparts):\n\u001b[0;32m    112\u001b[0m         clean_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m part)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:960\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:594\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args):\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 594\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:578\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    576\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 578\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[0;32m    581\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not dict"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfigs\n",
    "from nirs4all.dataset import dataset\n",
    "from nirs4all.dataset.dataset_config import DatasetConfigs\n",
    "from sample import dataset_config, pipeline_config\n",
    "# from sample_classif import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "d_config = DatasetConfigs(dataset_config)\n",
    "p_config = PipelineConfigs(pipeline_config)\n",
    "runner = PipelineRunner()\n",
    "runner.run(p_config, d_config)\n",
    "\n",
    "print(\"‚úÖ Simple transformers pipeline completed successfully!\")\n",
    "# print(f\"Final dataset: {res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae6a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è Dataset does not have data for train_group.\n",
      "‚ö†Ô∏è Dataset does not have data for test_group.\n",
      "\u001b[97müìä Dataset: regression\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "Configuration generates 80 configurations.\n",
      "Generated 80 configurations.\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfigs\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config, generator_config\n",
    "# from sample_classif import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfigs(generator_config)\n",
    "if config.has_configurations:\n",
    "    print(f\"Generated {len(config.steps)} configurations.\")\n",
    "    # for i, step in enumerate(config.steps):\n",
    "        # print(config)\n",
    "# runner = PipelineRunner()\n",
    "# res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "# print(\"‚úÖ Simple transformers pipeline completed successfully!\")\n",
    "# print(f\"Final dataset: {res_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc19bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfigs(pipeline_config)\n",
    "runner = PipelineRunner()\n",
    "\n",
    "pipeline_path = \"results/sample_data/config_89f2f434\"\n",
    "pred_dataset, pred_context = PipelineRunner.predict(\n",
    "    path=pipeline_path,\n",
    "    dataset=data,  # Use same data for testing\n",
    "    verbose=1\n",
    ")\n",
    "print(\"‚úÖ Prediction mode completed successfully!\")\n",
    "\n",
    "predictions_obj = pred_dataset._predictions\n",
    "pred_count = len(predictions_obj._predictions)\n",
    "print(f\"üéØ Generated {pred_count} prediction records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76236b",
   "metadata": {},
   "source": [
    "# üìä Prediction Visualization Testing\n",
    "\n",
    "Test the new `PredictionVisualizer` class for analyzing and displaying prediction results graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc283ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and test the PredictionVisualizer\n",
    "from nirs4all.dataset.prediction_visualizer import PredictionVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üîç Testing PredictionVisualizer with current prediction results...\")\n",
    "viz = PredictionVisualizer(predictions_obj)\n",
    "print(\"\\n\" + viz.summary_report())\n",
    "\n",
    "# Show only best predictions\n",
    "viz.plot_filtered_predictions(prediction_filter='best_only', metric='r2')\n",
    "\n",
    "# Performance matrix with only global predictions\n",
    "viz.plot_performance_matrix(prediction_filter='global_only')\n",
    "\n",
    "# Bar chart comparing all prediction types\n",
    "viz.plot_filtered_predictions(chart_type='bar', metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63943e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Performance Matrix Visualization\n",
    "print(\"üìà Testing Performance Matrix...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Create performance matrix plot\n",
    "\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Performance matrix plot created successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating performance matrix: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Multi-Metric Comparison\n",
    "print(\"üìä Testing Multi-Metric Comparison...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Create multi-metric comparison plot\n",
    "        fig2 = viz.plot_multi_metric_comparison(\n",
    "            metrics=['rmse', 'mae', 'r2', 'mse'],  # Show all metrics\n",
    "            sort_by='r2',      # Sort by R¬≤ (best first)\n",
    "            ascending=False,   # Higher R¬≤ is better\n",
    "            figsize=(16, 10)\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Multi-metric comparison plot created successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating multi-metric plot: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc18d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Prediction Scatter Plots\n",
    "print(\"üéØ Testing Prediction Scatter Plots...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Create scatter plot of true vs predicted values\n",
    "        fig3 = viz.plot_prediction_scatter(\n",
    "            figsize=(14, 8)  # Show scatter plots for all model-config combinations\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Prediction scatter plots created successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating scatter plots: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Demonstrate different sorting and filtering options\n",
    "print(\"‚öôÔ∏è Testing Advanced Visualization Options...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Test different sorting options\n",
    "        print(\"\\nüîÑ Creating performance matrix sorted by best config:\")\n",
    "        fig4a = viz.plot_performance_matrix(\n",
    "            metric='r2',         # Use R¬≤ metric\n",
    "            sort_by='config',    # Sort by config performance\n",
    "            ascending=False,     # Best (highest R¬≤) first\n",
    "            figsize=(10, 6),\n",
    "            cmap='RdYlGn',      # Green is better for R¬≤\n",
    "            show_values=True\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nüìà Creating RMSE matrix sorted by model:\")\n",
    "        fig4b = viz.plot_performance_matrix(\n",
    "            metric='rmse',       # Use RMSE metric\n",
    "            sort_by='model',     # Sort by model performance\n",
    "            ascending=True,      # Best (lowest RMSE) first\n",
    "            figsize=(10, 6),\n",
    "            cmap='RdYlGn_r',    # Red is worse for RMSE\n",
    "            show_values=True\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"‚úÖ Advanced visualization options tested successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in advanced visualization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Explore dictionary structure\n",
    "print(\"üîç Exploring prediction dictionary structure...\")\n",
    "\n",
    "try:\n",
    "    if 'pred_dataset' in globals() and hasattr(pred_dataset, '_predictions'):\n",
    "        predictions_obj = pred_dataset._predictions\n",
    "        pred_data = predictions_obj._predictions\n",
    "\n",
    "        print(f\"Dictionary keys: {list(pred_data.keys())}\")\n",
    "\n",
    "        # Explore each key\n",
    "        for key, value in pred_data.items():\n",
    "            print(f\"\\nKey: {key}\")\n",
    "            print(f\"  Type: {type(value)}\")\n",
    "            if hasattr(value, '__len__'):\n",
    "                print(f\"  Length: {len(value)}\")\n",
    "\n",
    "            # Show first few items or structure\n",
    "            if isinstance(value, (list, tuple)) and len(value) > 0:\n",
    "                print(f\"  First item type: {type(value[0])}\")\n",
    "                print(f\"  First item: {value[0]}\")\n",
    "            elif isinstance(value, dict):\n",
    "                print(f\"  Dict keys: {list(value.keys())}\")\n",
    "                for subkey, subvalue in list(value.items())[:2]:  # Show first 2 items\n",
    "                    print(f\"    {subkey}: {type(subvalue)} = {subvalue}\")\n",
    "            else:\n",
    "                print(f\"  Value: {value}\")\n",
    "\n",
    "        # Try the get_predictions method\n",
    "        print(f\"\\nüîç Trying get_predictions():\")\n",
    "        try:\n",
    "            preds = predictions_obj.get_predictions()\n",
    "            print(f\"get_predictions() type: {type(preds)}\")\n",
    "            if isinstance(preds, dict):\n",
    "                print(f\"Keys: {list(preds.keys())}\")\n",
    "                for key, value in list(preds.items())[:2]:\n",
    "                    print(f\"  {key}: {type(value)}\")\n",
    "                    if isinstance(value, dict):\n",
    "                        print(f\"    Sub-keys: {list(value.keys())}\")\n",
    "            elif isinstance(preds, list):\n",
    "                print(f\"List length: {len(preds)}\")\n",
    "                if preds:\n",
    "                    print(f\"First item: {preds[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"get_predictions() error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exploring structure: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473080b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Cross-Validation Prediction Analysis\n",
    "print(\"üéØ CROSS-VALIDATION PREDICTION ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_cv_predictions(predictions_obj):\n",
    "    \"\"\"Analyze cross-validation predictions by grouping fold results properly\"\"\"\n",
    "\n",
    "    # Group predictions by base model name and partition type\n",
    "    model_groups = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for key, pred_data in predictions_obj._predictions.items():\n",
    "        model_name = pred_data['model']\n",
    "        partition = pred_data['partition']\n",
    "\n",
    "        # Extract base model name (everything before the last underscore + number)\n",
    "        base_model_match = re.match(r'(.+?)_\\d+$', model_name)\n",
    "        base_model = base_model_match.group(1) if base_model_match else model_name\n",
    "\n",
    "        # Categorize partition types\n",
    "        if 'test_fold' in partition:\n",
    "            partition_type = 'Fold Test'\n",
    "        elif 'val_fold' in partition:\n",
    "            partition_type = 'Fold Validation'\n",
    "        elif 'train_fold' in partition:\n",
    "            partition_type = 'Fold Train'\n",
    "        elif 'test' in partition and 'fold' not in partition:\n",
    "            partition_type = 'Global Test'\n",
    "        elif 'global_train' in partition:\n",
    "            partition_type = 'Global Train'\n",
    "        else:\n",
    "            partition_type = partition\n",
    "\n",
    "        model_groups[base_model][partition_type].append({\n",
    "            'model_instance': model_name,\n",
    "            'partition': partition,\n",
    "            'data': pred_data,\n",
    "            'fold_idx': pred_data.get('fold_idx', 0)\n",
    "        })\n",
    "\n",
    "    return model_groups\n",
    "\n",
    "if 'cv_result' in globals() and hasattr(cv_result, '_predictions'):\n",
    "    print(\"üìä Analyzing Cross-Validation Results...\")\n",
    "    print(f\"Total prediction records: {len(cv_result._predictions)}\")\n",
    "\n",
    "    # Analyze CV predictions\n",
    "    cv_analysis = analyze_cv_predictions(cv_result._predictions)\n",
    "\n",
    "    print(f\"Base models found: {list(cv_analysis.keys())}\")\n",
    "    print()\n",
    "\n",
    "    for base_model, prediction_types in cv_analysis.items():\n",
    "        print(f\"ü§ñ Base Model: {base_model}\")\n",
    "        print(\"‚îÄ\" * 50)\n",
    "\n",
    "        for pred_type, predictions in prediction_types.items():\n",
    "            print(f\"  üìà {pred_type}:\")\n",
    "\n",
    "            if len(predictions) == 1:\n",
    "                # Single prediction\n",
    "                pred = predictions[0]\n",
    "                data = pred['data']\n",
    "                y_true = np.array(data['y_true']).flatten()\n",
    "                y_pred = np.array(data['y_pred']).flatten()\n",
    "\n",
    "                # Calculate metrics\n",
    "                rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "                r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "                mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "                print(f\"    ‚úÖ {len(y_true)} samples\")\n",
    "                print(f\"    üìä RMSE: {rmse:.4f}, R¬≤: {r2:.4f}, MAE: {mae:.4f}\")\n",
    "                print(f\"    üéØ Model Instance: {pred['model_instance']}\")\n",
    "\n",
    "            else:\n",
    "                # Multiple fold predictions\n",
    "                print(f\"    ‚úÖ {len(predictions)} fold predictions:\")\n",
    "\n",
    "                all_rmse = []\n",
    "                all_r2 = []\n",
    "                all_samples = 0\n",
    "\n",
    "                for pred in sorted(predictions, key=lambda x: x['fold_idx']):\n",
    "                    data = pred['data']\n",
    "                    y_true = np.array(data['y_true']).flatten()\n",
    "                    y_pred = np.array(data['y_pred']).flatten()\n",
    "\n",
    "                    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "                    r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "\n",
    "                    all_rmse.append(rmse)\n",
    "                    all_r2.append(r2)\n",
    "                    all_samples += len(y_true)\n",
    "\n",
    "                    fold_info = pred['partition'].split('_')[-1] if '_' in pred['partition'] else pred['fold_idx']\n",
    "                    print(f\"      Fold {fold_info}: {len(y_true)} samples, RMSE={rmse:.4f}, R¬≤={r2:.4f}\")\n",
    "                    print(f\"        Instance: {pred['model_instance']}\")\n",
    "\n",
    "                # Show aggregate statistics\n",
    "                print(f\"    üìä Aggregate: {all_samples} total samples\")\n",
    "                print(f\"    üìà Average Performance: RMSE={np.mean(all_rmse):.4f}¬±{np.std(all_rmse):.4f}\")\n",
    "                print(f\"                           R¬≤={np.mean(all_r2):.4f}¬±{np.std(all_r2):.4f}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "    # Now test combining fold predictions properly\n",
    "    print(\"=\" * 55)\n",
    "    print(\"üßÆ TESTING FOLD COMBINATION\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    # Get base model name for combination\n",
    "    base_models = list(cv_analysis.keys())\n",
    "    if base_models:\n",
    "        test_base_model = base_models[0]\n",
    "        print(f\"Testing fold combination for: {test_base_model}\")\n",
    "\n",
    "        # Find all test fold predictions for this base model\n",
    "        test_folds = cv_analysis[test_base_model].get('Fold Test', [])\n",
    "\n",
    "        if test_folds:\n",
    "            print(f\"Found {len(test_folds)} test fold predictions to combine\")\n",
    "\n",
    "            # Combine manually since each fold has different sample indices\n",
    "            combined_y_true = []\n",
    "            combined_y_pred = []\n",
    "            combined_indices = []\n",
    "\n",
    "            for fold_pred in test_folds:\n",
    "                data = fold_pred['data']\n",
    "                combined_y_true.extend(data['y_true'])\n",
    "                combined_y_pred.extend(data['y_pred'])\n",
    "                combined_indices.extend(data.get('sample_indices', []))\n",
    "\n",
    "            combined_y_true = np.array(combined_y_true)\n",
    "            combined_y_pred = np.array(combined_y_pred)\n",
    "\n",
    "            # Calculate combined performance\n",
    "            combined_rmse = np.sqrt(np.mean((combined_y_true - combined_y_pred) ** 2))\n",
    "            combined_r2 = 1 - (np.sum((combined_y_true - combined_y_pred) ** 2) /\n",
    "                              np.sum((combined_y_true - np.mean(combined_y_true)) ** 2))\n",
    "\n",
    "            print(f\"‚úÖ Successfully combined fold predictions:\")\n",
    "            print(f\"   üìä Total samples: {len(combined_y_true)}\")\n",
    "            print(f\"   üìà Combined RMSE: {combined_rmse:.4f}\")\n",
    "            print(f\"   üìà Combined R¬≤: {combined_r2:.4f}\")\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No test fold predictions found for combination\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No cross-validation results available\")\n",
    "    print(\"   Please run a cross-validation pipeline first to see comprehensive prediction analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Prediction Visualizer with Filtering\n",
    "print(\"üéØ ENHANCED PREDICTION VISUALIZATION WITH FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    if 'pred_dataset' in globals() and hasattr(pred_dataset, '_predictions'):\n",
    "        # Create enhanced visualizer with dataset name override\n",
    "        dataset_name = getattr(pred_dataset, 'name', 'Sample NIRS Dataset')\n",
    "        viz_enhanced = PredictionVisualizer(pred_dataset._predictions, dataset_name_override=dataset_name)\n",
    "\n",
    "        print(f\"‚úÖ Enhanced visualizer created with dataset: {dataset_name}\")\n",
    "        print(f\"üìä Total prediction records: {len(viz_enhanced.data)}\")\n",
    "\n",
    "        # Test different filter types\n",
    "        filter_types = ['all', 'best_only', 'folds_only', 'averaged_only', 'global_only']\n",
    "\n",
    "        for filter_type in filter_types:\n",
    "            print(f\"\\nüîç Testing filter: {filter_type}\")\n",
    "\n",
    "            # Get filtered data count\n",
    "            filtered_data = viz_enhanced._filter_data_by_prediction_types(filter_type)\n",
    "            print(f\"   üìà Filtered to {len(filtered_data)} records\")\n",
    "\n",
    "            if len(filtered_data) > 0:\n",
    "                # Show sample of what's included\n",
    "                sample_partitions = [pred.get('partition', 'unknown') for pred in filtered_data[:3]]\n",
    "                print(f\"   üéØ Sample partitions: {sample_partitions}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ All filters tested successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå No prediction data found. Run the prediction cell first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing enhanced visualizer: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c341a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Filtering Visualizations\n",
    "print(\"üé® TESTING FILTERED VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    if 'viz_enhanced' in globals():\n",
    "        # Test 1: Show all predictions as bar chart\n",
    "        print(\"\\nüìä 1. All Predictions Bar Chart\")\n",
    "        fig1 = viz_enhanced.plot_filtered_predictions(\n",
    "            prediction_filter='all',\n",
    "            metric='rmse',\n",
    "            chart_type='bar',\n",
    "            figsize=(14, 8)\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # Test 2: Show only best predictions\n",
    "        print(\"\\nüèÜ 2. Best Predictions Only\")\n",
    "        fig2 = viz_enhanced.plot_filtered_predictions(\n",
    "            prediction_filter='best_only',\n",
    "            metric='r2',\n",
    "            chart_type='bar',\n",
    "            figsize=(12, 6)\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # Test 3: Show only fold predictions\n",
    "        print(\"\\nüìà 3. Cross-Validation Folds Only\")\n",
    "        fig3 = viz_enhanced.plot_filtered_predictions(\n",
    "            prediction_filter='folds_only',\n",
    "            metric='rmse',\n",
    "            chart_type='bar',\n",
    "            figsize=(16, 8)\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # Test 4: Performance matrix with filtering\n",
    "        print(\"\\nüî• 4. Performance Matrix - Global Predictions Only\")\n",
    "        fig4 = viz_enhanced.plot_performance_matrix(\n",
    "            metric='r2',\n",
    "            prediction_filter='global_only',\n",
    "            figsize=(10, 6),\n",
    "            show_values=True\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"‚úÖ All filtered visualizations completed successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Enhanced visualizer not found. Run previous cell first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in filtered visualizations: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91691968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Prediction Summary with Proper Dataset Names\n",
    "print(\"üìã COMPREHENSIVE PREDICTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    if 'viz_enhanced' in globals():\n",
    "        # Show comprehensive summary with fixed dataset names\n",
    "        summary = viz_enhanced.comprehensive_prediction_summary()\n",
    "        print(summary)\n",
    "\n",
    "        # Show how the dataset name override works\n",
    "        print(f\"\\nüè∑Ô∏è  Dataset Name Override: {viz_enhanced.dataset_name_override}\")\n",
    "        print(f\"üìä Data Records: {len(viz_enhanced.data)}\")\n",
    "\n",
    "        # Demonstrate filtering examples\n",
    "        print(f\"\\nüîç FILTERING EXAMPLES:\")\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "        filters_to_test = {\n",
    "            'all': 'All prediction types',\n",
    "            'best_only': 'Best performing prediction per model',\n",
    "            'folds_only': 'Cross-validation fold predictions only',\n",
    "            'averaged_only': 'Averaged predictions only',\n",
    "            'global_only': 'Global train/test predictions only'\n",
    "        }\n",
    "\n",
    "        for filter_name, description in filters_to_test.items():\n",
    "            filtered_count = len(viz_enhanced._filter_data_by_prediction_types(filter_name))\n",
    "            print(f\"‚Ä¢ {filter_name:15} | {description:35} | {filtered_count:2} records\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Enhanced PredictionVisualizer ready with:\")\n",
    "        print(f\"   üéØ Dataset name fixes (no more 'unknown')\")\n",
    "        print(f\"   üîç Filtering capabilities (5 filter types)\")\n",
    "        print(f\"   üìä Enhanced visualization methods\")\n",
    "        print(f\"   üìà Bar charts for prediction type comparison\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Enhanced visualizer not found. Run previous cell first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in comprehensive summary: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
