{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d843c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering controller: DummyController\n",
      "Registering controller: SklearnModelController\n",
      "Registering controller: TensorFlowModelController\n",
      "Registering controller: PyTorchModelController\n",
      "Registering controller: TransformerMixinController\n",
      "Registering controller: YTransformerMixinController\n",
      "Registering controller: FeatureAugmentationController\n",
      "Registering controller: SampleAugmentationController\n",
      "Registering controller: CrossValidatorController\n",
      "Registering controller: SpectraChartController\n",
      "Registering controller: FoldChartController\n",
      "Registering controller: YChartController\n",
      "Empty predictions: üìà Predictions: No predictions stored\n",
      "Length: 0\n",
      "After adding: üìà Predictions: 1 entries\n",
      "   Datasets: ['test']\n",
      "   Pipelines: ['test_pipe']\n",
      "   Models: ['TestModel']\n",
      "‚úÖ Predictions class is working correctly!\n",
      "\n",
      "==================================================\n",
      "Now running the actual pipeline...\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è Dataset does not have data for train_group.\n",
      "‚ö†Ô∏è Dataset does not have data for test_group.\n",
      "\u001b[97müìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94müöÄ Starting pipeline config_demo_pipeline_00090c on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94müîÑ Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92müî∑ Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "üíæ Saved 1_0_MinMaxScaler_1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 2: feature_augmentation\u001b[0m\n",
      "üîπ Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1\n",
      "\u001b[96m   ‚ñ∂ Skipping no-op feature augmentation\u001b[0m\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator Gaussian\n",
      "üíæ Saved 2_1_0_Gaussian_2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94müîÑ Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator StandardScaler\n",
      "üíæ Saved 2_3_0_StandardScaler_3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator Haar\n",
      "üíæ Saved 2_4_0_Haar_4.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "üîπ Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "üíæ Saved 3_folds_ShuffleSplit.csv to results\\sample_data\\config_demo_pipeline_00090c\\3_folds_ShuffleSplit.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 4: y_processing\u001b[0m\n",
      "üîπ Executing controller YTransformerMixinController with operator StandardScaler\n",
      "üíæ Saved 4_StandardScaler_numeric_StandardScaler5.pkl to results\\sample_data\\config_demo_pipeline_00090c\\4_StandardScaler_numeric_StandardScaler5.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "üîπ Executing controller SklearnModelController with operator PLSRegression\n",
      "üîç Optimizing 1 parameters with random search (20 trials)...\n",
      "(291, 6453) (291,) (33, 6453) (33, 1)\n",
      "üèÜ Training with best parameters: {'n_components': 20}\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "üíæ Saved 5_finetuned_PLSRegression_6.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_finetuned_PLSRegression_6.pkl\n",
      "üíæ Saved 5_predictions_finetuned_7.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_finetuned_7.csv\n",
      "üíæ Saved 5_trained_PLSRegression_8_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_8_simple_cv_fold1.pkl\n",
      "üíæ Saved 5_predictions_trained_9_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_9_simple_cv_fold1.csv\n",
      "üíæ Saved 5_trained_PLSRegression_10_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_10_simple_cv_fold2.pkl\n",
      "üíæ Saved 5_predictions_trained_11_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_11_simple_cv_fold2.csv\n",
      "üíæ Saved 5_trained_PLSRegression_12_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_12_simple_cv_fold3.pkl\n",
      "üíæ Saved 5_predictions_trained_13_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_13_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 6: (finetune) nicon()\u001b[0m\n",
      "üîπ Executing controller TensorFlowModelController without operator\n",
      "üîç Optimizing 3 parameters with random search (2 trials)...\n",
      "(291, 2151, 3) (291,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "üíæ Saved 6_finetuned_Sequential_14.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_finetuned_Sequential_14.pkl\n",
      "üíæ Saved 6_predictions_finetuned_15.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_finetuned_15.csv\n",
      "üíæ Saved 6_trained_Sequential_16_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_16_simple_cv_fold1.pkl\n",
      "üíæ Saved 6_predictions_trained_17_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_17_simple_cv_fold1.csv\n",
      "üíæ Saved 6_trained_Sequential_18_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_18_simple_cv_fold2.pkl\n",
      "üíæ Saved 6_predictions_trained_19_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_19_simple_cv_fold2.csv\n",
      "üíæ Saved 6_trained_Sequential_20_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_20_simple_cv_fold3.pkl\n",
      "üíæ Saved 6_predictions_trained_21_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_21_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m‚úÖ Pipeline config_demo_pipeline_00090c completed successfully on dataset sample_data\u001b[0m\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Final dataset predictions: üìà Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n"
     ]
    }
   ],
   "source": [
    "# Fresh reload - restart kernel then run this first\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear any cached modules\n",
    "modules_to_clear = [k for k in sys.modules.keys() if k.startswith('nirs4all')]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Now reimport everything\n",
    "from nirs4all.dataset.predictions import Predictions\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "# Test the new predictions class first\n",
    "test_predictions = Predictions()\n",
    "print(f\"Empty predictions: {test_predictions}\")\n",
    "print(f\"Length: {len(test_predictions)}\")\n",
    "\n",
    "# Test adding a prediction\n",
    "import numpy as np\n",
    "test_predictions.add_prediction(\n",
    "    dataset=\"test\",\n",
    "    pipeline=\"test_pipe\",\n",
    "    model=\"TestModel\",\n",
    "    partition=\"test\",\n",
    "    y_true=np.array([1.0, 2.0]),\n",
    "    y_pred=np.array([1.1, 2.1])\n",
    ")\n",
    "\n",
    "print(f\"After adding: {test_predictions}\")\n",
    "print(\"‚úÖ Predictions class is working correctly!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Now running the actual pipeline...\")\n",
    "\n",
    "# Run the pipeline\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "try:\n",
    "    res_dataset, history, pipeline = runner.run(config, data)\n",
    "    print(f\"\\nPipeline completed successfully!\")\n",
    "    print(f\"Final dataset predictions: {res_dataset._predictions}\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d0840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Results Analysis:\n",
      "==================================================\n",
      "Final dataset: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\n",
      "\n",
      "Predictions stored: üìà Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n",
      "Number of predictions: 8\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2']\n",
      "Datasets: ['sample_data']\n",
      "Pipelines: ['config_demo_pipeline_00090c']\n",
      "Models: ['PLSRegression', 'function', 'Sequential']\n",
      "Partitions: ['test', 'test_fold_0', 'test_fold_1', 'test_fold_2']\n",
      "\n",
      "First prediction details:\n",
      "  Dataset: sample_data\n",
      "  Pipeline: config_demo_pipeline_00090c\n",
      "  Model: PLSRegression\n",
      "  Partition: test\n",
      "  Y_true shape: (33, 1)\n",
      "  Y_pred shape: (33, 1)\n",
      "  Sample indices: 33 samples\n",
      "  Fold index: None\n",
      "  Metadata: {'y_processing': 'numeric_StandardScaler5', 'model_type': 'PLSRegression', 'partition': 'test'}\n"
     ]
    }
   ],
   "source": [
    "# Check the pipeline results and predictions\n",
    "print(\"Pipeline Results Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Final dataset: {res_dataset}\")\n",
    "print(f\"\\nPredictions stored: {res_dataset._predictions}\")\n",
    "print(f\"Number of predictions: {len(res_dataset._predictions)}\")\n",
    "\n",
    "if len(res_dataset._predictions) > 0:\n",
    "    print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "    print(f\"Datasets: {res_dataset._predictions.list_datasets()}\")\n",
    "    print(f\"Pipelines: {res_dataset._predictions.list_pipelines()}\")\n",
    "    print(f\"Models: {res_dataset._predictions.list_models()}\")\n",
    "    print(f\"Partitions: {res_dataset._predictions.list_partitions()}\")\n",
    "\n",
    "    # Get details of first prediction\n",
    "    keys = res_dataset._predictions.list_keys()\n",
    "    if keys:\n",
    "        first_key = keys[0]\n",
    "        first_pred_parts = first_key.split('_', 3)  # Split into 4 parts max\n",
    "        if len(first_pred_parts) >= 4:\n",
    "            dataset_name, pipeline_name, model_name, partition_name = first_pred_parts\n",
    "            first_pred = res_dataset._predictions.get_prediction_data(\n",
    "                dataset_name, pipeline_name, model_name, partition_name\n",
    "            )\n",
    "            if first_pred:\n",
    "                print(f\"\\nFirst prediction details:\")\n",
    "                print(f\"  Dataset: {first_pred['dataset']}\")\n",
    "                print(f\"  Pipeline: {first_pred['pipeline']}\")\n",
    "                print(f\"  Model: {first_pred['model']}\")\n",
    "                print(f\"  Partition: {first_pred['partition']}\")\n",
    "                print(f\"  Y_true shape: {first_pred['y_true'].shape}\")\n",
    "                print(f\"  Y_pred shape: {first_pred['y_pred'].shape}\")\n",
    "                print(f\"  Sample indices: {len(first_pred['sample_indices'])} samples\")\n",
    "                print(f\"  Fold index: {first_pred['fold_idx']}\")\n",
    "                print(f\"  Metadata: {first_pred['metadata']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No predictions were stored during pipeline execution\")\n",
    "    print(\"This might be because:\")\n",
    "    print(\"1. The prediction storage integration is not working correctly\")\n",
    "    print(\"2. The dataset parameter is not being passed properly\")\n",
    "    print(\"3. There were errors in the model training that prevented prediction storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Analysis Example:\n",
      "==================================================\n",
      "PLSRegression predictions: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2']\n",
      "\n",
      "PLS Test Set Performance:\n",
      "  MSE: 15.1360\n",
      "  MAE: 1.1331\n",
      "  R¬≤:  0.9624\n",
      "  Y processing: numeric_StandardScaler5\n",
      "\n",
      "PLS Cross-Validation Combined Performance:\n",
      "  Samples: 99\n",
      "  MSE: 415.0967\n",
      "  MAE: 14.9935\n",
      "  R¬≤:  0.2374\n",
      "  Folds: 3\n",
      "\n",
      "Model Comparison (Test Set):\n",
      "  PLSRegression: MSE=15.1360, R¬≤=0.9624\n",
      "  TestModel: MSE=0.0200, R¬≤=0.9700\n",
      "  Sequential: MSE=446.0420, R¬≤=-0.1083\n",
      "\n",
      "‚úÖ Prediction storage and analysis system is fully functional!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate prediction analysis capabilities\n",
    "print(\"Prediction Analysis Example:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get all predictions from PLSRegression model\n",
    "pls_predictions = res_dataset._predictions.get_predictions(model=\"PLSRegression\")\n",
    "print(f\"PLSRegression predictions: {list(pls_predictions.keys())}\")\n",
    "\n",
    "# Get specific test set prediction for PLS\n",
    "pls_test = res_dataset._predictions.get_prediction_data(\n",
    "    \"sample_data\", \"config_demo_pipeline_00090c\", \"PLSRegression\", \"test\"\n",
    ")\n",
    "\n",
    "if pls_test:\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "    y_true = pls_test['y_true'].flatten()\n",
    "    y_pred = pls_test['y_pred'].flatten()\n",
    "\n",
    "    print(f\"\\nPLS Test Set Performance:\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"  R¬≤:  {r2_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Y processing: {pls_test['metadata']['y_processing']}\")\n",
    "\n",
    "# Combine cross-validation folds for PLS\n",
    "pls_cv_combined = res_dataset._predictions.combine_folds(\n",
    "    \"sample_data\", \"config_demo_pipeline_00090c\", \"PLSRegression\", \"test_fold\"\n",
    ")\n",
    "\n",
    "if pls_cv_combined:\n",
    "    y_true_cv = pls_cv_combined['y_true'].flatten()\n",
    "    y_pred_cv = pls_cv_combined['y_pred'].flatten()\n",
    "\n",
    "    print(f\"\\nPLS Cross-Validation Combined Performance:\")\n",
    "    print(f\"  Samples: {len(y_true_cv)}\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  R¬≤:  {r2_score(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  Folds: {pls_cv_combined['metadata']['num_folds']}\")\n",
    "\n",
    "# Compare all models on test set\n",
    "print(f\"\\nModel Comparison (Test Set):\")\n",
    "models = res_dataset._predictions.list_models()\n",
    "for model in models:\n",
    "    test_pred = res_dataset._predictions.get_predictions(\n",
    "        model=model, partition=\"test\"\n",
    "    )\n",
    "    if test_pred:\n",
    "        first_key = list(test_pred.keys())[0]\n",
    "        pred_data = test_pred[first_key]\n",
    "        y_true = pred_data['y_true'].flatten()\n",
    "        y_pred = pred_data['y_pred'].flatten()\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\"  {model}: MSE={mse:.4f}, R¬≤={r2:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Prediction storage and analysis system is fully functional!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering controller: DummyController\n",
      "Registering controller: SklearnModelController\n",
      "Registering controller: TensorFlowModelController\n",
      "Registering controller: PyTorchModelController\n",
      "Registering controller: TransformerMixinController\n",
      "Registering controller: YTransformerMixinController\n",
      "Registering controller: FeatureAugmentationController\n",
      "Registering controller: SampleAugmentationController\n",
      "Registering controller: CrossValidatorController\n",
      "Registering controller: SpectraChartController\n",
      "Registering controller: FoldChartController\n",
      "Registering controller: YChartController\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è Dataset does not have data for train_group.\n",
      "‚ö†Ô∏è Dataset does not have data for test_group.\n",
      "\u001b[97müìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94müöÄ Starting pipeline config_demo_pipeline_00090c on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94müîÑ Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92müî∑ Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "üíæ Saved 1_0_MinMaxScaler_1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 2: feature_augmentation\u001b[0m\n",
      "üîπ Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1\n",
      "\u001b[96m   ‚ñ∂ Skipping no-op feature augmentation\u001b[0m\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator Gaussian\n",
      "üíæ Saved 2_1_0_Gaussian_2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94müîÑ Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator StandardScaler\n",
      "üíæ Saved 2_3_0_StandardScaler_3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   ‚ñ∂ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "üîπ Executing controller TransformerMixinController with operator Haar\n",
      "üíæ Saved 2_4_0_Haar_4.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "üîπ Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "üíæ Saved 3_folds_ShuffleSplit.csv to results\\sample_data\\config_demo_pipeline_00090c\\3_folds_ShuffleSplit.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 4: y_processing\u001b[0m\n",
      "üîπ Executing controller YTransformerMixinController with operator StandardScaler\n",
      "üíæ Saved 4_StandardScaler_numeric_StandardScaler5.pkl to results\\sample_data\\config_demo_pipeline_00090c\\4_StandardScaler_numeric_StandardScaler5.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: üìä Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "üîπ Executing controller SklearnModelController with operator PLSRegression\n",
      "üîç Optimizing 1 parameters with random search (20 trials)...\n",
      "(291, 6453) (291,) (33, 6453) (33, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Training with best parameters: {'n_components': 18}\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "üíæ Saved 5_finetuned_PLSRegression_6.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_finetuned_PLSRegression_6.pkl\n",
      "üíæ Saved 5_predictions_finetuned_7.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_finetuned_7.csv\n",
      "üíæ Saved 5_trained_PLSRegression_8_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_8_simple_cv_fold1.pkl\n",
      "üíæ Saved 5_predictions_trained_9_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_9_simple_cv_fold1.csv\n",
      "üíæ Saved 5_trained_PLSRegression_10_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_10_simple_cv_fold2.pkl\n",
      "üíæ Saved 5_predictions_trained_11_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_11_simple_cv_fold2.csv\n",
      "üíæ Saved 5_trained_PLSRegression_12_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_12_simple_cv_fold3.pkl\n",
      "üíæ Saved 5_predictions_trained_13_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_13_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92müî∑ Step 6: (finetune) nicon()\u001b[0m\n",
      "üîπ Executing controller TensorFlowModelController without operator\n",
      "üîç Optimizing 3 parameters with random search (2 trials)...\n",
      "(291, 2151, 3) (291,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF64D24D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF64D24D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "üíæ Saved 6_finetuned_Sequential_14.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_finetuned_Sequential_14.pkl\n",
      "üíæ Saved 6_predictions_finetuned_15.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_finetuned_15.csv\n",
      "üíæ Saved 6_trained_Sequential_16_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_16_simple_cv_fold1.pkl\n",
      "üíæ Saved 6_predictions_trained_17_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_17_simple_cv_fold1.csv\n",
      "üíæ Saved 6_trained_Sequential_18_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_18_simple_cv_fold2.pkl\n",
      "üíæ Saved 6_predictions_trained_19_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_19_simple_cv_fold2.csv\n",
      "üíæ Saved 6_trained_Sequential_20_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_20_simple_cv_fold3.pkl\n",
      "üíæ Saved 6_predictions_trained_21_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_21_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m‚úÖ Pipeline config_demo_pipeline_00090c completed successfully on dataset sample_data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "\n",
    "# json_config = PipelineConfig(\"sample.json\")\n",
    "# yaml_config = PipelineConfig(\"sample.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Predictions functionality...\n",
      "\n",
      "Dataset before running:\n",
      "Predictions: üìà Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n",
      "\n",
      "Number of predictions stored: 8\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2']\n",
      "\n",
      "After adding test prediction:\n",
      "Number of predictions: 9\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2', 'test_dataset_test_pipeline_TestModel_test']\n",
      "\n",
      "Test prediction data: {'dataset': 'test_dataset', 'pipeline': 'test_pipeline', 'model': 'TestModel', 'partition': 'test', 'y_true': array([1., 2., 3.]), 'y_pred': array([1.1, 1.9, 3.2]), 'sample_indices': [0, 1, 2], 'fold_idx': None, 'metadata': {'test': True}}\n"
     ]
    }
   ],
   "source": [
    "# Test the new Predictions functionality\n",
    "print(\"Testing Predictions functionality...\")\n",
    "print(\"\\nDataset before running:\")\n",
    "print(f\"Predictions: {res_dataset._predictions}\")\n",
    "\n",
    "# Let's check if any predictions were stored\n",
    "print(f\"\\nNumber of predictions stored: {len(res_dataset._predictions)}\")\n",
    "print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "\n",
    "# Try to manually add a test prediction\n",
    "import numpy as np\n",
    "\n",
    "res_dataset._predictions.add_prediction(\n",
    "    dataset=\"test_dataset\",\n",
    "    pipeline=\"test_pipeline\",\n",
    "    model=\"TestModel\",\n",
    "    partition=\"test\",\n",
    "    y_true=np.array([1.0, 2.0, 3.0]),\n",
    "    y_pred=np.array([1.1, 1.9, 3.2]),\n",
    "    metadata={\"test\": True}\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter adding test prediction:\")\n",
    "print(f\"Number of predictions: {len(res_dataset._predictions)}\")\n",
    "print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "\n",
    "# Get the test prediction\n",
    "test_pred = res_dataset._predictions.get_prediction_data(\n",
    "    \"test_dataset\", \"test_pipeline\", \"TestModel\", \"test\"\n",
    ")\n",
    "print(f\"\\nTest prediction data: {test_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
