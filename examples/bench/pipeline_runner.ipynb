{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb8aa524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing TensorFlow controller with train_params...\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94m🚀 Starting pipeline config_tf_train_params_test_8bdc1c on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m🔄 Running 3 steps in sequential mode\u001b[0m\n",
      "\u001b[92m🔷 Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "💾 Saved 1_0_MinMaxScaler.pkl to results\\sample_data\\config_tf_train_params_test_8bdc1c\\1_0_MinMaxScaler.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 2: y_processing\u001b[0m\n",
      "🔹 Executing controller YTransformerMixinController with operator StandardScaler\n",
      "💾 Saved 2_StandardScaler_numeric_StandardScaler2.pkl to results\\sample_data\\config_tf_train_params_test_8bdc1c\\2_StandardScaler_numeric_StandardScaler2.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler2'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler2: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 3: train nicon()\u001b[0m\n",
      "🔹 Executing controller TensorFlowModelController without operator\n",
      "🧠 Executing TensorFlow model controller\n",
      "🤖 Executing model controller: TensorFlowModelController\n",
      "🎯 Model mode: train\n",
      "📊 Data shapes - Train: X(130, 2151), y(130, 1) | Test: X(189, 2151), y(189, 1)\n",
      "🏋️ Training model...\n",
      "🔗 Model function nicon will be instantiated during training\n",
      "📊 Reshaping 2D input (130, 2151) to 3D for TensorFlow CNN\n",
      "📊 TensorFlow data prepared: X.shape=(130, 2151, 1), y.shape=(130,)\n",
      "📊 Reshaping 2D input (189, 2151) to 3D for TensorFlow CNN\n",
      "📊 TensorFlow data prepared: X.shape=(189, 2151, 1), y.shape=(189,)\n",
      "🏗️ Creating TensorFlow model from function nicon with input_shape=(2151, 1)\n",
      "🏗️ Creating TensorFlow model from function nicon with input_shape=(2151, 1)\n",
      "🧠 Training Sequential with TensorFlow\n",
      "🔧 Training parameters: {'epochs': 10, 'patience': 5, 'learning_rate': 0.001, 'optimizer': 'adam', 'loss': 'mse', 'metrics': ['mae', 'mse'], 'early_stopping': True, 'best_model_memory': True, 'cyclic_lr': True, 'step_size': 5, 'batch_size': 8, 'verbose': 0}\n",
      "🔧 Created Adam optimizer with lr=0.001\n",
      "🏗️ Model compiled with: {'optimizer': <keras.src.optimizers.adam.Adam object at 0x000001A384D03A90>, 'loss': 'mse', 'metrics': ['mae', 'mse']}\n",
      "🛑 Added EarlyStopping: monitor=val_loss, patience=5\n",
      "🔄 Added CyclicLR: base_lr=0.0001, max_lr=0.01, step_size=5\n",
      "🏆 Added BestModelMemory callback\n",
      "🏋️ Training configuration:\n",
      "   - Epochs: 10\n",
      "   - Batch size: 8\n",
      "   - Optimizer: adam (lr=0.001)\n",
      "   - Loss: mse\n",
      "   - Metrics: ['mae', 'mse']\n",
      "   - Validation split: 0.2\n",
      "   - Callbacks: 3 configured\n",
      "     * EarlyStopping (patience=5)\n",
      "     * CyclicLR\n",
      "     * BestModelMemory\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0020799999999999994.\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.004059999999999999.\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.006040000000000002.\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008020000000000001.\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.01.\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.008020000000000001.\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006040000000000002.\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "🏆 Restored best weights with val_loss=0.6459\n",
      "✅ Training completed successfully\n",
      "💾 Saved 3_trained_Sequential_3.pkl to results\\sample_data\\config_tf_train_params_test_8bdc1c\\3_trained_Sequential_3.pkl\n",
      "💾 Saved 3_predictions_trained_4.csv to results\\sample_data\\config_tf_train_params_test_8bdc1c\\3_predictions_trained_4.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m✅ Pipeline config_tf_train_params_test_8bdc1c completed successfully on dataset sample_data\u001b[0m\n",
      "\n",
      "✅ SUCCESS: TensorFlow train_params are now working!\n",
      "    - epochs: 500 should be used instead of default 100\n",
      "    - patience: 100 should be used instead of default 10\n"
     ]
    }
   ],
   "source": [
    "# Test TensorFlow train_params specifically\n",
    "%reload_ext autoreload\n",
    "\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from sample import dataset_config\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from nirs4all.operators.models.cirad_tf import nicon\n",
    "\n",
    "print(\"🧪 Testing TensorFlow controller with train_params...\")\n",
    "\n",
    "# Test with your exact configuration\n",
    "test_data = get_dataset(dataset_config)\n",
    "tf_config = {\n",
    "    \"pipeline\": [\n",
    "        MinMaxScaler(feature_range=(0.1, 0.8)),\n",
    "        {\"y_processing\": StandardScaler()},\n",
    "        {\n",
    "            \"model\": nicon,\n",
    "            \"train_params\": {\n",
    "                \"epochs\": 10,\n",
    "                \"patience\": 5,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"loss\": \"mse\",\n",
    "                \"metrics\": [\"mae\", \"mse\"],\n",
    "                \"early_stopping\": True,\n",
    "                # \"reduce_lr_on_plateau\": True,\n",
    "                # \"reduce_lr_factor\": 0.5,\n",
    "                # \"reduce_lr_patience\": 3,\n",
    "                \"best_model_memory\": True,\n",
    "                \"cyclic_lr\": True,\n",
    "                \"step_size\": 5,\n",
    "                \"batch_size\": 8,\n",
    "                \"verbose\": 0\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = PipelineConfig(tf_config, \"tf_train_params_test\")\n",
    "runner = PipelineRunner()\n",
    "\n",
    "try:\n",
    "    result = runner.run(config, test_data)\n",
    "    print(\"\\n✅ SUCCESS: TensorFlow train_params are now working!\")\n",
    "    print(\"    - epochs: 500 should be used instead of default 100\")\n",
    "    print(\"    - patience: 100 should be used instead of default 10\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94m🚀 Starting pipeline config_demo_pipeline_9a390a on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m🔄 Running 3 steps in sequential mode\u001b[0m\n",
      "\u001b[92m🔷 Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "💾 Saved 1_0_MinMaxScaler.pkl to results\\sample_data\\config_demo_pipeline_9a390a\\1_0_MinMaxScaler.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 2: y_processing\u001b[0m\n",
      "🔹 Executing controller YTransformerMixinController with operator StandardScaler\n",
      "💾 Saved 2_StandardScaler_numeric_StandardScaler2.pkl to results\\sample_data\\config_demo_pipeline_9a390a\\2_StandardScaler_numeric_StandardScaler2.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler2'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler2: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 3: train nicon()\u001b[0m\n",
      "🔹 Executing controller TensorFlowModelController without operator\n",
      "🧠 Executing TensorFlow model controller\n",
      "🤖 Executing model controller: TensorFlowModelController\n",
      "🎯 Model mode: train\n",
      "📊 Data shapes - Train: X(130, 2151), y(130, 1) | Test: X(189, 2151), y(189, 1)\n",
      "🏋️ Training model...\n",
      "🔗 Model function nicon will be instantiated during training\n",
      "📊 Reshaping 2D input (130, 2151) to 3D for TensorFlow CNN\n",
      "📊 TensorFlow data prepared: X.shape=(130, 2151, 1), y.shape=(130,)\n",
      "📊 Reshaping 2D input (189, 2151) to 3D for TensorFlow CNN\n",
      "📊 TensorFlow data prepared: X.shape=(189, 2151, 1), y.shape=(189,)\n",
      "🏗️ Creating TensorFlow model from function nicon with input_shape=(2151, 1)\n",
      "🏗️ Creating TensorFlow model from function nicon with input_shape=(2151, 1)\n",
      "🧠 Training Sequential with TensorFlow\n",
      "🔧 Training parameters: {'epochs': 100, 'patience': 10, 'batch_size': 16, 'cyclic_lr': True, 'step_size': 20, 'verbose': 0}\n",
      "🏗️ Model compiled with: {'optimizer': 'adam', 'loss': 'mse', 'metrics': ['mae']}\n",
      "🛑 Added EarlyStopping: monitor=val_loss, patience=10\n",
      "🔄 Added CyclicLR: base_lr=0.0001, max_lr=0.01, step_size=20\n",
      "🏆 Added BestModelMemory callback\n",
      "🏋️ Training configuration:\n",
      "   - Epochs: 100\n",
      "   - Batch size: 16\n",
      "   - Optimizer: adam\n",
      "   - Loss: mse\n",
      "   - Metrics: ['mae']\n",
      "   - Validation split: 0.2\n",
      "   - Callbacks: 3 configured\n",
      "     * EarlyStopping (patience=10)\n",
      "     * CyclicLR\n",
      "     * BestModelMemory\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0005950000000000005.\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001090000000000001.\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0015849999999999994.\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0020799999999999994.\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.002575.\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0030700000000000007.\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.003565000000000001.\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.004059999999999999.\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004555.\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.005050000000000001.\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.005545000000000001.\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.006040000000000002.\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.006535.\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.007030000000000001.\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.007525000000000001.\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.008020000000000001.\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.008515000000000002.\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.009009999999999999.\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.009505.\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.01.\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.009505.\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.009009999999999999.\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.008515000000000002.\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.008020000000000001.\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.007525000000000001.\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.007030000000000001.\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.006535.\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.006040000000000002.\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "🏆 Restored best weights with val_loss=0.6309\n",
      "✅ Training completed successfully\n",
      "💾 Saved 3_trained_Sequential_3.pkl to results\\sample_data\\config_demo_pipeline_9a390a\\3_trained_Sequential_3.pkl\n",
      "💾 Saved 3_predictions_trained_4.csv to results\\sample_data\\config_demo_pipeline_9a390a\\3_predictions_trained_4.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m✅ Pipeline config_demo_pipeline_9a390a completed successfully on dataset sample_data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "\n",
    "# json_config = PipelineConfig(\"sample.json\")\n",
    "# yaml_config = PipelineConfig(\"sample.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06187de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing nested configuration parsing...\n",
      "🔍 Testing model config extraction...\n",
      "✓ Extracted model config: {'model': RandomForestRegressor(random_state=42), 'train_params': {'oob_score': True, 'n_jobs': -1}, 'finetune_params': {'n_trials': 4, 'approach': 'grid', 'model_params': {'n_estimators': [10, 30], 'max_depth': [3, 5]}, 'train_params': {'n_jobs': 1}}, 'model_instance': RandomForestRegressor(random_state=42)}\n",
      "✓ Found train_params: {'oob_score': True, 'n_jobs': -1}\n",
      "✓ Found finetune_params: {'n_trials': 4, 'approach': 'grid', 'model_params': {'n_estimators': [10, 30], 'max_depth': [3, 5]}, 'train_params': {'n_jobs': 1}}\n",
      "✓ Found model_params in finetune_params: {'n_estimators': [10, 30], 'max_depth': [3, 5]}\n",
      "✓ Found train_params in finetune_params: {'n_jobs': 1}\n",
      "\n",
      "🔍 Testing hyperparameter sampling...\n",
      "✓ Sampled parameters: {'n_estimators': 10, 'max_depth': 3}\n",
      "  ✓ n_estimators: 10\n",
      "  ✓ max_depth: 3\n",
      "\n",
      "✅ Configuration structure parsing test completed successfully!\n",
      "The new nested structure (model_params within finetune_params) is working!\n"
     ]
    }
   ],
   "source": [
    "# Direct test of nested configuration structure (just test the config parsing)\n",
    "print(\"🧪 Testing nested configuration parsing...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Test configuration structure compatibility without running full pipeline\n",
    "# Just verify that the configuration parsing works correctly\n",
    "\n",
    "# Test nested structure config\n",
    "nested_config = {\n",
    "    \"model\": RandomForestRegressor(random_state=42),\n",
    "    \"train_params\": {\n",
    "        # Final training parameters\n",
    "        \"oob_score\": True,\n",
    "        \"n_jobs\": -1\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 4,\n",
    "        \"approach\": \"grid\",\n",
    "        \"model_params\": {\n",
    "            # Parameters to optimize\n",
    "            \"n_estimators\": [10, 30],\n",
    "            \"max_depth\": [3, 5]\n",
    "        },\n",
    "        \"train_params\": {\n",
    "            # Training parameters during trials\n",
    "            \"n_jobs\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test the extraction logic directly\n",
    "print(\"🔍 Testing model config extraction...\")\n",
    "from nirs4all.controllers.sklearn.op_model import SklearnModelController\n",
    "\n",
    "controller = SklearnModelController()\n",
    "extracted_config = controller._extract_model_config(nested_config)\n",
    "\n",
    "print(f\"✓ Extracted model config: {extracted_config}\")\n",
    "print(f\"✓ Found train_params: {extracted_config.get('train_params', {})}\")\n",
    "print(f\"✓ Found finetune_params: {extracted_config.get('finetune_params', {})}\")\n",
    "\n",
    "if 'finetune_params' in extracted_config:\n",
    "    finetune = extracted_config['finetune_params']\n",
    "    print(f\"✓ Found model_params in finetune_params: {finetune.get('model_params', {})}\")\n",
    "    print(f\"✓ Found train_params in finetune_params: {finetune.get('train_params', {})}\")\n",
    "\n",
    "# Test hyperparameter sampling with nested structure\n",
    "print(\"\\n🔍 Testing hyperparameter sampling...\")\n",
    "from unittest.mock import Mock\n",
    "\n",
    "mock_trial = Mock()\n",
    "mock_trial.suggest_categorical = Mock(side_effect=lambda name, choices: choices[0])\n",
    "\n",
    "finetune_params = nested_config['finetune_params']\n",
    "sampled_params = controller._sample_hyperparameters(mock_trial, finetune_params)\n",
    "\n",
    "print(f\"✓ Sampled parameters: {sampled_params}\")\n",
    "expected_params = ['n_estimators', 'max_depth']\n",
    "for param in expected_params:\n",
    "    if param in sampled_params:\n",
    "        print(f\"  ✓ {param}: {sampled_params[param]}\")\n",
    "    else:\n",
    "        print(f\"  ❌ Missing {param}\")\n",
    "\n",
    "print(\"\\n✅ Configuration structure parsing test completed successfully!\")\n",
    "print(\"The new nested structure (model_params within finetune_params) is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f52506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Testing backward compatibility with old flat structure...\n",
      "✓ Extracted old format config: {'model': RandomForestRegressor(random_state=42), 'train_params': {'oob_score': True, 'n_jobs': -1}, 'finetune_params': {'n_trials': 4, 'approach': 'grid', 'n_estimators': [10, 30], 'max_depth': [3, 5]}, 'model_instance': RandomForestRegressor(random_state=42)}\n",
      "✓ Sampled parameters from old format: {'n_estimators': 10, 'max_depth': 3}\n",
      "  ✓ n_estimators: 10\n",
      "  ✓ max_depth: 3\n",
      "\n",
      "✅ Backward compatibility test passed!\n",
      "Both old flat structure and new nested structure work correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test backward compatibility with old flat structure\n",
    "print(\"🔄 Testing backward compatibility with old flat structure...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Test old flat structure (parameters directly in finetune_params)\n",
    "old_flat_config = {\n",
    "    \"model\": RandomForestRegressor(random_state=42),\n",
    "    \"train_params\": {\n",
    "        \"oob_score\": True,\n",
    "        \"n_jobs\": -1\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        # Old style - parameters directly in finetune_params\n",
    "        \"n_trials\": 4,\n",
    "        \"approach\": \"grid\",\n",
    "        \"n_estimators\": [10, 30],  # Direct parameter\n",
    "        \"max_depth\": [3, 5]        # Direct parameter\n",
    "    }\n",
    "}\n",
    "\n",
    "controller = SklearnModelController()\n",
    "extracted_config = controller._extract_model_config(old_flat_config)\n",
    "\n",
    "print(f\"✓ Extracted old format config: {extracted_config}\")\n",
    "\n",
    "# Test hyperparameter sampling with old flat structure\n",
    "mock_trial = Mock()\n",
    "mock_trial.suggest_categorical = Mock(side_effect=lambda name, choices: choices[0])\n",
    "\n",
    "finetune_params = old_flat_config['finetune_params']\n",
    "sampled_params = controller._sample_hyperparameters(mock_trial, finetune_params)\n",
    "\n",
    "print(f\"✓ Sampled parameters from old format: {sampled_params}\")\n",
    "expected_params = ['n_estimators', 'max_depth']\n",
    "for param in expected_params:\n",
    "    if param in sampled_params:\n",
    "        print(f\"  ✓ {param}: {sampled_params[param]}\")\n",
    "    else:\n",
    "        print(f\"  ❌ Missing {param}\")\n",
    "\n",
    "print(\"\\n✅ Backward compatibility test passed!\")\n",
    "print(\"Both old flat structure and new nested structure work correctly!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
