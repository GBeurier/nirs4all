{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "âš ï¸ Dataset does not have data for train_group.\n",
      "âš ï¸ Dataset does not have data for test_group.\n",
      "\u001b[97mğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mğŸš€ Starting pipeline config_demo_pipeline_f32f08 on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94mğŸ”„ Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92mğŸ”· Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "ğŸ’¾ Saved 1_0_MinMaxScaler.pkl to results\\sample_data\\config_demo_pipeline_f32f08\\1_0_MinMaxScaler.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 2: y_processing\u001b[0m\n",
      "ğŸ”¹ Executing controller YTransformerMixinController with operator StandardScaler\n",
      "ğŸ’¾ Saved 2_StandardScaler_numeric_StandardScaler2.pkl to results\\sample_data\\config_demo_pipeline_f32f08\\2_StandardScaler_numeric_StandardScaler2.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler2'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler2: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 3: (finetune) RandomForestRegressor(max_depth=10, random_state=42)\u001b[0m\n",
      "ğŸ”¹ Executing controller SklearnModelController with operator RandomForestRegressor\n",
      "ğŸ” Optimizing 2 parameters with grid search (4 trials)...\n",
      "ğŸ’¾ Saved 3_finetuned_RandomForestRegressor_3.pkl to results\\sample_data\\config_demo_pipeline_f32f08\\3_finetuned_RandomForestRegressor_3.pkl\n",
      "ğŸ’¾ Saved 3_predictions_finetuned_4.csv to results\\sample_data\\config_demo_pipeline_f32f08\\3_predictions_finetuned_4.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 4: (finetune) PLSRegression()\u001b[0m\n",
      "ğŸ”¹ Executing controller SklearnModelController with operator PLSRegression\n",
      "ğŸ” Optimizing 1 parameters with random search (20 trials)...\n",
      "ğŸ† Training with best parameters: {'n_components': 17}\n",
      "ğŸ’¾ Saved 4_finetuned_PLSRegression_5.pkl to results\\sample_data\\config_demo_pipeline_f32f08\\4_finetuned_PLSRegression_5.pkl\n",
      "ğŸ’¾ Saved 4_predictions_finetuned_6.csv to results\\sample_data\\config_demo_pipeline_f32f08\\4_predictions_finetuned_6.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 5: train nicon()\u001b[0m\n",
      "ğŸ”¹ Executing controller TensorFlowModelController without operator\n",
      "ğŸ’¾ Saved 5_trained_Sequential_7.pkl to results\\sample_data\\config_demo_pipeline_f32f08\\5_trained_Sequential_7.pkl\n",
      "ğŸ’¾ Saved 5_predictions_trained_8.csv to results\\sample_data\\config_demo_pipeline_f32f08\\5_predictions_trained_8.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 6: (finetune) nicon()\u001b[0m\n",
      "ğŸ”¹ Executing controller TensorFlowModelController without operator\n",
      "ğŸ” Optimizing 3 parameters with random search (10 trials)...\n",
      "ğŸ’¾ Saved 6_finetuned_Sequential_9.pkl to results\\sample_data\\config_demo_pipeline_f32f08\\6_finetuned_Sequential_9.pkl\n",
      "ğŸ’¾ Saved 6_predictions_finetuned_10.csv to results\\sample_data\\config_demo_pipeline_f32f08\\6_predictions_finetuned_10.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94mâœ… Pipeline config_demo_pipeline_f32f08 completed successfully on dataset sample_data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "\n",
    "# json_config = PipelineConfig(\"sample.json\")\n",
    "# yaml_config = PipelineConfig(\"sample.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
