{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "âš ï¸ Dataset does not have data for train_group.\n",
      "âš ï¸ Dataset does not have data for test_group.\n",
      "\u001b[97mğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mğŸš€ Starting pipeline config_00090c97 on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94mğŸ”„ Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92mğŸ”· Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "ğŸ’¾ Saved 1_0_MinMaxScaler_1.pkl to results\\sample_data\\config_00090c97\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 2: feature_augmentation\u001b[0m\n",
      "ğŸ”¹ Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1, mode: train\n",
      "Skipping no-op feature augmentation\n",
      "\u001b[96m   â–¶ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator Gaussian\n",
      "ğŸ’¾ Saved 2_1_0_Gaussian_2.pkl to results\\sample_data\\config_00090c97\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   â–¶ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94mğŸ”„ Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   â–¶ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator StandardScaler\n",
      "ğŸ’¾ Saved 2_3_0_StandardScaler_3.pkl to results\\sample_data\\config_00090c97\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   â–¶ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator Haar\n",
      "ğŸ’¾ Saved 2_4_0_Haar_4.pkl to results\\sample_data\\config_00090c97\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "ğŸ”¹ Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "ğŸ’¾ Saved 3_folds_ShuffleSplit.csv to results\\sample_data\\config_00090c97\\3_folds_ShuffleSplit.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 4: y_processing\u001b[0m\n",
      "ğŸ”¹ Executing controller YTransformerMixinController with operator StandardScaler\n",
      "ğŸ’¾ Saved 4_StandardScaler_numeric_StandardScaler5.pkl to results\\sample_data\\config_00090c97\\4_StandardScaler_numeric_StandardScaler5.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "ğŸ”¹ Executing controller SklearnModelController with operator PLSRegression\n",
      "ğŸ” Optimizing 1 parameters with random search (20 trials)...\n",
      "(291, 6453) (291,) (33, 6453) (33, 1)\n",
      "ğŸ† Training with best parameters: {'n_components': 23}\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "ğŸ’¾ Saved 5_finetuned_PLSRegression_6.pkl to results\\sample_data\\config_00090c97\\5_finetuned_PLSRegression_6.pkl\n",
      "ğŸ’¾ Saved 5_predictions_finetuned_7.csv to results\\sample_data\\config_00090c97\\5_predictions_finetuned_7.csv\n",
      "ğŸ’¾ Saved 5_trained_PLSRegression_8_simple_cv_fold1.pkl to results\\sample_data\\config_00090c97\\5_trained_PLSRegression_8_simple_cv_fold1.pkl\n",
      "ğŸ’¾ Saved 5_predictions_trained_9_simple_cv_fold1.csv to results\\sample_data\\config_00090c97\\5_predictions_trained_9_simple_cv_fold1.csv\n",
      "ğŸ’¾ Saved 5_trained_PLSRegression_10_simple_cv_fold2.pkl to results\\sample_data\\config_00090c97\\5_trained_PLSRegression_10_simple_cv_fold2.pkl\n",
      "ğŸ’¾ Saved 5_predictions_trained_11_simple_cv_fold2.csv to results\\sample_data\\config_00090c97\\5_predictions_trained_11_simple_cv_fold2.csv\n",
      "ğŸ’¾ Saved 5_trained_PLSRegression_12_simple_cv_fold3.pkl to results\\sample_data\\config_00090c97\\5_trained_PLSRegression_12_simple_cv_fold3.pkl\n",
      "ğŸ’¾ Saved 5_predictions_trained_13_simple_cv_fold3.csv to results\\sample_data\\config_00090c97\\5_predictions_trained_13_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 6: (finetune) nicon()\u001b[0m\n",
      "ğŸ”¹ Executing controller TensorFlowModelController without operator\n",
      "ğŸ” Optimizing 3 parameters with random search (2 trials)...\n",
      "(291, 2151, 3) (291,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "ğŸ’¾ Saved 6_finetuned_Sequential_14.pkl to results\\sample_data\\config_00090c97\\6_finetuned_Sequential_14.pkl\n",
      "ğŸ’¾ Saved 6_predictions_finetuned_15.csv to results\\sample_data\\config_00090c97\\6_predictions_finetuned_15.csv\n",
      "ğŸ’¾ Saved 6_trained_Sequential_16_simple_cv_fold1.pkl to results\\sample_data\\config_00090c97\\6_trained_Sequential_16_simple_cv_fold1.pkl\n",
      "ğŸ’¾ Saved 6_predictions_trained_17_simple_cv_fold1.csv to results\\sample_data\\config_00090c97\\6_predictions_trained_17_simple_cv_fold1.csv\n",
      "ğŸ’¾ Saved 6_trained_Sequential_18_simple_cv_fold2.pkl to results\\sample_data\\config_00090c97\\6_trained_Sequential_18_simple_cv_fold2.pkl\n",
      "ğŸ’¾ Saved 6_predictions_trained_19_simple_cv_fold2.csv to results\\sample_data\\config_00090c97\\6_predictions_trained_19_simple_cv_fold2.csv\n",
      "ğŸ’¾ Saved 6_trained_Sequential_20_simple_cv_fold3.pkl to results\\sample_data\\config_00090c97\\6_trained_Sequential_20_simple_cv_fold3.pkl\n",
      "ğŸ’¾ Saved 6_predictions_trained_21_simple_cv_fold3.csv to results\\sample_data\\config_00090c97\\6_predictions_trained_21_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94mâœ… Pipeline config_00090c97 completed successfully on dataset sample_data\u001b[0m\n",
      "âœ… Simple transformers pipeline completed successfully!\n",
      "Final dataset: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config)\n",
    "runner = PipelineRunner()\n",
    "res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "print(\"âœ… Simple transformers pipeline completed successfully!\")\n",
    "print(f\"Final dataset: {res_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecc19bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "âš ï¸ Dataset does not have data for train_group.\n",
      "âš ï¸ Dataset does not have data for test_group.\n",
      "\u001b[97mğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "ğŸ”® Starting prediction mode for pipeline on dataset sample_data\n",
      "ğŸ“¦ Available binaries for 22 operations across 8 steps\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mğŸš€ Starting pipeline prediction_sample_data on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94mğŸ”„ Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92mğŸ”· Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_pred'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_pred']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_pred']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 2: feature_augmentation\u001b[0m\n",
      "ğŸ”¹ Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1, mode: predict\n",
      "Skipping no-op feature augmentation\n",
      "\u001b[96m   â–¶ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator Gaussian\n",
      "\u001b[96m   â–¶ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94mğŸ”„ Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   â–¶ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator StandardScaler\n",
      "\u001b[96m   â–¶ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "ğŸ”¹ Executing controller TransformerMixinController with operator Haar\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: ğŸ“Š Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_pred', 'raw_MinMaxScaler_pred_Gaussian_pred', 'raw_MinMaxScaler_pred_StandardScaler_pred_Haar_pred'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_pred', 'raw_MinMaxScaler_pred_Gaussian_pred', 'raw_MinMaxScaler_pred_StandardScaler_pred_Haar_pred']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_pred', 'raw_MinMaxScaler_pred_Gaussian_pred', 'raw_MinMaxScaler_pred_StandardScaler_pred_Haar_pred']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "Type mismatch: <class 'float'> != <class 'NoneType'>\n",
      "ğŸ”„ Skipping step 3 in prediction mode\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 4: y_processing\u001b[0m\n",
      "ğŸ”„ Skipping step 4 in prediction mode\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "ğŸ”¹ Executing controller SklearnModelController with operator PLSRegression\n",
      "ğŸ¯ Using all available data for prediction\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92mğŸ”· Step 6: (finetune) nicon()\u001b[0m\n",
      "ğŸ”¹ Executing controller TensorFlowModelController without operator\n",
      "ğŸ¯ Using all available data for prediction\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94mâœ… Pipeline prediction_sample_data completed successfully on dataset sample_data\u001b[0m\n",
      "âœ… Prediction completed successfully\n",
      "âœ… Prediction mode completed successfully!\n",
      "ğŸ¯ Generated 2 prediction records\n"
     ]
    }
   ],
   "source": [
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config)\n",
    "runner = PipelineRunner()\n",
    "\n",
    "pipeline_path = \"results/sample_data/config_00090c97/\"\n",
    "pred_dataset, pred_context = PipelineRunner.predict(\n",
    "    path=pipeline_path,\n",
    "    dataset=data,  # Use same data for testing\n",
    "    verbose=1\n",
    ")\n",
    "print(\"âœ… Prediction mode completed successfully!\")\n",
    "\n",
    "predictions_obj = pred_dataset._predictions\n",
    "pred_count = len(predictions_obj._predictions)\n",
    "print(f\"ğŸ¯ Generated {pred_count} prediction records\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
