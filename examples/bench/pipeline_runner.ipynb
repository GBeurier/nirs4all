{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      ">> Browsing ./sample_data\n",
      "No train_group file found for ./sample_data.\n",
      "No test_group file found for ./sample_data.\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "Python Dataset:\n",
      " FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 1, 2151), dtype=float32, processing_ids=['raw'], mean=0.46620577573776245, variance=0.1492786705493927)\n",
      "Index:\n",
      "shape: (189, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---        â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat        â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦          â”‚\n",
      "â”‚ 184 â”† 184    â”† null   â”† test      â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 185 â”† 185    â”† null   â”† test      â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 186 â”† 186    â”† null   â”† test      â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 187 â”† 187    â”† null   â”† test      â”† 0     â”† 0      â”† raw        â”‚\n",
      "â”‚ 188 â”† 188    â”† null   â”† test      â”† 0     â”† 0      â”† raw        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      " ======================================================================================================================================================================================================== \n",
      "Running Python Config:\n",
      "\n",
      "ðŸš€ Starting Pipeline Runner\n",
      "ðŸ”„ Running 3 steps in sequential mode\n",
      "ðŸ”„ Running steps sequentially with shared context\n",
      "ðŸ”¹ 0: Step Dict with 3 keys\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': 'raw'}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}\n",
      "ðŸ“¦ Deserializing dict operation: class\n",
      "ðŸ”„ Executing controller TransformerMixinController for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}, operator: MinMaxScaler(feature_range=(0.2, 0.8)), source: -1, context: {'branch': 0, 'processing': 'raw', 'step_id': 1}\n",
      "ðŸ”„ Running single operator MinMaxScaler(feature_range=(0.2, 0.8)) for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}, source: 0\n",
      "Executing transformer operation for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}, keyword: , source: 0\n",
      "130 ['raw'] [0]\n",
      "(130, 1, 2151)\n",
      "ðŸ”„ Fitting operator MinMaxScaler_1 with data shape: (130, 2151)\n",
      "189 ['raw'] [0]\n",
      "(189, 1, 2151)\n",
      "ðŸ”„ Transforming data with operator MinMaxScaler_1 with data shape: (189, 2151)\n",
      "âœ… Transformation complete, transformed data shape: (189, 2151)\n",
      "âœ… Successfully applied MinMaxScaler_1 transformation\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Dataset state after step 1:\n",
      "FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 1, 2151), dtype=float32, processing_ids=['MinMaxScaler_1'], mean=0.6808587312698364, variance=0.011851510033011436)\n",
      "Index:\n",
      "shape: (189, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing     â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---            â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat            â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦              â”‚\n",
      "â”‚ 184 â”† 184    â”† null   â”† test      â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 185 â”† 185    â”† null   â”† test      â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 186 â”† 186    â”† null   â”† test      â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 187 â”† 187    â”† null   â”† test      â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 188 â”† 188    â”† null   â”† test      â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Updated context after step: {'branch': 0, 'processing': 'MinMaxScaler_1', 'step_id': 1}\n",
      "ðŸ”¹ 1: Step feature_augmentation\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': 'MinMaxScaler_1', 'step_id': 1}\n",
      "ðŸ”¹ Step config: {'feature_augmentation': [None, 'nirs4all.operators.transformations.nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.signal.Gaussian']]}\n",
      "ðŸ“‹ Workflow operation: feature_augmentation\n",
      "ðŸ”„ Executing controller FeatureAugmentationController for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.signal.Gaussian']]}, operator: None, source: -1, context: {'branch': 0, 'processing': 'MinMaxScaler_1', 'step_id': 2}\n",
      "ðŸ”„ Running single operator None for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.signal.Gaussian']]}, source: 0\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.signal.Gaussian']]}, keyword: , source: 0\n",
      "189 ['MinMaxScaler_1'] [0]\n",
      "(189, 1, 2151)\n",
      "ðŸ”„ Running 3 steps in parallel mode\n",
      "ðŸ”„ Running steps in parallel with 4 workers\n",
      "ðŸ”¹ 2: Step No operation\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': 'MinMaxScaler_1', 'step_id': 2}\n",
      "ðŸ”¹ Step config: None\n",
      "ðŸ”¹ No operation defined for this step, skipping.\n",
      "ðŸ”¹ 3: Step nirs4all.operators.transformations.nirs.SavitzkyGolay\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': 'sample_augmentation_1', 'step_id': 2, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”¹ Step config: nirs4all.operators.transformations.nirs.SavitzkyGolay\n",
      "ðŸ“¦ Deserializing str operation: nirs4all.operators.transformations.nirs.SavitzkyGolay\n",
      "ðŸ”„ Executing controller TransformerMixinController for step: nirs4all.operators.transformations.nirs.SavitzkyGolay, operator: SavitzkyGolay(), source: -1, context: {'branch': 0, 'processing': 'sample_augmentation_1', 'step_id': 4, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”„ Running single operator SavitzkyGolay() for step: nirs4all.operators.transformations.nirs.SavitzkyGolay, source: 0\n",
      "Executing transformer operation for step: nirs4all.operators.transformations.nirs.SavitzkyGolay, keyword: , source: 0\n",
      "ðŸ”¹ 4: Step Sub-pipeline (2 steps)\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': 'sample_augmentation_2', 'step_id': 2, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”¹ Step config: ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.signal.Gaussian']\n",
      "ðŸ”— Sub-pipeline with 2 steps\n",
      "ðŸ”„ Running 2 steps in sequential mode\n",
      "ðŸ”„ Running steps sequentially with shared context\n",
      "ðŸ”¹ 5: Step sklearn.preprocessing._data.StandardScaler\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': 'sample_augmentation_2', 'step_id': 2, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”¹ Step config: sklearn.preprocessing._data.StandardScaler\n",
      "ðŸ“¦ Deserializing str operation: sklearn.preprocessing._data.StandardScaler\n",
      "ðŸ”„ Executing controller TransformerMixinController for step: sklearn.preprocessing._data.StandardScaler, operator: StandardScaler(), source: -1, context: {'branch': 0, 'processing': 'sample_augmentation_2', 'step_id': 6, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”„ Running single operator StandardScaler() for step: sklearn.preprocessing._data.StandardScaler, source: 0\n",
      "Executing transformer operation for step: sklearn.preprocessing._data.StandardScaler, keyword: , source: 0\n",
      "189 ['sample_augmentation_1'] [1]\n",
      "189 ['sample_augmentation_2'] [2]\n",
      "(189, 3, 2151)\n",
      "ðŸ”„ Fitting operator SavitzkyGolay_4 with data shape: (189, 2151)\n",
      "(189, 3, 2151)\n",
      "ðŸ”„ Fitting operator StandardScaler_6 with data shape: (189, 2151)\n",
      "189 ['sample_augmentation_1'] [1]\n",
      "(189, 3, 2151)\n",
      "ðŸ”„ Transforming data with operator SavitzkyGolay_4 with data shape: (189, 2151)\n",
      "189 ['sample_augmentation_2'] [2]\n",
      "âœ… Transformation complete, transformed data shape: (189, 2151)\n",
      "(189, 3, 2151)\n",
      "ðŸ”„ Transforming data with operator StandardScaler_6 with data shape: (189, 2151)\n",
      "âœ… Successfully applied SavitzkyGolay_4 transformation\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Dataset state after step 6:\n",
      "âœ… Transformation complete, transformed data shape: (189, 2151)\n",
      "âœ… Successfully applied StandardScaler_6 transformation\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Dataset state after step 6:\n",
      "FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 3, 2151), dtype=float32, processing_ids=['MinMaxScaler_1', 'SavitzkyGolay_4', 'StandardScaler_6'], mean=0.6808581352233887, variance=1.4011162519454956)\n",
      "Index:\n",
      "shape: (567, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing       â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---              â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat              â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦                â”‚\n",
      "â”‚ 562 â”† 184    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 563 â”† 185    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 564 â”† 186    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 565 â”† 187    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 566 â”† 188    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 3, 2151), dtype=float32, processing_ids=['MinMaxScaler_1', 'SavitzkyGolay_4', 'StandardScaler_6'], mean=1.861656429014147e-08, variance=0.9999998807907104)\n",
      "Index:\n",
      "shape: (567, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing       â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---              â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat              â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1   â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦                â”‚\n",
      "â”‚ 562 â”† 184    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 563 â”† 185    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 564 â”† 186    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 565 â”† 187    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â”‚ 566 â”† 188    â”† null   â”† train     â”† 0     â”† 0      â”† StandardScaler_6 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Updated context after step: {'branch': 0, 'processing': 'StandardScaler_6', 'step_id': 6, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”¹ 6: Step nirs4all.operators.transformations.signal.Gaussian\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': 'StandardScaler_6', 'step_id': 6, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”¹ Step config: nirs4all.operators.transformations.signal.Gaussian\n",
      "ðŸ“¦ Deserializing str operation: nirs4all.operators.transformations.signal.Gaussian\n",
      "ðŸ”„ Executing controller TransformerMixinController for step: nirs4all.operators.transformations.signal.Gaussian, operator: Gaussian(), source: -1, context: {'branch': 0, 'processing': 'StandardScaler_6', 'step_id': 7, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "ðŸ”„ Running single operator Gaussian() for step: nirs4all.operators.transformations.signal.Gaussian, source: 0\n",
      "Executing transformer operation for step: nirs4all.operators.transformations.signal.Gaussian, keyword: , source: 0\n",
      "189 ['StandardScaler_6'] [2]\n",
      "(189, 3, 2151)\n",
      "ðŸ”„ Fitting operator Gaussian_7 with data shape: (189, 2151)\n",
      "189 ['StandardScaler_6'] [2]\n",
      "(189, 3, 2151)\n",
      "ðŸ”„ Transforming data with operator Gaussian_7 with data shape: (189, 2151)\n",
      "âœ… Transformation complete, transformed data shape: (189, 2151)\n",
      "âœ… Successfully applied Gaussian_7 transformation\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Dataset state after step 7:\n",
      "FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 3, 2151), dtype=float32, processing_ids=['MinMaxScaler_1', 'SavitzkyGolay_4', 'Gaussian_7'], mean=6.719846033786947e-13, variance=8.480403266730718e-06)\n",
      "Index:\n",
      "shape: (567, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing     â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---            â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat            â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦              â”‚\n",
      "â”‚ 562 â”† 184    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 563 â”† 185    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 564 â”† 186    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 565 â”† 187    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 566 â”† 188    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Updated context after step: {'branch': 0, 'processing': 'Gaussian_7', 'step_id': 7, 'sample': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188]}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Dataset state after step 7:\n",
      "FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 3, 2151), dtype=float32, processing_ids=['MinMaxScaler_1', 'SavitzkyGolay_4', 'Gaussian_7'], mean=6.719846033786947e-13, variance=8.480403266730718e-06)\n",
      "Index:\n",
      "shape: (567, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing     â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---            â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat            â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦              â”‚\n",
      "â”‚ 562 â”† 184    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 563 â”† 185    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 564 â”† 186    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 565 â”† 187    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 566 â”† 188    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Dataset state after step 7:\n",
      "FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 3, 2151), dtype=float32, processing_ids=['MinMaxScaler_1', 'SavitzkyGolay_4', 'Gaussian_7'], mean=6.719846033786947e-13, variance=8.480403266730718e-06)\n",
      "Index:\n",
      "shape: (567, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing     â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---            â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat            â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦              â”‚\n",
      "â”‚ 562 â”† 184    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 563 â”† 185    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 564 â”† 186    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 565 â”† 187    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 566 â”† 188    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Updated context after step: {'branch': 0, 'processing': ['MinMaxScaler_1', 'SavitzkyGolay_4', 'Gaussian_7'], 'step_id': 2}\n",
      "ðŸ”¹ 7: Step Dict with 2 keys\n",
      "ðŸ”¹ Current context: {'branch': 0, 'processing': ['MinMaxScaler_1', 'SavitzkyGolay_4', 'Gaussian_7'], 'step_id': 2}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}\n",
      "ðŸ“¦ Deserializing dict operation: class\n",
      "ðŸ”„ Executing controller TransformerMixinController for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, operator: MinMaxScaler(), source: -1, context: {'branch': 0, 'processing': ['MinMaxScaler_1', 'SavitzkyGolay_4', 'Gaussian_7'], 'step_id': 8}\n",
      "ðŸ”„ Running single operator MinMaxScaler() for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, source: 0\n",
      "Executing transformer operation for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, keyword: , source: 0\n",
      "189 ['Gaussian_7', 'MinMaxScaler_1', 'SavitzkyGolay_4'] [0, 1, 2]\n",
      "(189, 3, 2151)\n",
      "âŒ Error applying transformation: shape mismatch: indexing arrays could not be broadcast together with shapes (189,) (3,) \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Dataset state after step 8:\n",
      "FeatureBlock with 1 sources and 189 samples\n",
      "Source 0: FeatureSource(shape=(189, 3, 2151), dtype=float32, processing_ids=['MinMaxScaler_1', 'SavitzkyGolay_4', 'Gaussian_7'], mean=6.719846033786947e-13, variance=8.480403266730718e-06)\n",
      "Index:\n",
      "shape: (567, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ row â”† sample â”† origin â”† partition â”† group â”† branch â”† processing     â”‚\n",
      "â”‚ --- â”† ---    â”† ---    â”† ---       â”† ---   â”† ---    â”† ---            â”‚\n",
      "â”‚ i32 â”† i32    â”† i32    â”† cat       â”† i8    â”† i8     â”† cat            â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0   â”† 0      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 1   â”† 1      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 2   â”† 2      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 3   â”† 3      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ 4   â”† 4      â”† null   â”† train     â”† 0     â”† 0      â”† MinMaxScaler_1 â”‚\n",
      "â”‚ â€¦   â”† â€¦      â”† â€¦      â”† â€¦         â”† â€¦     â”† â€¦      â”† â€¦              â”‚\n",
      "â”‚ 562 â”† 184    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 563 â”† 185    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 564 â”† 186    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 565 â”† 187    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â”‚ 566 â”† 188    â”† null   â”† train     â”† 0     â”† 0      â”† Gaussian_7     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "âŒ Pipeline failed: Pipeline step failed: shape mismatch: indexing arrays could not be broadcast together with shapes (189,) (3,) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "sys:1: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "sys:1: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 138, in _run_step\n",
      "    return self._execute_controller(controller, step, operator, dataset, context)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 195, in _execute_controller\n",
      "    return controller.execute(step, operator, dataset, context, self, source)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\sklearn\\op_transformermixin.py\", line 45, in execute\n",
      "    fit_data = dataset.x(train_context, \"2d\", source=source)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\dataset.py\", line 34, in x\n",
      "    return self.features.x(filter_dict, layout, source, src_concat)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\features.py\", line 70, in x\n",
      "    res.append(source_avai.get_features(indices, processings, layout))\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\feature_source.py\", line 130, in get_features\n",
      "    selected_data = self._array[indices, processings_indices, :].reshape(len(indices), -1)\n",
      "IndexError: shape mismatch: indexing arrays could not be broadcast together with shapes (189,) (3,) \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pipeline step failed: shape mismatch: indexing arrays could not be broadcast together with shapes (189,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:138\u001b[0m, in \u001b[0;36mPipelineRunner._run_step\u001b[1;34m(self, step, dataset, context)\u001b[0m\n\u001b[0;32m    137\u001b[0m     context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# self.history.complete_step(step_execution.step_id)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:195\u001b[0m, in \u001b[0;36mPipelineRunner._execute_controller\u001b[1;34m(self, controller, step, operator, dataset, context, source)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Running single operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\sklearn\\op_transformermixin.py:45\u001b[0m, in \u001b[0;36mTransformerMixinController.execute\u001b[1;34m(self, step, operator, dataset, context, runner, source)\u001b[0m\n\u001b[0;32m     44\u001b[0m train_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m fit_data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Fitting operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfit_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\dataset.py:34\u001b[0m, in \u001b[0;36mSpectroDataset.x\u001b[1;34m(self, filter_dict, layout, source, src_concat)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mx\u001b[39m(\u001b[38;5;28mself\u001b[39m, filter_dict: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}, layout: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, source: Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, src_concat: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_concat\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\features.py:70\u001b[0m, in \u001b[0;36mFeatures.x\u001b[1;34m(self, filter_dict, layout, source, src_concat)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(\u001b[43msource_avai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src_concat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\dataset\\feature_source.py:130\u001b[0m, in \u001b[0;36mFeatureSource.get_features\u001b[1;34m(self, indices, processings, layout)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_array[indices, :, :]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 130\u001b[0m     selected_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessings_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(indices), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m layout \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2d_interleaved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (189,) (3,) ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# json_config = PipelineConfig(\"sample.json\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# yaml_config = PipelineConfig(\"sample.yaml\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m runner \u001b[38;5;241m=\u001b[39m PipelineRunner(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, continue_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m res_dataset, history, pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpy_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_py\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:49\u001b[0m, in \u001b[0;36mPipelineRunner.run\u001b[1;34m(self, config, dataset)\u001b[0m\n\u001b[0;32m     46\u001b[0m context \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbranch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# self.history.complete_execution()\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Pipeline completed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:75\u001b[0m, in \u001b[0;36mPipelineRunner.run_steps\u001b[1;34m(self, steps, dataset, context, execution)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Running steps sequentially with shared context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m---> 75\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”¹ Updated context after step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:151\u001b[0m, in \u001b[0;36mPipelineRunner._run_step\u001b[1;34m(self, step, dataset, context)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš ï¸ Step failed but continuing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline step failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Pipeline step failed: shape mismatch: indexing arrays could not be broadcast together with shapes (189,) (3,) "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import SpectroDataset\n",
    "from sample import config as python_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "dataset_py = get_dataset(python_config[\"dataset\"])\n",
    "print(\"\\n\", \"=\"*200, \"\\nPython Dataset:\\n\", dataset_py)\n",
    "\n",
    "print(\"\\n\", \"=\"*200, \"\\nRunning Python Config:\\n\")\n",
    "py_config = PipelineConfig(python_config)\n",
    "# json_config = PipelineConfig(\"sample.json\")\n",
    "# yaml_config = PipelineConfig(\"sample.yaml\")\n",
    "\n",
    "runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "res_dataset, history, pipeline = runner.run(py_config, dataset_py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd8910",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (2115837227.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    first = array([1,2,3,6,5], :, :]\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.zeros((100,100,100))\n",
    "first = array[[1,2,3,6,5], :, :]\n",
    "print(first.shape)\n",
    "second = first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
