{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new GLOBAL_AVERAGE parameter strategy\n",
    "print(\"🌍 Testing GLOBAL_AVERAGE Parameter Strategy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from nirs4all.controllers.models.base_model_controller import ParamStrategy\n",
    "print(\"Available parameter strategies:\")\n",
    "for strategy in ParamStrategy:\n",
    "    print(f\"  - {strategy.value}\")\n",
    "\n",
    "# Create a simple test configuration using global_average\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "global_avg_config = {\n",
    "    \"name\": \"global_average_test\",\n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"pls_global_avg\",\n",
    "            \"controller\": \"sklearn\",\n",
    "            \"model\": PLSRegression(),\n",
    "            \"finetune_params\": {\n",
    "                \"cv_mode\": \"per_fold\",\n",
    "                \"param_strategy\": \"global_average\",  # ⭐ NEW STRATEGY\n",
    "                \"n_trials\": 5,  # Small number for quick test\n",
    "                \"verbose\": 1,\n",
    "                \"model_params\": {\n",
    "                    \"n_components\": (\"int\", 1, 8)\n",
    "                },\n",
    "                \"train_params\": {\n",
    "                    \"verbose\": 0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nGlobal Average Strategy:\")\n",
    "print(\"- Evaluates each parameter set on ALL folds simultaneously\")\n",
    "print(\"- Averages the validation scores across all folds\")\n",
    "print(\"- Selects the parameter set with best average performance\")\n",
    "print(\"- More computationally expensive but more generalizable\")\n",
    "print(f\"\\nConfiguration: {global_avg_config['steps'][0]['finetune_params']}\")\n",
    "\n",
    "# Test dataset (using smaller sample for quick demo)\n",
    "test_dataset_config = {\n",
    "    'source': ['sample_data/regression/Protein_NIR.xlsx'],\n",
    "    'y': 'Protein',\n",
    "    'folds': 3,  # Fewer folds for quick test\n",
    "    'train': 0.7,\n",
    "    'val': 0.15,\n",
    "    'test': 0.15,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"\\n🚀 Running global_average optimization...\")\n",
    "print(\"(This demonstrates simultaneous optimization across all folds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the global_average test\n",
    "import time\n",
    "\n",
    "# Load the test dataset\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "test_data = get_dataset(test_dataset_config)\n",
    "\n",
    "# Create and run the pipeline\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "\n",
    "config = PipelineConfig(global_avg_config, \"global_avg_test\")\n",
    "runner = PipelineRunner()\n",
    "\n",
    "print(\"🎯 Starting optimization with global_average strategy...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result_dataset, history, pipeline = runner.run(config, test_data)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    print(f\"✅ Global Average optimization completed in {execution_time:.1f} seconds!\")\n",
    "\n",
    "    # Analyze results\n",
    "    predictions = result_dataset._predictions\n",
    "    print(f\"📊 Generated {len(predictions)} prediction sets\")\n",
    "\n",
    "    if len(predictions) > 0:\n",
    "        # Get performance metrics\n",
    "        from sklearn.metrics import mean_squared_error, r2_score\n",
    "        import numpy as np\n",
    "\n",
    "        # Find the global average predictions\n",
    "        pred_keys = predictions.list_keys()\n",
    "        global_avg_preds = [k for k in pred_keys if 'global_avg' in k and 'test_fold' in k]\n",
    "\n",
    "        if global_avg_preds:\n",
    "            print(f\"\\nFound {len(global_avg_preds)} cross-validation predictions:\")\n",
    "            for key in global_avg_preds:\n",
    "                print(f\"  - {key}\")\n",
    "\n",
    "            # Combine all fold predictions\n",
    "            combined = predictions.combine_folds(\n",
    "                \"sample_data\", config.name, \"PLSRegression\", \"test_fold\"\n",
    "            )\n",
    "\n",
    "            if combined:\n",
    "                y_true = combined['y_true'].flatten()\n",
    "                y_pred = combined['y_pred'].flatten()\n",
    "\n",
    "                mse = mean_squared_error(y_true, y_pred)\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "\n",
    "                print(f\"\\n🎯 Global Average Cross-Validation Performance:\")\n",
    "                print(f\"  RMSE: {rmse:.4f}\")\n",
    "                print(f\"  R²:   {r2:.4f}\")\n",
    "                print(f\"  Samples: {len(y_true)}\")\n",
    "                print(f\"  Folds: {combined['metadata']['num_folds']}\")\n",
    "\n",
    "        print(f\"\\n✨ Key advantages of global_average:\")\n",
    "        print(f\"  ✓ Single optimal parameter set for all folds\")\n",
    "        print(f\"  ✓ Parameters optimized for average performance\")\n",
    "        print(f\"  ✓ More generalizable than per-fold optimization\")\n",
    "        print(f\"  ✓ Reduces fold-specific overfitting\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ No predictions were generated\")\n",
    "\n",
    "except Exception as e:\n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"❌ Test failed after {execution_time:.1f} seconds: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare global_average vs per_fold_best strategies\n",
    "print(\"🔬 Comparing Parameter Strategies\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def quick_strategy_test(strategy_name, n_trials=3):\n",
    "    \"\"\"Run a quick test of a parameter strategy.\"\"\"\n",
    "    print(f\"\\nTesting {strategy_name}...\")\n",
    "\n",
    "    test_config = {\n",
    "        \"name\": f\"test_{strategy_name}\",\n",
    "        \"steps\": [{\n",
    "            \"name\": \"pls_test\",\n",
    "            \"controller\": \"sklearn\",\n",
    "            \"model\": PLSRegression(),\n",
    "            \"finetune_params\": {\n",
    "                \"cv_mode\": \"per_fold\",\n",
    "                \"param_strategy\": strategy_name,\n",
    "                \"n_trials\": n_trials,\n",
    "                \"verbose\": 0,  # Silent for comparison\n",
    "                \"model_params\": {\n",
    "                    \"n_components\": (\"int\", 1, 6)\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    config = PipelineConfig(test_config, f\"test_{strategy_name}\")\n",
    "    runner = PipelineRunner()\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        result, _, _ = runner.run(config, test_data)\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        # Get performance\n",
    "        combined = result._predictions.combine_folds(\n",
    "            \"sample_data\", config.name, \"PLSRegression\", \"test_fold\"\n",
    "        )\n",
    "\n",
    "        if combined:\n",
    "            y_true = combined['y_true'].flatten()\n",
    "            y_pred = combined['y_pred'].flatten()\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "            return {\n",
    "                'strategy': strategy_name,\n",
    "                'time': elapsed,\n",
    "                'rmse': rmse,\n",
    "                'success': True\n",
    "            }\n",
    "        else:\n",
    "            return {'strategy': strategy_name, 'success': False, 'error': 'No predictions'}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'strategy': strategy_name, 'success': False, 'error': str(e)}\n",
    "\n",
    "# Test both strategies\n",
    "strategies_to_test = ['per_fold_best', 'global_average']\n",
    "results = []\n",
    "\n",
    "for strategy in strategies_to_test:\n",
    "    result = quick_strategy_test(strategy)\n",
    "    results.append(result)\n",
    "\n",
    "    if result['success']:\n",
    "        print(f\"  ✅ {strategy}: RMSE={result['rmse']:.4f}, Time={result['time']:.1f}s\")\n",
    "    else:\n",
    "        print(f\"  ❌ {strategy}: Failed - {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Summary comparison\n",
    "successful_results = [r for r in results if r['success']]\n",
    "if len(successful_results) >= 2:\n",
    "    print(f\"\\n📊 Strategy Comparison:\")\n",
    "\n",
    "    per_fold = next(r for r in successful_results if r['strategy'] == 'per_fold_best')\n",
    "    global_avg = next(r for r in successful_results if r['strategy'] == 'global_average')\n",
    "\n",
    "    time_ratio = global_avg['time'] / per_fold['time']\n",
    "    perf_diff = global_avg['rmse'] - per_fold['rmse']\n",
    "\n",
    "    print(f\"  Execution Time:\")\n",
    "    print(f\"    per_fold_best:  {per_fold['time']:.1f}s\")\n",
    "    print(f\"    global_average: {global_avg['time']:.1f}s ({time_ratio:.1f}x slower)\")\n",
    "\n",
    "    print(f\"  Performance (RMSE):\")\n",
    "    print(f\"    per_fold_best:  {per_fold['rmse']:.4f}\")\n",
    "    print(f\"    global_average: {global_avg['rmse']:.4f} ({'better' if perf_diff < 0 else 'worse'} by {abs(perf_diff):.4f})\")\n",
    "\n",
    "    if perf_diff < -0.001:  # Significantly better\n",
    "        print(f\"  🏆 global_average achieved better generalization!\")\n",
    "    elif abs(perf_diff) < 0.001:  # Similar performance\n",
    "        print(f\"  📊 Similar performance, but global_average provides more consistent parameters\")\n",
    "    else:\n",
    "        print(f\"  📈 per_fold_best achieved better performance on this dataset\")\n",
    "\n",
    "print(f\"\\n💡 When to use each strategy:\")\n",
    "print(f\"  per_fold_best:  Standard optimization, faster execution\")\n",
    "print(f\"  global_average: More generalizable parameters, production deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new use_full_train_for_final option\n",
    "print(\"🎯 Testing use_full_train_for_final Option\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"NEW FEATURE: use_full_train_for_final=True\")\n",
    "print(\"- Use cross-validation for hyperparameter optimization\")\n",
    "print(\"- Train final model on FULL combined training data\")\n",
    "print(\"- Get single unified model instead of fold-specific models\")\n",
    "\n",
    "# Configuration with full training option\n",
    "full_train_config = {\n",
    "    \"name\": \"full_train_demo\",\n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"pls_full_train\",\n",
    "            \"controller\": \"sklearn\",\n",
    "            \"model\": PLSRegression(),\n",
    "            \"finetune_params\": {\n",
    "                \"cv_mode\": \"per_fold\",\n",
    "                \"param_strategy\": \"global_average\",\n",
    "                \"use_full_train_for_final\": True,  # ⭐ NEW OPTION\n",
    "                \"n_trials\": 5,  # Quick demo\n",
    "                \"verbose\": 1,\n",
    "                \"model_params\": {\n",
    "                    \"n_components\": (\"int\", 1, 8)\n",
    "                },\n",
    "                \"train_params\": {\n",
    "                    \"verbose\": 0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nRunning with use_full_train_for_final=True...\")\n",
    "\n",
    "# Run the test\n",
    "config = PipelineConfig(full_train_config, \"full_train_test\")\n",
    "runner = PipelineRunner()\n",
    "\n",
    "start = time.time()\n",
    "result_dataset, _, _ = runner.run(config, test_data)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"✅ Completed in {elapsed:.1f} seconds\")\n",
    "\n",
    "# Check results\n",
    "predictions = result_dataset._predictions\n",
    "pred_keys = predictions.list_keys()\n",
    "\n",
    "print(f\"📊 Generated prediction keys: {pred_keys}\")\n",
    "\n",
    "# Look for full training predictions (should have different naming pattern)\n",
    "full_train_preds = [k for k in pred_keys if 'global_avg' in k or 'full' in k]\n",
    "\n",
    "if full_train_preds:\n",
    "    print(f\"\\n🎯 Full training predictions found: {full_train_preds}\")\n",
    "\n",
    "    # Get performance\n",
    "    key_parts = full_train_preds[0].split('_', 3)\n",
    "    if len(key_parts) >= 4:\n",
    "        pred_data = predictions.get_prediction_data(*key_parts)\n",
    "        if pred_data:\n",
    "            y_true = pred_data['y_true'].flatten()\n",
    "            y_pred = pred_data['y_pred'].flatten()\n",
    "\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "\n",
    "            print(f\"\\n📈 Full Training Model Performance:\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  R²:   {r2:.4f}\")\n",
    "            print(f\"  Test samples: {len(y_true)}\")\n",
    "            print(f\"  Model trained on: Combined training data from all folds\")\n",
    "\n",
    "print(f\"\\n🔄 Key Differences from Traditional Approach:\")\n",
    "print(f\"  Traditional: 3 separate models (one per fold)\")\n",
    "print(f\"  Full Train:  1 unified model (trained on all data)\")\n",
    "print(f\"  Benefit:     More training data → Often better performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d843c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering controller: DummyController\n",
      "Registering controller: SklearnModelController\n",
      "Registering controller: TensorFlowModelController\n",
      "Registering controller: PyTorchModelController\n",
      "Registering controller: TransformerMixinController\n",
      "Registering controller: YTransformerMixinController\n",
      "Registering controller: FeatureAugmentationController\n",
      "Registering controller: SampleAugmentationController\n",
      "Registering controller: CrossValidatorController\n",
      "Registering controller: SpectraChartController\n",
      "Registering controller: FoldChartController\n",
      "Registering controller: YChartController\n",
      "Empty predictions: 📈 Predictions: No predictions stored\n",
      "Length: 0\n",
      "After adding: 📈 Predictions: 1 entries\n",
      "   Datasets: ['test']\n",
      "   Pipelines: ['test_pipe']\n",
      "   Models: ['TestModel']\n",
      "✅ Predictions class is working correctly!\n",
      "\n",
      "==================================================\n",
      "Now running the actual pipeline...\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94m🚀 Starting pipeline config_demo_pipeline_00090c on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m🔄 Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92m🔷 Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "💾 Saved 1_0_MinMaxScaler_1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 2: feature_augmentation\u001b[0m\n",
      "🔹 Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1\n",
      "\u001b[96m   ▶ Skipping no-op feature augmentation\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Gaussian\n",
      "💾 Saved 2_1_0_Gaussian_2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94m🔄 Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator StandardScaler\n",
      "💾 Saved 2_3_0_StandardScaler_3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Haar\n",
      "💾 Saved 2_4_0_Haar_4.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "🔹 Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "💾 Saved 3_folds_ShuffleSplit.csv to results\\sample_data\\config_demo_pipeline_00090c\\3_folds_ShuffleSplit.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 4: y_processing\u001b[0m\n",
      "🔹 Executing controller YTransformerMixinController with operator StandardScaler\n",
      "💾 Saved 4_StandardScaler_numeric_StandardScaler5.pkl to results\\sample_data\\config_demo_pipeline_00090c\\4_StandardScaler_numeric_StandardScaler5.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "🔹 Executing controller SklearnModelController with operator PLSRegression\n",
      "🔍 Optimizing 1 parameters with random search (20 trials)...\n",
      "(291, 6453) (291,) (33, 6453) (33, 1)\n",
      "🏆 Training with best parameters: {'n_components': 20}\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "💾 Saved 5_finetuned_PLSRegression_6.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_finetuned_PLSRegression_6.pkl\n",
      "💾 Saved 5_predictions_finetuned_7.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_finetuned_7.csv\n",
      "💾 Saved 5_trained_PLSRegression_8_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_8_simple_cv_fold1.pkl\n",
      "💾 Saved 5_predictions_trained_9_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_9_simple_cv_fold1.csv\n",
      "💾 Saved 5_trained_PLSRegression_10_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_10_simple_cv_fold2.pkl\n",
      "💾 Saved 5_predictions_trained_11_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_11_simple_cv_fold2.csv\n",
      "💾 Saved 5_trained_PLSRegression_12_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_12_simple_cv_fold3.pkl\n",
      "💾 Saved 5_predictions_trained_13_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_13_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 6: (finetune) nicon()\u001b[0m\n",
      "🔹 Executing controller TensorFlowModelController without operator\n",
      "🔍 Optimizing 3 parameters with random search (2 trials)...\n",
      "(291, 2151, 3) (291,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "💾 Saved 6_finetuned_Sequential_14.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_finetuned_Sequential_14.pkl\n",
      "💾 Saved 6_predictions_finetuned_15.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_finetuned_15.csv\n",
      "💾 Saved 6_trained_Sequential_16_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_16_simple_cv_fold1.pkl\n",
      "💾 Saved 6_predictions_trained_17_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_17_simple_cv_fold1.csv\n",
      "💾 Saved 6_trained_Sequential_18_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_18_simple_cv_fold2.pkl\n",
      "💾 Saved 6_predictions_trained_19_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_19_simple_cv_fold2.csv\n",
      "💾 Saved 6_trained_Sequential_20_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_20_simple_cv_fold3.pkl\n",
      "💾 Saved 6_predictions_trained_21_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_21_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m✅ Pipeline config_demo_pipeline_00090c completed successfully on dataset sample_data\u001b[0m\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Final dataset predictions: 📈 Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n"
     ]
    }
   ],
   "source": [
    "# Fresh reload - restart kernel then run this first\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear any cached modules\n",
    "modules_to_clear = [k for k in sys.modules.keys() if k.startswith('nirs4all')]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Now reimport everything\n",
    "from nirs4all.dataset.predictions import Predictions\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "# Test the new predictions class first\n",
    "test_predictions = Predictions()\n",
    "print(f\"Empty predictions: {test_predictions}\")\n",
    "print(f\"Length: {len(test_predictions)}\")\n",
    "\n",
    "# Test adding a prediction\n",
    "import numpy as np\n",
    "test_predictions.add_prediction(\n",
    "    dataset=\"test\",\n",
    "    pipeline=\"test_pipe\",\n",
    "    model=\"TestModel\",\n",
    "    partition=\"test\",\n",
    "    y_true=np.array([1.0, 2.0]),\n",
    "    y_pred=np.array([1.1, 2.1])\n",
    ")\n",
    "\n",
    "print(f\"After adding: {test_predictions}\")\n",
    "print(\"✅ Predictions class is working correctly!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Now running the actual pipeline...\")\n",
    "\n",
    "# Run the pipeline\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "try:\n",
    "    res_dataset, history, pipeline = runner.run(config, data)\n",
    "    print(f\"\\nPipeline completed successfully!\")\n",
    "    print(f\"Final dataset predictions: {res_dataset._predictions}\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d0840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Results Analysis:\n",
      "==================================================\n",
      "Final dataset: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\n",
      "\n",
      "Predictions stored: 📈 Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n",
      "Number of predictions: 8\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2']\n",
      "Datasets: ['sample_data']\n",
      "Pipelines: ['config_demo_pipeline_00090c']\n",
      "Models: ['PLSRegression', 'function', 'Sequential']\n",
      "Partitions: ['test', 'test_fold_0', 'test_fold_1', 'test_fold_2']\n",
      "\n",
      "First prediction details:\n",
      "  Dataset: sample_data\n",
      "  Pipeline: config_demo_pipeline_00090c\n",
      "  Model: PLSRegression\n",
      "  Partition: test\n",
      "  Y_true shape: (33, 1)\n",
      "  Y_pred shape: (33, 1)\n",
      "  Sample indices: 33 samples\n",
      "  Fold index: None\n",
      "  Metadata: {'y_processing': 'numeric_StandardScaler5', 'model_type': 'PLSRegression', 'partition': 'test'}\n"
     ]
    }
   ],
   "source": [
    "# Check the pipeline results and predictions\n",
    "print(\"Pipeline Results Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Final dataset: {res_dataset}\")\n",
    "print(f\"\\nPredictions stored: {res_dataset._predictions}\")\n",
    "print(f\"Number of predictions: {len(res_dataset._predictions)}\")\n",
    "\n",
    "if len(res_dataset._predictions) > 0:\n",
    "    print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "    print(f\"Datasets: {res_dataset._predictions.list_datasets()}\")\n",
    "    print(f\"Pipelines: {res_dataset._predictions.list_pipelines()}\")\n",
    "    print(f\"Models: {res_dataset._predictions.list_models()}\")\n",
    "    print(f\"Partitions: {res_dataset._predictions.list_partitions()}\")\n",
    "\n",
    "    # Get details of first prediction\n",
    "    keys = res_dataset._predictions.list_keys()\n",
    "    if keys:\n",
    "        first_key = keys[0]\n",
    "        first_pred_parts = first_key.split('_', 3)  # Split into 4 parts max\n",
    "        if len(first_pred_parts) >= 4:\n",
    "            dataset_name, pipeline_name, model_name, partition_name = first_pred_parts\n",
    "            first_pred = res_dataset._predictions.get_prediction_data(\n",
    "                dataset_name, pipeline_name, model_name, partition_name\n",
    "            )\n",
    "            if first_pred:\n",
    "                print(f\"\\nFirst prediction details:\")\n",
    "                print(f\"  Dataset: {first_pred['dataset']}\")\n",
    "                print(f\"  Pipeline: {first_pred['pipeline']}\")\n",
    "                print(f\"  Model: {first_pred['model']}\")\n",
    "                print(f\"  Partition: {first_pred['partition']}\")\n",
    "                print(f\"  Y_true shape: {first_pred['y_true'].shape}\")\n",
    "                print(f\"  Y_pred shape: {first_pred['y_pred'].shape}\")\n",
    "                print(f\"  Sample indices: {len(first_pred['sample_indices'])} samples\")\n",
    "                print(f\"  Fold index: {first_pred['fold_idx']}\")\n",
    "                print(f\"  Metadata: {first_pred['metadata']}\")\n",
    "else:\n",
    "    print(\"⚠️ No predictions were stored during pipeline execution\")\n",
    "    print(\"This might be because:\")\n",
    "    print(\"1. The prediction storage integration is not working correctly\")\n",
    "    print(\"2. The dataset parameter is not being passed properly\")\n",
    "    print(\"3. There were errors in the model training that prevented prediction storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade1c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Analysis Example:\n",
      "==================================================\n",
      "PLSRegression predictions: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2']\n",
      "\n",
      "PLS Test Set Performance:\n",
      "  MSE: 0.3200\n",
      "  MAE: 0.1108\n",
      "  R²:  0.9991\n",
      "  Y processing: numeric_StandardScaler5\n",
      "\n",
      "PLS Cross-Validation Combined Performance:\n",
      "  Samples: 99\n",
      "  MSE: 336.5601\n",
      "  MAE: 14.7830\n",
      "  R²:  0.2857\n",
      "  Folds: 3\n",
      "\n",
      "Model Comparison (Test Set):\n",
      "  Sequential: MSE=346.8119, R²=-0.0150\n",
      "  PLSRegression: MSE=0.3200, R²=0.9991\n",
      "\n",
      "✅ Prediction storage and analysis system is fully functional!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate prediction analysis capabilities\n",
    "print(\"Prediction Analysis Example:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get all predictions from PLSRegression model\n",
    "pls_predictions = res_dataset._predictions.get_predictions(model=\"PLSRegression\")\n",
    "print(f\"PLSRegression predictions: {list(pls_predictions.keys())}\")\n",
    "\n",
    "# Get specific test set prediction for PLS\n",
    "pls_test = res_dataset._predictions.get_prediction_data(\n",
    "    \"sample_data\", \"config_demo_pipeline_00090c\", \"PLSRegression\", \"test\"\n",
    ")\n",
    "\n",
    "if pls_test:\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "    y_true = pls_test['y_true'].flatten()\n",
    "    y_pred = pls_test['y_pred'].flatten()\n",
    "\n",
    "    print(f\"\\nPLS Test Set Performance:\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"  R²:  {r2_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Y processing: {pls_test['metadata']['y_processing']}\")\n",
    "\n",
    "# Combine cross-validation folds for PLS\n",
    "pls_cv_combined = res_dataset._predictions.combine_folds(\n",
    "    \"sample_data\", \"config_demo_pipeline_00090c\", \"PLSRegression\", \"test_fold\"\n",
    ")\n",
    "\n",
    "if pls_cv_combined:\n",
    "    y_true_cv = pls_cv_combined['y_true'].flatten()\n",
    "    y_pred_cv = pls_cv_combined['y_pred'].flatten()\n",
    "\n",
    "    print(f\"\\nPLS Cross-Validation Combined Performance:\")\n",
    "    print(f\"  Samples: {len(y_true_cv)}\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  R²:  {r2_score(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  Folds: {pls_cv_combined['metadata']['num_folds']}\")\n",
    "\n",
    "# Compare all models on test set\n",
    "print(f\"\\nModel Comparison (Test Set):\")\n",
    "models = res_dataset._predictions.list_models()\n",
    "for model in models:\n",
    "    test_pred = res_dataset._predictions.get_predictions(\n",
    "        model=model, partition=\"test\"\n",
    "    )\n",
    "    if test_pred:\n",
    "        first_key = list(test_pred.keys())[0]\n",
    "        pred_data = test_pred[first_key]\n",
    "        y_true = pred_data['y_true'].flatten()\n",
    "        y_pred = pred_data['y_pred'].flatten()\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\"  {model}: MSE={mse:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Prediction storage and analysis system is fully functional!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94m🚀 Starting pipeline config_demo_pipeline_00090c on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m🔄 Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92m🔷 Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "💾 Saved 1_0_MinMaxScaler_1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 2: feature_augmentation\u001b[0m\n",
      "🔹 Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1\n",
      "\u001b[96m   ▶ Skipping no-op feature augmentation\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Gaussian\n",
      "💾 Saved 2_1_0_Gaussian_2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94m🔄 Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator StandardScaler\n",
      "💾 Saved 2_3_0_StandardScaler_3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Haar\n",
      "💾 Saved 2_4_0_Haar_4.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "🔹 Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "💾 Saved 3_folds_ShuffleSplit.csv to results\\sample_data\\config_demo_pipeline_00090c\\3_folds_ShuffleSplit.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 4: y_processing\u001b[0m\n",
      "🔹 Executing controller YTransformerMixinController with operator StandardScaler\n",
      "💾 Saved 4_StandardScaler_numeric_StandardScaler5.pkl to results\\sample_data\\config_demo_pipeline_00090c\\4_StandardScaler_numeric_StandardScaler5.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "🔹 Executing controller SklearnModelController with operator PLSRegression\n",
      "🔍 Optimizing 1 parameters with random search (20 trials)...\n",
      "(291, 6453) (291,) (33, 6453) (33, 1)\n",
      "🏆 Training with best parameters: {'n_components': 27}\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "💾 Saved 5_finetuned_PLSRegression_6.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_finetuned_PLSRegression_6.pkl\n",
      "💾 Saved 5_predictions_finetuned_7.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_finetuned_7.csv\n",
      "💾 Saved 5_trained_PLSRegression_8_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_8_simple_cv_fold1.pkl\n",
      "💾 Saved 5_predictions_trained_9_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_9_simple_cv_fold1.csv\n",
      "💾 Saved 5_trained_PLSRegression_10_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_10_simple_cv_fold2.pkl\n",
      "💾 Saved 5_predictions_trained_11_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_11_simple_cv_fold2.csv\n",
      "💾 Saved 5_trained_PLSRegression_12_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_12_simple_cv_fold3.pkl\n",
      "💾 Saved 5_predictions_trained_13_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_13_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 6: (finetune) nicon()\u001b[0m\n",
      "🔹 Executing controller TensorFlowModelController without operator\n",
      "🔍 Optimizing 3 parameters with random search (2 trials)...\n",
      "(291, 2151, 3) (291,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "💾 Saved 6_finetuned_Sequential_14.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_finetuned_Sequential_14.pkl\n",
      "💾 Saved 6_predictions_finetuned_15.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_finetuned_15.csv\n",
      "💾 Saved 6_trained_Sequential_16_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_16_simple_cv_fold1.pkl\n",
      "💾 Saved 6_predictions_trained_17_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_17_simple_cv_fold1.csv\n",
      "💾 Saved 6_trained_Sequential_18_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_18_simple_cv_fold2.pkl\n",
      "💾 Saved 6_predictions_trained_19_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_19_simple_cv_fold2.csv\n",
      "💾 Saved 6_trained_Sequential_20_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_20_simple_cv_fold3.pkl\n",
      "💾 Saved 6_predictions_trained_21_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_21_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m✅ Pipeline config_demo_pipeline_00090c completed successfully on dataset sample_data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "\n",
    "# json_config = PipelineConfig(\"sample.json\")\n",
    "# yaml_config = PipelineConfig(\"sample.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Predictions functionality...\n",
      "\n",
      "Dataset before running:\n",
      "Predictions: 📈 Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n",
      "\n",
      "Number of predictions stored: 8\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2']\n",
      "\n",
      "After adding test prediction:\n",
      "Number of predictions: 9\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2', 'test_dataset_test_pipeline_TestModel_test']\n",
      "\n",
      "Test prediction data: {'dataset': 'test_dataset', 'pipeline': 'test_pipeline', 'model': 'TestModel', 'partition': 'test', 'y_true': array([1., 2., 3.]), 'y_pred': array([1.1, 1.9, 3.2]), 'sample_indices': [0, 1, 2], 'fold_idx': None, 'metadata': {'test': True}}\n"
     ]
    }
   ],
   "source": [
    "# Test the new Predictions functionality\n",
    "print(\"Testing Predictions functionality...\")\n",
    "print(\"\\nDataset before running:\")\n",
    "print(f\"Predictions: {res_dataset._predictions}\")\n",
    "\n",
    "# Let's check if any predictions were stored\n",
    "print(f\"\\nNumber of predictions stored: {len(res_dataset._predictions)}\")\n",
    "print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "\n",
    "# Try to manually add a test prediction\n",
    "import numpy as np\n",
    "\n",
    "res_dataset._predictions.add_prediction(\n",
    "    dataset=\"test_dataset\",\n",
    "    pipeline=\"test_pipeline\",\n",
    "    model=\"TestModel\",\n",
    "    partition=\"test\",\n",
    "    y_true=np.array([1.0, 2.0, 3.0]),\n",
    "    y_pred=np.array([1.1, 1.9, 3.2]),\n",
    "    metadata={\"test\": True}\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter adding test prediction:\")\n",
    "print(f\"Number of predictions: {len(res_dataset._predictions)}\")\n",
    "print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "\n",
    "# Get the test prediction\n",
    "test_pred = res_dataset._predictions.get_prediction_data(\n",
    "    \"test_dataset\", \"test_pipeline\", \"TestModel\", \"test\"\n",
    ")\n",
    "print(f\"\\nTest prediction data: {test_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
