{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d843c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering controller: DummyController\n",
      "Registering controller: SklearnModelController\n",
      "Registering controller: TensorFlowModelController\n",
      "Registering controller: PyTorchModelController\n",
      "Registering controller: TransformerMixinController\n",
      "Registering controller: YTransformerMixinController\n",
      "Registering controller: FeatureAugmentationController\n",
      "Registering controller: SampleAugmentationController\n",
      "Registering controller: CrossValidatorController\n",
      "Registering controller: SpectraChartController\n",
      "Registering controller: FoldChartController\n",
      "Registering controller: YChartController\n",
      "Empty predictions: 📈 Predictions: No predictions stored\n",
      "Length: 0\n",
      "After adding: 📈 Predictions: 1 entries\n",
      "   Datasets: ['test']\n",
      "   Pipelines: ['test_pipe']\n",
      "   Models: ['TestModel']\n",
      "✅ Predictions class is working correctly!\n",
      "\n",
      "==================================================\n",
      "Now running the actual pipeline...\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94m🚀 Starting pipeline config_demo_pipeline_00090c on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m🔄 Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92m🔷 Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "💾 Saved 1_0_MinMaxScaler_1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 2: feature_augmentation\u001b[0m\n",
      "🔹 Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1\n",
      "\u001b[96m   ▶ Skipping no-op feature augmentation\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Gaussian\n",
      "💾 Saved 2_1_0_Gaussian_2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94m🔄 Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator StandardScaler\n",
      "💾 Saved 2_3_0_StandardScaler_3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Haar\n",
      "💾 Saved 2_4_0_Haar_4.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "🔹 Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "💾 Saved 3_folds_ShuffleSplit.csv to results\\sample_data\\config_demo_pipeline_00090c\\3_folds_ShuffleSplit.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 4: y_processing\u001b[0m\n",
      "🔹 Executing controller YTransformerMixinController with operator StandardScaler\n",
      "💾 Saved 4_StandardScaler_numeric_StandardScaler5.pkl to results\\sample_data\\config_demo_pipeline_00090c\\4_StandardScaler_numeric_StandardScaler5.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "🔹 Executing controller SklearnModelController with operator PLSRegression\n",
      "🔍 Optimizing 1 parameters with random search (20 trials)...\n",
      "(291, 6453) (291,) (33, 6453) (33, 1)\n",
      "🏆 Training with best parameters: {'n_components': 20}\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "💾 Saved 5_finetuned_PLSRegression_6.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_finetuned_PLSRegression_6.pkl\n",
      "💾 Saved 5_predictions_finetuned_7.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_finetuned_7.csv\n",
      "💾 Saved 5_trained_PLSRegression_8_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_8_simple_cv_fold1.pkl\n",
      "💾 Saved 5_predictions_trained_9_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_9_simple_cv_fold1.csv\n",
      "💾 Saved 5_trained_PLSRegression_10_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_10_simple_cv_fold2.pkl\n",
      "💾 Saved 5_predictions_trained_11_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_11_simple_cv_fold2.csv\n",
      "💾 Saved 5_trained_PLSRegression_12_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_12_simple_cv_fold3.pkl\n",
      "💾 Saved 5_predictions_trained_13_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_13_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 6: (finetune) nicon()\u001b[0m\n",
      "🔹 Executing controller TensorFlowModelController without operator\n",
      "🔍 Optimizing 3 parameters with random search (2 trials)...\n",
      "(291, 2151, 3) (291,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "💾 Saved 6_finetuned_Sequential_14.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_finetuned_Sequential_14.pkl\n",
      "💾 Saved 6_predictions_finetuned_15.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_finetuned_15.csv\n",
      "💾 Saved 6_trained_Sequential_16_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_16_simple_cv_fold1.pkl\n",
      "💾 Saved 6_predictions_trained_17_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_17_simple_cv_fold1.csv\n",
      "💾 Saved 6_trained_Sequential_18_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_18_simple_cv_fold2.pkl\n",
      "💾 Saved 6_predictions_trained_19_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_19_simple_cv_fold2.csv\n",
      "💾 Saved 6_trained_Sequential_20_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_20_simple_cv_fold3.pkl\n",
      "💾 Saved 6_predictions_trained_21_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_21_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m✅ Pipeline config_demo_pipeline_00090c completed successfully on dataset sample_data\u001b[0m\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Final dataset predictions: 📈 Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n"
     ]
    }
   ],
   "source": [
    "# Fresh reload - restart kernel then run this first\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear any cached modules\n",
    "modules_to_clear = [k for k in sys.modules.keys() if k.startswith('nirs4all')]\n",
    "for module in modules_to_clear:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Now reimport everything\n",
    "from nirs4all.dataset.predictions import Predictions\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "# Test the new predictions class first\n",
    "test_predictions = Predictions()\n",
    "print(f\"Empty predictions: {test_predictions}\")\n",
    "print(f\"Length: {len(test_predictions)}\")\n",
    "\n",
    "# Test adding a prediction\n",
    "import numpy as np\n",
    "test_predictions.add_prediction(\n",
    "    dataset=\"test\",\n",
    "    pipeline=\"test_pipe\",\n",
    "    model=\"TestModel\",\n",
    "    partition=\"test\",\n",
    "    y_true=np.array([1.0, 2.0]),\n",
    "    y_pred=np.array([1.1, 2.1])\n",
    ")\n",
    "\n",
    "print(f\"After adding: {test_predictions}\")\n",
    "print(\"✅ Predictions class is working correctly!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Now running the actual pipeline...\")\n",
    "\n",
    "# Run the pipeline\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "try:\n",
    "    res_dataset, history, pipeline = runner.run(config, data)\n",
    "    print(f\"\\nPipeline completed successfully!\")\n",
    "    print(f\"Final dataset predictions: {res_dataset._predictions}\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d0840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Results Analysis:\n",
      "==================================================\n",
      "Final dataset: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\n",
      "\n",
      "Predictions stored: 📈 Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n",
      "Number of predictions: 8\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2']\n",
      "Datasets: ['sample_data']\n",
      "Pipelines: ['config_demo_pipeline_00090c']\n",
      "Models: ['PLSRegression', 'function', 'Sequential']\n",
      "Partitions: ['test', 'test_fold_0', 'test_fold_1', 'test_fold_2']\n",
      "\n",
      "First prediction details:\n",
      "  Dataset: sample_data\n",
      "  Pipeline: config_demo_pipeline_00090c\n",
      "  Model: PLSRegression\n",
      "  Partition: test\n",
      "  Y_true shape: (33, 1)\n",
      "  Y_pred shape: (33, 1)\n",
      "  Sample indices: 33 samples\n",
      "  Fold index: None\n",
      "  Metadata: {'y_processing': 'numeric_StandardScaler5', 'model_type': 'PLSRegression', 'partition': 'test'}\n"
     ]
    }
   ],
   "source": [
    "# Check the pipeline results and predictions\n",
    "print(\"Pipeline Results Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Final dataset: {res_dataset}\")\n",
    "print(f\"\\nPredictions stored: {res_dataset._predictions}\")\n",
    "print(f\"Number of predictions: {len(res_dataset._predictions)}\")\n",
    "\n",
    "if len(res_dataset._predictions) > 0:\n",
    "    print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "    print(f\"Datasets: {res_dataset._predictions.list_datasets()}\")\n",
    "    print(f\"Pipelines: {res_dataset._predictions.list_pipelines()}\")\n",
    "    print(f\"Models: {res_dataset._predictions.list_models()}\")\n",
    "    print(f\"Partitions: {res_dataset._predictions.list_partitions()}\")\n",
    "\n",
    "    # Get details of first prediction\n",
    "    keys = res_dataset._predictions.list_keys()\n",
    "    if keys:\n",
    "        first_key = keys[0]\n",
    "        first_pred_parts = first_key.split('_', 3)  # Split into 4 parts max\n",
    "        if len(first_pred_parts) >= 4:\n",
    "            dataset_name, pipeline_name, model_name, partition_name = first_pred_parts\n",
    "            first_pred = res_dataset._predictions.get_prediction_data(\n",
    "                dataset_name, pipeline_name, model_name, partition_name\n",
    "            )\n",
    "            if first_pred:\n",
    "                print(f\"\\nFirst prediction details:\")\n",
    "                print(f\"  Dataset: {first_pred['dataset']}\")\n",
    "                print(f\"  Pipeline: {first_pred['pipeline']}\")\n",
    "                print(f\"  Model: {first_pred['model']}\")\n",
    "                print(f\"  Partition: {first_pred['partition']}\")\n",
    "                print(f\"  Y_true shape: {first_pred['y_true'].shape}\")\n",
    "                print(f\"  Y_pred shape: {first_pred['y_pred'].shape}\")\n",
    "                print(f\"  Sample indices: {len(first_pred['sample_indices'])} samples\")\n",
    "                print(f\"  Fold index: {first_pred['fold_idx']}\")\n",
    "                print(f\"  Metadata: {first_pred['metadata']}\")\n",
    "else:\n",
    "    print(\"⚠️ No predictions were stored during pipeline execution\")\n",
    "    print(\"This might be because:\")\n",
    "    print(\"1. The prediction storage integration is not working correctly\")\n",
    "    print(\"2. The dataset parameter is not being passed properly\")\n",
    "    print(\"3. There were errors in the model training that prevented prediction storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Analysis Example:\n",
      "==================================================\n",
      "PLSRegression predictions: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2']\n",
      "\n",
      "PLS Test Set Performance:\n",
      "  MSE: 15.1360\n",
      "  MAE: 1.1331\n",
      "  R²:  0.9624\n",
      "  Y processing: numeric_StandardScaler5\n",
      "\n",
      "PLS Cross-Validation Combined Performance:\n",
      "  Samples: 99\n",
      "  MSE: 415.0967\n",
      "  MAE: 14.9935\n",
      "  R²:  0.2374\n",
      "  Folds: 3\n",
      "\n",
      "Model Comparison (Test Set):\n",
      "  PLSRegression: MSE=15.1360, R²=0.9624\n",
      "  TestModel: MSE=0.0200, R²=0.9700\n",
      "  Sequential: MSE=446.0420, R²=-0.1083\n",
      "\n",
      "✅ Prediction storage and analysis system is fully functional!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate prediction analysis capabilities\n",
    "print(\"Prediction Analysis Example:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get all predictions from PLSRegression model\n",
    "pls_predictions = res_dataset._predictions.get_predictions(model=\"PLSRegression\")\n",
    "print(f\"PLSRegression predictions: {list(pls_predictions.keys())}\")\n",
    "\n",
    "# Get specific test set prediction for PLS\n",
    "pls_test = res_dataset._predictions.get_prediction_data(\n",
    "    \"sample_data\", \"config_demo_pipeline_00090c\", \"PLSRegression\", \"test\"\n",
    ")\n",
    "\n",
    "if pls_test:\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "    y_true = pls_test['y_true'].flatten()\n",
    "    y_pred = pls_test['y_pred'].flatten()\n",
    "\n",
    "    print(f\"\\nPLS Test Set Performance:\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"  R²:  {r2_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Y processing: {pls_test['metadata']['y_processing']}\")\n",
    "\n",
    "# Combine cross-validation folds for PLS\n",
    "pls_cv_combined = res_dataset._predictions.combine_folds(\n",
    "    \"sample_data\", \"config_demo_pipeline_00090c\", \"PLSRegression\", \"test_fold\"\n",
    ")\n",
    "\n",
    "if pls_cv_combined:\n",
    "    y_true_cv = pls_cv_combined['y_true'].flatten()\n",
    "    y_pred_cv = pls_cv_combined['y_pred'].flatten()\n",
    "\n",
    "    print(f\"\\nPLS Cross-Validation Combined Performance:\")\n",
    "    print(f\"  Samples: {len(y_true_cv)}\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  R²:  {r2_score(y_true_cv, y_pred_cv):.4f}\")\n",
    "    print(f\"  Folds: {pls_cv_combined['metadata']['num_folds']}\")\n",
    "\n",
    "# Compare all models on test set\n",
    "print(f\"\\nModel Comparison (Test Set):\")\n",
    "models = res_dataset._predictions.list_models()\n",
    "for model in models:\n",
    "    test_pred = res_dataset._predictions.get_predictions(\n",
    "        model=model, partition=\"test\"\n",
    "    )\n",
    "    if test_pred:\n",
    "        first_key = list(test_pred.keys())[0]\n",
    "        pred_data = test_pred[first_key]\n",
    "        y_true = pred_data['y_true'].flatten()\n",
    "        y_pred = pred_data['y_pred'].flatten()\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\"  {model}: MSE={mse:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Prediction storage and analysis system is fully functional!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering controller: DummyController\n",
      "Registering controller: SklearnModelController\n",
      "Registering controller: TensorFlowModelController\n",
      "Registering controller: PyTorchModelController\n",
      "Registering controller: TransformerMixinController\n",
      "Registering controller: YTransformerMixinController\n",
      "Registering controller: FeatureAugmentationController\n",
      "Registering controller: SampleAugmentationController\n",
      "Registering controller: CrossValidatorController\n",
      "Registering controller: SpectraChartController\n",
      "Registering controller: FoldChartController\n",
      "Registering controller: YChartController\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94m🚀 Starting pipeline config_demo_pipeline_00090c on dataset sample_data\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m🔄 Running 6 steps in sequential mode\u001b[0m\n",
      "\u001b[92m🔷 Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "💾 Saved 1_0_MinMaxScaler_1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.1, max=0.89, mean=0.661, var=0.016)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 2: feature_augmentation\u001b[0m\n",
      "🔹 Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1\n",
      "\u001b[96m   ▶ Skipping no-op feature augmentation\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Gaussian\n",
      "💾 Saved 2_1_0_Gaussian_2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94m🔄 Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator StandardScaler\n",
      "💾 Saved 2_3_0_StandardScaler_3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Haar\n",
      "💾 Saved 2_4_0_Haar_4.pkl to results\\sample_data\\config_demo_pipeline_00090c\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 3: ShuffleSplit(n_splits=3, test_size=0.25)\u001b[0m\n",
      "🔹 Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "💾 Saved 3_folds_ShuffleSplit.csv to results\\sample_data\\config_demo_pipeline_00090c\\3_folds_ShuffleSplit.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 4: y_processing\u001b[0m\n",
      "🔹 Executing controller YTransformerMixinController with operator StandardScaler\n",
      "💾 Saved 4_StandardScaler_numeric_StandardScaler5.pkl to results\\sample_data\\config_demo_pipeline_00090c\\4_StandardScaler_numeric_StandardScaler5.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: sample_data\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.215, max=0.89, mean=0.22, var=0.102)\n",
      "Targets: (samples=189, targets=1, processings=['numeric', 'numeric_StandardScaler5'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "- numeric_StandardScaler5: min=-1.23, max=4.156, mean=0.019\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 130 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 59 samples\n",
      "Folds: [(97, 33), (97, 33), (97, 33)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 5: (finetune) PLSRegression()\u001b[0m\n",
      "🔹 Executing controller SklearnModelController with operator PLSRegression\n",
      "🔍 Optimizing 1 parameters with random search (20 trials)...\n",
      "(291, 6453) (291,) (33, 6453) (33, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Training with best parameters: {'n_components': 18}\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "(97, 6453) (97,) (33, 6453) (33, 1)\n",
      "💾 Saved 5_finetuned_PLSRegression_6.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_finetuned_PLSRegression_6.pkl\n",
      "💾 Saved 5_predictions_finetuned_7.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_finetuned_7.csv\n",
      "💾 Saved 5_trained_PLSRegression_8_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_8_simple_cv_fold1.pkl\n",
      "💾 Saved 5_predictions_trained_9_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_9_simple_cv_fold1.csv\n",
      "💾 Saved 5_trained_PLSRegression_10_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_10_simple_cv_fold2.pkl\n",
      "💾 Saved 5_predictions_trained_11_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_11_simple_cv_fold2.csv\n",
      "💾 Saved 5_trained_PLSRegression_12_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\5_trained_PLSRegression_12_simple_cv_fold3.pkl\n",
      "💾 Saved 5_predictions_trained_13_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\5_predictions_trained_13_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 6: (finetune) nicon()\u001b[0m\n",
      "🔹 Executing controller TensorFlowModelController without operator\n",
      "🔍 Optimizing 3 parameters with random search (2 trials)...\n",
      "(291, 2151, 3) (291,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF64D24D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002AF64D24D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 2151, 3) (97,) (33, 2151, 3) (33, 1)\n",
      "💾 Saved 6_finetuned_Sequential_14.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_finetuned_Sequential_14.pkl\n",
      "💾 Saved 6_predictions_finetuned_15.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_finetuned_15.csv\n",
      "💾 Saved 6_trained_Sequential_16_simple_cv_fold1.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_16_simple_cv_fold1.pkl\n",
      "💾 Saved 6_predictions_trained_17_simple_cv_fold1.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_17_simple_cv_fold1.csv\n",
      "💾 Saved 6_trained_Sequential_18_simple_cv_fold2.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_18_simple_cv_fold2.pkl\n",
      "💾 Saved 6_predictions_trained_19_simple_cv_fold2.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_19_simple_cv_fold2.csv\n",
      "💾 Saved 6_trained_Sequential_20_simple_cv_fold3.pkl to results\\sample_data\\config_demo_pipeline_00090c\\6_trained_Sequential_20_simple_cv_fold3.pkl\n",
      "💾 Saved 6_predictions_trained_21_simple_cv_fold3.csv to results\\sample_data\\config_demo_pipeline_00090c\\6_predictions_trained_21_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m✅ Pipeline config_demo_pipeline_00090c completed successfully on dataset sample_data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config, \"demo_pipeline\")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "\n",
    "# json_config = PipelineConfig(\"sample.json\")\n",
    "# yaml_config = PipelineConfig(\"sample.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Predictions functionality...\n",
      "\n",
      "Dataset before running:\n",
      "Predictions: 📈 Predictions: 8 entries\n",
      "   Datasets: ['sample_data']\n",
      "   Pipelines: ['config_demo_pipeline_00090c']\n",
      "   Models: ['PLSRegression', 'function', 'Sequential']\n",
      "\n",
      "Number of predictions stored: 8\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2']\n",
      "\n",
      "After adding test prediction:\n",
      "Number of predictions: 9\n",
      "Prediction keys: ['sample_data_config_demo_pipeline_00090c_PLSRegression_test', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_0', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_1', 'sample_data_config_demo_pipeline_00090c_PLSRegression_test_fold_2', 'sample_data_config_demo_pipeline_00090c_Sequential_test', 'sample_data_config_demo_pipeline_00090c_function_test_fold_0', 'sample_data_config_demo_pipeline_00090c_function_test_fold_1', 'sample_data_config_demo_pipeline_00090c_function_test_fold_2', 'test_dataset_test_pipeline_TestModel_test']\n",
      "\n",
      "Test prediction data: {'dataset': 'test_dataset', 'pipeline': 'test_pipeline', 'model': 'TestModel', 'partition': 'test', 'y_true': array([1., 2., 3.]), 'y_pred': array([1.1, 1.9, 3.2]), 'sample_indices': [0, 1, 2], 'fold_idx': None, 'metadata': {'test': True}}\n"
     ]
    }
   ],
   "source": [
    "# Test the new Predictions functionality\n",
    "print(\"Testing Predictions functionality...\")\n",
    "print(\"\\nDataset before running:\")\n",
    "print(f\"Predictions: {res_dataset._predictions}\")\n",
    "\n",
    "# Let's check if any predictions were stored\n",
    "print(f\"\\nNumber of predictions stored: {len(res_dataset._predictions)}\")\n",
    "print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "\n",
    "# Try to manually add a test prediction\n",
    "import numpy as np\n",
    "\n",
    "res_dataset._predictions.add_prediction(\n",
    "    dataset=\"test_dataset\",\n",
    "    pipeline=\"test_pipeline\",\n",
    "    model=\"TestModel\",\n",
    "    partition=\"test\",\n",
    "    y_true=np.array([1.0, 2.0, 3.0]),\n",
    "    y_pred=np.array([1.1, 1.9, 3.2]),\n",
    "    metadata={\"test\": True}\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter adding test prediction:\")\n",
    "print(f\"Number of predictions: {len(res_dataset._predictions)}\")\n",
    "print(f\"Prediction keys: {res_dataset._predictions.list_keys()}\")\n",
    "\n",
    "# Get the test prediction\n",
    "test_pred = res_dataset._predictions.get_prediction_data(\n",
    "    \"test_dataset\", \"test_pipeline\", \"TestModel\", \"test\"\n",
    ")\n",
    "print(f\"\\nTest prediction data: {test_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
