{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d701b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: classification\n",
      "Features (samples=66, sources=1):\n",
      "- Source 0: (66, 1, 2152), processings=['raw'], min=-0.032, max=0.948, mean=0.34, var=0.106)\n",
      "Targets: (samples=66, targets=1, processings=['numeric'])\n",
      "- numeric: min=0.0, max=9.0, mean=2.894\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 48 samples\n",
      "- \"test\", ['raw']: 18 samples\u001b[0m\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94m🚀 Starting pipeline config_d4e86654 on dataset classification\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94m🔄 Running 11 steps in sequential mode\u001b[0m\n",
      "\u001b[92m🔷 Step 1: MinMaxScaler(feature_range=[0.1, 0.8])\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator MinMaxScaler\n",
      "💾 Saved 1_0_MinMaxScaler_1.pkl to results\\classification\\config_d4e86654\\1_0_MinMaxScaler_1.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: classification\n",
      "Features (samples=66, sources=1):\n",
      "- Source 0: (66, 1, 2152), processings=['raw_MinMaxScaler_1'], min=0.093, max=0.8, mean=0.445, var=0.029)\n",
      "Targets: (samples=66, targets=1, processings=['numeric'])\n",
      "- numeric: min=0.0, max=9.0, mean=2.894\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1']: 48 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1']: 18 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 2: feature_augmentation\u001b[0m\n",
      "🔹 Executing controller FeatureAugmentationController without operator\n",
      "Executing feature augmentation for step: {'feature_augmentation': [None, 'nirs4all.operators.transformations.signal.Gaussian', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.operators.transformations.nirs.Haar']]}, keyword: , source: -1, mode: train\n",
      "Skipping no-op feature augmentation\n",
      "\u001b[96m   ▶ Sub-step 2.1: nirs4all.operators.transformations.signal.Gaussian\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Gaussian\n",
      "💾 Saved 2_1_0_Gaussian_2.pkl to results\\classification\\config_d4e86654\\2_1_0_Gaussian_2.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.2: Sub-pipeline (2 steps)\u001b[0m\n",
      "\u001b[94m🔄 Running 2 steps in sequential mode\u001b[0m\n",
      "\u001b[96m   ▶ Sub-step 2.3: sklearn.preprocessing._data.StandardScaler\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator StandardScaler\n",
      "💾 Saved 2_3_0_StandardScaler_3.pkl to results\\classification\\config_d4e86654\\2_3_0_StandardScaler_3.pkl\n",
      "\u001b[96m   ▶ Sub-step 2.4: nirs4all.operators.transformations.nirs.Haar\u001b[0m\n",
      "🔹 Executing controller TransformerMixinController with operator Haar\n",
      "💾 Saved 2_4_0_Haar_4.pkl to results\\classification\\config_d4e86654\\2_4_0_Haar_4.pkl\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: classification\n",
      "Features (samples=66, sources=1):\n",
      "- Source 0: (66, 3, 2152), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.254, max=0.8, mean=0.148, var=0.054)\n",
      "Targets: (samples=66, targets=1, processings=['numeric'])\n",
      "- numeric: min=0.0, max=9.0, mean=2.894\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 48 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 18 samples\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 3: ShuffleSplit(n_splits=3, test_size=0.25, random_state=42)\u001b[0m\n",
      "🔹 Executing controller CrossValidatorController with operator ShuffleSplit\n",
      "Generated 3 folds.\n",
      "💾 Saved 3_folds_ShuffleSplit_seed42.csv to results\\classification\\config_d4e86654\\3_folds_ShuffleSplit_seed42.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[97mUpdate: 📊 Dataset: classification\n",
      "Features (samples=66, sources=1):\n",
      "- Source 0: (66, 3, 2152), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4'], min=-0.254, max=0.8, mean=0.148, var=0.054)\n",
      "Targets: (samples=66, targets=1, processings=['numeric'])\n",
      "- numeric: min=0.0, max=9.0, mean=2.894\n",
      "Indexes:\n",
      "- \"train\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 48 samples\n",
      "- \"test\", ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_Gaussian_2', 'raw_MinMaxScaler_1_StandardScaler_3_Haar_4']: 18 samples\n",
      "Folds: [(36, 12), (36, 12), (36, 12)]\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 4: y_chart\u001b[0m\n",
      "🔹 Executing controller YChartController without operator\n",
      "66 48 18\n",
      "💾 Saved 4_Y_distribution_train_test.png to results\\classification\\config_d4e86654\\4_Y_distribution_train_test.png\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 5: (finetune) RandomForestClassifier(max_depth=10, random_state=42)\u001b[0m\n",
      "🔹 Executing controller SklearnModelController with operator RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Optimizing 3 parameters with grid search (4 trials)...\n",
      "(108, 6456) (108,) (12, 6456) (12, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 6456) (36,) (12, 6456) (12, 1)\n",
      "(36, 6456) (36,) (12, 6456) (12, 1)\n",
      "(36, 6456) (36,) (12, 6456) (12, 1)\n",
      "💾 Saved 5_finetuned_RandomForestClassifier_6.pkl to results\\classification\\config_d4e86654\\5_finetuned_RandomForestClassifier_6.pkl\n",
      "💾 Saved 5_predictions_finetuned_7.csv to results\\classification\\config_d4e86654\\5_predictions_finetuned_7.csv\n",
      "💾 Saved 5_trained_RandomForestClassifier_9_simple_cv_fold1.pkl to results\\classification\\config_d4e86654\\5_trained_RandomForestClassifier_9_simple_cv_fold1.pkl\n",
      "💾 Saved 5_predictions_trained_10_simple_cv_fold1.csv to results\\classification\\config_d4e86654\\5_predictions_trained_10_simple_cv_fold1.csv\n",
      "💾 Saved 5_trained_RandomForestClassifier_12_simple_cv_fold2.pkl to results\\classification\\config_d4e86654\\5_trained_RandomForestClassifier_12_simple_cv_fold2.pkl\n",
      "💾 Saved 5_predictions_trained_13_simple_cv_fold2.csv to results\\classification\\config_d4e86654\\5_predictions_trained_13_simple_cv_fold2.csv\n",
      "💾 Saved 5_trained_RandomForestClassifier_15_simple_cv_fold3.pkl to results\\classification\\config_d4e86654\\5_trained_RandomForestClassifier_15_simple_cv_fold3.pkl\n",
      "💾 Saved 5_predictions_trained_16_simple_cv_fold3.csv to results\\classification\\config_d4e86654\\5_predictions_trained_16_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 6: (finetune) LinearDiscriminantAnalysis()\u001b[0m\n",
      "🔹 Executing controller SklearnModelController with operator LinearDiscriminantAnalysis\n",
      "🔍 Optimizing 2 parameters with grid search (4 trials)...\n",
      "(108, 6456) (108,) (12, 6456) (12, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_shrunk_covariance.py:349: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_empirical_covariance.py:102: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_shrunk_covariance.py:349: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_empirical_covariance.py:102: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_shrunk_covariance.py:349: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_empirical_covariance.py:102: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_shrunk_covariance.py:349: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_empirical_covariance.py:102: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_shrunk_covariance.py:349: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_empirical_covariance.py:102: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_shrunk_covariance.py:349: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\covariance\\_empirical_covariance.py:102: UserWarning: Only one sample available. You may want to reshape your data array\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Training with best parameters: {'solver': 'eigen', 'shrinkage': 'auto'}\n",
      "(36, 6456) (36,) (12, 6456) (12, 1)\n",
      "(36, 6456) (36,) (12, 6456) (12, 1)\n",
      "(36, 6456) (36,) (12, 6456) (12, 1)\n",
      "💾 Saved 6_finetuned_LinearDiscriminantAnalysis_18.pkl to results\\classification\\config_d4e86654\\6_finetuned_LinearDiscriminantAnalysis_18.pkl\n",
      "💾 Saved 6_predictions_finetuned_19.csv to results\\classification\\config_d4e86654\\6_predictions_finetuned_19.csv\n",
      "💾 Saved 6_trained_LinearDiscriminantAnalysis_21_simple_cv_fold1.pkl to results\\classification\\config_d4e86654\\6_trained_LinearDiscriminantAnalysis_21_simple_cv_fold1.pkl\n",
      "💾 Saved 6_predictions_trained_22_simple_cv_fold1.csv to results\\classification\\config_d4e86654\\6_predictions_trained_22_simple_cv_fold1.csv\n",
      "💾 Saved 6_trained_LinearDiscriminantAnalysis_24_simple_cv_fold2.pkl to results\\classification\\config_d4e86654\\6_trained_LinearDiscriminantAnalysis_24_simple_cv_fold2.pkl\n",
      "💾 Saved 6_predictions_trained_25_simple_cv_fold2.csv to results\\classification\\config_d4e86654\\6_predictions_trained_25_simple_cv_fold2.csv\n",
      "💾 Saved 6_trained_LinearDiscriminantAnalysis_27_simple_cv_fold3.pkl to results\\classification\\config_d4e86654\\6_trained_LinearDiscriminantAnalysis_27_simple_cv_fold3.pkl\n",
      "💾 Saved 6_predictions_trained_28_simple_cv_fold3.csv to results\\classification\\config_d4e86654\\6_predictions_trained_28_simple_cv_fold3.csv\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[92m🔷 Step 7: train nicon_classification()\u001b[0m\n",
      "🔹 Executing controller TensorFlowModelController without operator\n",
      "(36, 2152, 3) (36,) (12, 2152, 3) (12, 1)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 135, in _create_model_from_function\n",
      "    model = model_function(input_shape, params)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\operators\\models\\cirad_tf.py\", line 681, in nicon_classification\n",
      "    model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 122, in add\n",
      "    self._maybe_rebuild()\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 149, in _maybe_rebuild\n",
      "    self.build(input_shape)\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 230, in build_wrapper\n",
      "    original_build_method(*args, **kwargs)\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 195, in build\n",
      "    x = layer(x)\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\backend\\common\\variables.py\", line 553, in standardize_dtype\n",
      "    raise ValueError(f\"Invalid dtype: {dtype}\")\n",
      "ValueError: Invalid dtype: TrackedDict\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 227, in run_step\n",
      "    return self._execute_controller(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 279, in _execute_controller\n",
      "    context, binaries = controller.execute(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 587, in execute\n",
      "    return super().execute(step, operator, dataset, context, runner, source, mode, loaded_binaries)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 211, in execute\n",
      "    return self._execute_training_mode(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 259, in _execute_training_mode\n",
      "    return self._execute_cross_validation(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 1357, in _execute_cross_validation\n",
      "    fold_context, fold_binaries = self._execute_train(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 1409, in _execute_train\n",
      "    trained_model = self._train_model(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 164, in _train_model\n",
      "    model = self._create_model_from_function(model, input_shape, model_params)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 140, in _create_model_from_function\n",
      "    raise ValueError(f\"Error creating model from function {model_function.__name__}: {e}\") from e\n",
      "ValueError: Error creating model from function nicon_classification: Invalid dtype: TrackedDict\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 135, in _create_model_from_function\n",
      "    model = model_function(input_shape, params)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\operators\\models\\cirad_tf.py\", line 681, in nicon_classification\n",
      "    model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 122, in add\n",
      "    self._maybe_rebuild()\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 149, in _maybe_rebuild\n",
      "    self.build(input_shape)\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 230, in build_wrapper\n",
      "    original_build_method(*args, **kwargs)\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 195, in build\n",
      "    x = layer(x)\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\backend\\common\\variables.py\", line 553, in standardize_dtype\n",
      "    raise ValueError(f\"Invalid dtype: {dtype}\")\n",
      "ValueError: Invalid dtype: TrackedDict\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 227, in run_step\n",
      "    return self._execute_controller(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 279, in _execute_controller\n",
      "    context, binaries = controller.execute(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 587, in execute\n",
      "    return super().execute(step, operator, dataset, context, runner, source, mode, loaded_binaries)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 211, in execute\n",
      "    return self._execute_training_mode(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 259, in _execute_training_mode\n",
      "    return self._execute_cross_validation(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 1357, in _execute_cross_validation\n",
      "    fold_context, fold_binaries = self._execute_train(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py\", line 1409, in _execute_train\n",
      "    trained_model = self._train_model(\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 164, in _train_model\n",
      "    model = self._create_model_from_function(model, input_shape, model_params)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py\", line 140, in _create_model_from_function\n",
      "    raise ValueError(f\"Error creating model from function {model_function.__name__}: {e}\") from e\n",
      "ValueError: Error creating model from function nicon_classification: Invalid dtype: TrackedDict\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 90, in run\n",
      "    self.run_steps(config.steps, dataset, context, execution=\"sequential\")\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 133, in run_steps\n",
      "    context = self.run_step(step, dataset, context, is_substep=is_substep)\n",
      "  File \"D:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py\", line 242, in run_step\n",
      "    raise RuntimeError(f\"Pipeline step failed: {str(e)}\") from e\n",
      "RuntimeError: Pipeline step failed: Error creating model from function nicon_classification: Invalid dtype: TrackedDict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m❌ Pipeline config_d4e86654 on dataset classification failed: \n",
      "Pipeline step failed: Error creating model from function nicon_classification: Invalid dtype: TrackedDict\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pipeline step failed: Error creating model from function nicon_classification: Invalid dtype: TrackedDict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py:135\u001b[0m, in \u001b[0;36mTensorFlowModelController._create_model_from_function\u001b[1;34m(self, model_function, input_shape, params)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_tensorflow_model(model):\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\operators\\models\\cirad_tf.py:681\u001b[0m, in \u001b[0;36mnicon_classification\u001b[1;34m(input_shape, num_classes, params)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py:122\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py:149\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# model.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\layers\\layer.py:230\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[1;32m--> 230\u001b[0m     original_build_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\models\\sequential.py:195\u001b[0m, in \u001b[0;36mSequential.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\keras\\src\\backend\\common\\variables.py:553\u001b[0m, in \u001b[0;36mstandardize_dtype\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mALLOWED_DTYPES:\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: TrackedDict",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:227\u001b[0m, in \u001b[0;36mPipelineRunner.run_step\u001b[1;34m(self, step, dataset, context, is_substep)\u001b[0m\n\u001b[0;32m    226\u001b[0m     context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_controller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_binaries\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# self.history.complete_step(step_execution.step_id)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:279\u001b[0m, in \u001b[0;36mPipelineRunner._execute_controller\u001b[1;34m(self, controller, step, operator, dataset, context, source, loaded_binaries)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔹 Executing controller \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontroller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m without operator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 279\u001b[0m context, binaries \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaded_binaries\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Save binaries if in training mode and saving is enabled\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py:587\u001b[0m, in \u001b[0;36mTensorFlowModelController.execute\u001b[1;34m(self, step, operator, dataset, context, runner, source, mode, loaded_binaries)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# print(\"🧠 Executing TensorFlow model controller\")\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m# Call parent execute method\u001b[39;00m\n\u001b[1;32m--> 587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_binaries\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py:211\u001b[0m, in \u001b[0;36mBaseModelController.execute\u001b[1;34m(self, step, operator, dataset, context, runner, source, mode, loaded_binaries)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Training/finetuning mode - original logic\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_training_mode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py:259\u001b[0m, in \u001b[0;36mBaseModelController._execute_training_mode\u001b[1;34m(self, step, operator, dataset, context, runner)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Standard cross-validation without finetuning\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_cross_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;66;03m# Single training mode: no folds\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py:1357\u001b[0m, in \u001b[0;36mBaseModelController._execute_cross_validation\u001b[1;34m(self, model_config, data_splits, train_params, finetune_params, mode, context, runner, dataset)\u001b[0m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;66;03m# Train for this fold\u001b[39;00m\n\u001b[1;32m-> 1357\u001b[0m     fold_context, fold_binaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_idx\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;66;03m# Add fold suffix to binary names\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\models\\base_model_controller.py:1409\u001b[0m, in \u001b[0;36mBaseModelController._execute_train\u001b[1;34m(self, model_config, X_train, y_train, X_test, y_test, train_params, context, runner, dataset, fold_idx)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m-> 1409\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_prep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_prep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_params\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py:164\u001b[0m, in \u001b[0;36mTensorFlowModelController._train_model\u001b[1;34m(self, model, X_train, y_train, X_val, y_val, train_params)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# print(f\"🏗️ Creating TensorFlow model from function {model.__name__} with input_shape={input_shape}\")\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m verbose \u001b[38;5;241m=\u001b[39m train_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\controllers\\tensorflow\\op_model.py:140\u001b[0m, in \u001b[0;36mTensorFlowModelController._create_model_from_function\u001b[1;34m(self, model_function, input_shape, params)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError creating model from function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Error creating model from function nicon_classification: Invalid dtype: TrackedDict",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m config \u001b[38;5;241m=\u001b[39m PipelineConfig(pipeline_config)\n\u001b[0;32m     22\u001b[0m runner \u001b[38;5;241m=\u001b[39m PipelineRunner()\n\u001b[1;32m---> 23\u001b[0m res_dataset, history, pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Simple transformers pipeline completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres_dataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:90\u001b[0m, in \u001b[0;36mPipelineRunner.run\u001b[1;34m(self, config, dataset)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# context = {\"branch\": 0, \"processing\": \"raw\", \"y\": \"numeric\"} ## TODO handle branch indexing in context\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# self.history.complete_execution()\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Save enhanced configuration with metadata if saving binaries\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_binaries:\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:133\u001b[0m, in \u001b[0;36mPipelineRunner.run_steps\u001b[1;34m(self, steps, dataset, context, execution, is_substep)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(context, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# print(\"🔄 Running steps sequentially with shared context\")\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m--> 133\u001b[0m         context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_substep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_substep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;66;03m# print(f\"🔹 Updated context after step: {context}\")\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubstep_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Reset sub-step number after sequential execution\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\pipeline\\runner.py:242\u001b[0m, in \u001b[0;36mPipelineRunner.run_step\u001b[1;34m(self, step, dataset, context, is_substep)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ Step failed but continuing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline step failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_substep:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Pipeline step failed: Error creating model from function nicon_classification: Invalid dtype: TrackedDict"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "# from sample import dataset_config, pipeline_config\n",
    "from sample_classif import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config)\n",
    "runner = PipelineRunner()\n",
    "res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "print(\"✅ Simple transformers pipeline completed successfully!\")\n",
    "print(f\"Final dataset: {res_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae6a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "========================================================================================================================================================================================================\n",
      "\u001b[94mLoading dataset:\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "⚠️ Dataset does not have data for train_group.\n",
      "⚠️ Dataset does not have data for test_group.\n",
      "\u001b[97m📊 Dataset: regression\n",
      "Features (samples=189, sources=1):\n",
      "- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)\n",
      "Targets: (samples=189, targets=1, processings=['numeric'])\n",
      "- numeric: min=1.33, max=128.31, mean=30.779\n",
      "Indexes:\n",
      "- \"train\", ['raw']: 130 samples\n",
      "- \"test\", ['raw']: 59 samples\u001b[0m\n",
      "Configuration generates 80 configurations.\n",
      "Generated 80 configurations.\n"
     ]
    }
   ],
   "source": [
    "# Reset autoreload completely\n",
    "try:\n",
    "    %autoreload 0  # Disable autoreload\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2  # Re-enable with full reload\n",
    "except:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "from nirs4all.pipeline.runner import PipelineRunner\n",
    "from nirs4all.pipeline.config import PipelineConfig\n",
    "from nirs4all.dataset import dataset\n",
    "from sample import dataset_config, pipeline_config, generator_config\n",
    "# from sample_classif import dataset_config, pipeline_config\n",
    "from nirs4all.dataset.loader import get_dataset\n",
    "from nirs4all.controllers.registry import reset_registry\n",
    "from nirs4all.controllers import *\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(generator_config)\n",
    "if config.has_configurations:\n",
    "    print(f\"Generated {len(config.steps)} configurations.\")\n",
    "    # for i, step in enumerate(config.steps):\n",
    "        # print(config)\n",
    "# runner = PipelineRunner()\n",
    "# res_dataset, history, pipeline = runner.run(config, data)\n",
    "\n",
    "# print(\"✅ Simple transformers pipeline completed successfully!\")\n",
    "# print(f\"Final dataset: {res_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc19bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_dataset(dataset_config)\n",
    "config = PipelineConfig(pipeline_config)\n",
    "runner = PipelineRunner()\n",
    "\n",
    "pipeline_path = \"results/sample_data/config_89f2f434\"\n",
    "pred_dataset, pred_context = PipelineRunner.predict(\n",
    "    path=pipeline_path,\n",
    "    dataset=data,  # Use same data for testing\n",
    "    verbose=1\n",
    ")\n",
    "print(\"✅ Prediction mode completed successfully!\")\n",
    "\n",
    "predictions_obj = pred_dataset._predictions\n",
    "pred_count = len(predictions_obj._predictions)\n",
    "print(f\"🎯 Generated {pred_count} prediction records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76236b",
   "metadata": {},
   "source": [
    "# 📊 Prediction Visualization Testing\n",
    "\n",
    "Test the new `PredictionVisualizer` class for analyzing and displaying prediction results graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc283ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and test the PredictionVisualizer\n",
    "from nirs4all.dataset.prediction_visualizer import PredictionVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"🔍 Testing PredictionVisualizer with current prediction results...\")\n",
    "viz = PredictionVisualizer(predictions_obj)\n",
    "print(\"\\n\" + viz.summary_report())\n",
    "\n",
    "# Show only best predictions\n",
    "viz.plot_filtered_predictions(prediction_filter='best_only', metric='r2')\n",
    "\n",
    "# Performance matrix with only global predictions\n",
    "viz.plot_performance_matrix(prediction_filter='global_only')\n",
    "\n",
    "# Bar chart comparing all prediction types\n",
    "viz.plot_filtered_predictions(chart_type='bar', metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63943e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Performance Matrix Visualization\n",
    "print(\"📈 Testing Performance Matrix...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Create performance matrix plot\n",
    "\n",
    "        plt.show()\n",
    "        print(\"✅ Performance matrix plot created successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating performance matrix: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Multi-Metric Comparison\n",
    "print(\"📊 Testing Multi-Metric Comparison...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Create multi-metric comparison plot\n",
    "        fig2 = viz.plot_multi_metric_comparison(\n",
    "            metrics=['rmse', 'mae', 'r2', 'mse'],  # Show all metrics\n",
    "            sort_by='r2',      # Sort by R² (best first)\n",
    "            ascending=False,   # Higher R² is better\n",
    "            figsize=(16, 10)\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "        print(\"✅ Multi-metric comparison plot created successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating multi-metric plot: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc18d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Prediction Scatter Plots\n",
    "print(\"🎯 Testing Prediction Scatter Plots...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Create scatter plot of true vs predicted values\n",
    "        fig3 = viz.plot_prediction_scatter(\n",
    "            figsize=(14, 8)  # Show scatter plots for all model-config combinations\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "        print(\"✅ Prediction scatter plots created successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating scatter plots: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Demonstrate different sorting and filtering options\n",
    "print(\"⚙️ Testing Advanced Visualization Options...\")\n",
    "\n",
    "try:\n",
    "    if 'viz' in globals():\n",
    "        # Test different sorting options\n",
    "        print(\"\\n🔄 Creating performance matrix sorted by best config:\")\n",
    "        fig4a = viz.plot_performance_matrix(\n",
    "            metric='r2',         # Use R² metric\n",
    "            sort_by='config',    # Sort by config performance\n",
    "            ascending=False,     # Best (highest R²) first\n",
    "            figsize=(10, 6),\n",
    "            cmap='RdYlGn',      # Green is better for R²\n",
    "            show_values=True\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n📈 Creating RMSE matrix sorted by model:\")\n",
    "        fig4b = viz.plot_performance_matrix(\n",
    "            metric='rmse',       # Use RMSE metric\n",
    "            sort_by='model',     # Sort by model performance\n",
    "            ascending=True,      # Best (lowest RMSE) first\n",
    "            figsize=(10, 6),\n",
    "            cmap='RdYlGn_r',    # Red is worse for RMSE\n",
    "            show_values=True\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"✅ Advanced visualization options tested successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Visualizer not initialized. Run previous cell first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in advanced visualization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Explore dictionary structure\n",
    "print(\"🔍 Exploring prediction dictionary structure...\")\n",
    "\n",
    "try:\n",
    "    if 'pred_dataset' in globals() and hasattr(pred_dataset, '_predictions'):\n",
    "        predictions_obj = pred_dataset._predictions\n",
    "        pred_data = predictions_obj._predictions\n",
    "\n",
    "        print(f\"Dictionary keys: {list(pred_data.keys())}\")\n",
    "\n",
    "        # Explore each key\n",
    "        for key, value in pred_data.items():\n",
    "            print(f\"\\nKey: {key}\")\n",
    "            print(f\"  Type: {type(value)}\")\n",
    "            if hasattr(value, '__len__'):\n",
    "                print(f\"  Length: {len(value)}\")\n",
    "\n",
    "            # Show first few items or structure\n",
    "            if isinstance(value, (list, tuple)) and len(value) > 0:\n",
    "                print(f\"  First item type: {type(value[0])}\")\n",
    "                print(f\"  First item: {value[0]}\")\n",
    "            elif isinstance(value, dict):\n",
    "                print(f\"  Dict keys: {list(value.keys())}\")\n",
    "                for subkey, subvalue in list(value.items())[:2]:  # Show first 2 items\n",
    "                    print(f\"    {subkey}: {type(subvalue)} = {subvalue}\")\n",
    "            else:\n",
    "                print(f\"  Value: {value}\")\n",
    "\n",
    "        # Try the get_predictions method\n",
    "        print(f\"\\n🔍 Trying get_predictions():\")\n",
    "        try:\n",
    "            preds = predictions_obj.get_predictions()\n",
    "            print(f\"get_predictions() type: {type(preds)}\")\n",
    "            if isinstance(preds, dict):\n",
    "                print(f\"Keys: {list(preds.keys())}\")\n",
    "                for key, value in list(preds.items())[:2]:\n",
    "                    print(f\"  {key}: {type(value)}\")\n",
    "                    if isinstance(value, dict):\n",
    "                        print(f\"    Sub-keys: {list(value.keys())}\")\n",
    "            elif isinstance(preds, list):\n",
    "                print(f\"List length: {len(preds)}\")\n",
    "                if preds:\n",
    "                    print(f\"First item: {preds[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"get_predictions() error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error exploring structure: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473080b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Cross-Validation Prediction Analysis\n",
    "print(\"🎯 CROSS-VALIDATION PREDICTION ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_cv_predictions(predictions_obj):\n",
    "    \"\"\"Analyze cross-validation predictions by grouping fold results properly\"\"\"\n",
    "\n",
    "    # Group predictions by base model name and partition type\n",
    "    model_groups = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for key, pred_data in predictions_obj._predictions.items():\n",
    "        model_name = pred_data['model']\n",
    "        partition = pred_data['partition']\n",
    "\n",
    "        # Extract base model name (everything before the last underscore + number)\n",
    "        base_model_match = re.match(r'(.+?)_\\d+$', model_name)\n",
    "        base_model = base_model_match.group(1) if base_model_match else model_name\n",
    "\n",
    "        # Categorize partition types\n",
    "        if 'test_fold' in partition:\n",
    "            partition_type = 'Fold Test'\n",
    "        elif 'val_fold' in partition:\n",
    "            partition_type = 'Fold Validation'\n",
    "        elif 'train_fold' in partition:\n",
    "            partition_type = 'Fold Train'\n",
    "        elif 'test' in partition and 'fold' not in partition:\n",
    "            partition_type = 'Global Test'\n",
    "        elif 'global_train' in partition:\n",
    "            partition_type = 'Global Train'\n",
    "        else:\n",
    "            partition_type = partition\n",
    "\n",
    "        model_groups[base_model][partition_type].append({\n",
    "            'model_instance': model_name,\n",
    "            'partition': partition,\n",
    "            'data': pred_data,\n",
    "            'fold_idx': pred_data.get('fold_idx', 0)\n",
    "        })\n",
    "\n",
    "    return model_groups\n",
    "\n",
    "if 'cv_result' in globals() and hasattr(cv_result, '_predictions'):\n",
    "    print(\"📊 Analyzing Cross-Validation Results...\")\n",
    "    print(f\"Total prediction records: {len(cv_result._predictions)}\")\n",
    "\n",
    "    # Analyze CV predictions\n",
    "    cv_analysis = analyze_cv_predictions(cv_result._predictions)\n",
    "\n",
    "    print(f\"Base models found: {list(cv_analysis.keys())}\")\n",
    "    print()\n",
    "\n",
    "    for base_model, prediction_types in cv_analysis.items():\n",
    "        print(f\"🤖 Base Model: {base_model}\")\n",
    "        print(\"─\" * 50)\n",
    "\n",
    "        for pred_type, predictions in prediction_types.items():\n",
    "            print(f\"  📈 {pred_type}:\")\n",
    "\n",
    "            if len(predictions) == 1:\n",
    "                # Single prediction\n",
    "                pred = predictions[0]\n",
    "                data = pred['data']\n",
    "                y_true = np.array(data['y_true']).flatten()\n",
    "                y_pred = np.array(data['y_pred']).flatten()\n",
    "\n",
    "                # Calculate metrics\n",
    "                rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "                r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "                mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "                print(f\"    ✅ {len(y_true)} samples\")\n",
    "                print(f\"    📊 RMSE: {rmse:.4f}, R²: {r2:.4f}, MAE: {mae:.4f}\")\n",
    "                print(f\"    🎯 Model Instance: {pred['model_instance']}\")\n",
    "\n",
    "            else:\n",
    "                # Multiple fold predictions\n",
    "                print(f\"    ✅ {len(predictions)} fold predictions:\")\n",
    "\n",
    "                all_rmse = []\n",
    "                all_r2 = []\n",
    "                all_samples = 0\n",
    "\n",
    "                for pred in sorted(predictions, key=lambda x: x['fold_idx']):\n",
    "                    data = pred['data']\n",
    "                    y_true = np.array(data['y_true']).flatten()\n",
    "                    y_pred = np.array(data['y_pred']).flatten()\n",
    "\n",
    "                    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "                    r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "\n",
    "                    all_rmse.append(rmse)\n",
    "                    all_r2.append(r2)\n",
    "                    all_samples += len(y_true)\n",
    "\n",
    "                    fold_info = pred['partition'].split('_')[-1] if '_' in pred['partition'] else pred['fold_idx']\n",
    "                    print(f\"      Fold {fold_info}: {len(y_true)} samples, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
    "                    print(f\"        Instance: {pred['model_instance']}\")\n",
    "\n",
    "                # Show aggregate statistics\n",
    "                print(f\"    📊 Aggregate: {all_samples} total samples\")\n",
    "                print(f\"    📈 Average Performance: RMSE={np.mean(all_rmse):.4f}±{np.std(all_rmse):.4f}\")\n",
    "                print(f\"                           R²={np.mean(all_r2):.4f}±{np.std(all_r2):.4f}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "    # Now test combining fold predictions properly\n",
    "    print(\"=\" * 55)\n",
    "    print(\"🧮 TESTING FOLD COMBINATION\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    # Get base model name for combination\n",
    "    base_models = list(cv_analysis.keys())\n",
    "    if base_models:\n",
    "        test_base_model = base_models[0]\n",
    "        print(f\"Testing fold combination for: {test_base_model}\")\n",
    "\n",
    "        # Find all test fold predictions for this base model\n",
    "        test_folds = cv_analysis[test_base_model].get('Fold Test', [])\n",
    "\n",
    "        if test_folds:\n",
    "            print(f\"Found {len(test_folds)} test fold predictions to combine\")\n",
    "\n",
    "            # Combine manually since each fold has different sample indices\n",
    "            combined_y_true = []\n",
    "            combined_y_pred = []\n",
    "            combined_indices = []\n",
    "\n",
    "            for fold_pred in test_folds:\n",
    "                data = fold_pred['data']\n",
    "                combined_y_true.extend(data['y_true'])\n",
    "                combined_y_pred.extend(data['y_pred'])\n",
    "                combined_indices.extend(data.get('sample_indices', []))\n",
    "\n",
    "            combined_y_true = np.array(combined_y_true)\n",
    "            combined_y_pred = np.array(combined_y_pred)\n",
    "\n",
    "            # Calculate combined performance\n",
    "            combined_rmse = np.sqrt(np.mean((combined_y_true - combined_y_pred) ** 2))\n",
    "            combined_r2 = 1 - (np.sum((combined_y_true - combined_y_pred) ** 2) /\n",
    "                              np.sum((combined_y_true - np.mean(combined_y_true)) ** 2))\n",
    "\n",
    "            print(f\"✅ Successfully combined fold predictions:\")\n",
    "            print(f\"   📊 Total samples: {len(combined_y_true)}\")\n",
    "            print(f\"   📈 Combined RMSE: {combined_rmse:.4f}\")\n",
    "            print(f\"   📈 Combined R²: {combined_r2:.4f}\")\n",
    "\n",
    "        else:\n",
    "            print(\"⚠️ No test fold predictions found for combination\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No cross-validation results available\")\n",
    "    print(\"   Please run a cross-validation pipeline first to see comprehensive prediction analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Prediction Visualizer with Filtering\n",
    "print(\"🎯 ENHANCED PREDICTION VISUALIZATION WITH FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    if 'pred_dataset' in globals() and hasattr(pred_dataset, '_predictions'):\n",
    "        # Create enhanced visualizer with dataset name override\n",
    "        dataset_name = getattr(pred_dataset, 'name', 'Sample NIRS Dataset')\n",
    "        viz_enhanced = PredictionVisualizer(pred_dataset._predictions, dataset_name_override=dataset_name)\n",
    "\n",
    "        print(f\"✅ Enhanced visualizer created with dataset: {dataset_name}\")\n",
    "        print(f\"📊 Total prediction records: {len(viz_enhanced.data)}\")\n",
    "\n",
    "        # Test different filter types\n",
    "        filter_types = ['all', 'best_only', 'folds_only', 'averaged_only', 'global_only']\n",
    "\n",
    "        for filter_type in filter_types:\n",
    "            print(f\"\\n🔍 Testing filter: {filter_type}\")\n",
    "\n",
    "            # Get filtered data count\n",
    "            filtered_data = viz_enhanced._filter_data_by_prediction_types(filter_type)\n",
    "            print(f\"   📈 Filtered to {len(filtered_data)} records\")\n",
    "\n",
    "            if len(filtered_data) > 0:\n",
    "                # Show sample of what's included\n",
    "                sample_partitions = [pred.get('partition', 'unknown') for pred in filtered_data[:3]]\n",
    "                print(f\"   🎯 Sample partitions: {sample_partitions}\")\n",
    "\n",
    "        print(f\"\\n✅ All filters tested successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ No prediction data found. Run the prediction cell first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing enhanced visualizer: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c341a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Filtering Visualizations\n",
    "print(\"🎨 TESTING FILTERED VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    if 'viz_enhanced' in globals():\n",
    "        # Test 1: Show all predictions as bar chart\n",
    "        print(\"\\n📊 1. All Predictions Bar Chart\")\n",
    "        fig1 = viz_enhanced.plot_filtered_predictions(\n",
    "            prediction_filter='all',\n",
    "            metric='rmse',\n",
    "            chart_type='bar',\n",
    "            figsize=(14, 8)\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # Test 2: Show only best predictions\n",
    "        print(\"\\n🏆 2. Best Predictions Only\")\n",
    "        fig2 = viz_enhanced.plot_filtered_predictions(\n",
    "            prediction_filter='best_only',\n",
    "            metric='r2',\n",
    "            chart_type='bar',\n",
    "            figsize=(12, 6)\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # Test 3: Show only fold predictions\n",
    "        print(\"\\n📈 3. Cross-Validation Folds Only\")\n",
    "        fig3 = viz_enhanced.plot_filtered_predictions(\n",
    "            prediction_filter='folds_only',\n",
    "            metric='rmse',\n",
    "            chart_type='bar',\n",
    "            figsize=(16, 8)\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # Test 4: Performance matrix with filtering\n",
    "        print(\"\\n🔥 4. Performance Matrix - Global Predictions Only\")\n",
    "        fig4 = viz_enhanced.plot_performance_matrix(\n",
    "            metric='r2',\n",
    "            prediction_filter='global_only',\n",
    "            figsize=(10, 6),\n",
    "            show_values=True\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"✅ All filtered visualizations completed successfully!\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Enhanced visualizer not found. Run previous cell first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in filtered visualizations: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91691968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Prediction Summary with Proper Dataset Names\n",
    "print(\"📋 COMPREHENSIVE PREDICTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    if 'viz_enhanced' in globals():\n",
    "        # Show comprehensive summary with fixed dataset names\n",
    "        summary = viz_enhanced.comprehensive_prediction_summary()\n",
    "        print(summary)\n",
    "\n",
    "        # Show how the dataset name override works\n",
    "        print(f\"\\n🏷️  Dataset Name Override: {viz_enhanced.dataset_name_override}\")\n",
    "        print(f\"📊 Data Records: {len(viz_enhanced.data)}\")\n",
    "\n",
    "        # Demonstrate filtering examples\n",
    "        print(f\"\\n🔍 FILTERING EXAMPLES:\")\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "        filters_to_test = {\n",
    "            'all': 'All prediction types',\n",
    "            'best_only': 'Best performing prediction per model',\n",
    "            'folds_only': 'Cross-validation fold predictions only',\n",
    "            'averaged_only': 'Averaged predictions only',\n",
    "            'global_only': 'Global train/test predictions only'\n",
    "        }\n",
    "\n",
    "        for filter_name, description in filters_to_test.items():\n",
    "            filtered_count = len(viz_enhanced._filter_data_by_prediction_types(filter_name))\n",
    "            print(f\"• {filter_name:15} | {description:35} | {filtered_count:2} records\")\n",
    "\n",
    "        print(f\"\\n✅ Enhanced PredictionVisualizer ready with:\")\n",
    "        print(f\"   🎯 Dataset name fixes (no more 'unknown')\")\n",
    "        print(f\"   🔍 Filtering capabilities (5 filter types)\")\n",
    "        print(f\"   📊 Enhanced visualization methods\")\n",
    "        print(f\"   📈 Bar charts for prediction type comparison\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Enhanced visualizer not found. Run previous cell first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in comprehensive summary: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
