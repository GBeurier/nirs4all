{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304bfbf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnirs4all\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectraDataset\n\u001b[0;32m      7\u001b[0m dataset_reg_1_1 \u001b[38;5;241m=\u001b[39m SpectraDataset(task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Single-output regression\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dataset_reg_2_1 \u001b[38;5;241m=\u001b[39m SpectraDataset(task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Single-output regression (first output)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\spectra\\__init__.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# nirs4all/spectra/__init__.py\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"This module initializes the spectra package for NIRS4ALL.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mIt imports the main classes and functions used for handling spectral data,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mfeatures, and targets in the NIRS4ALL project.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectraDataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra_features\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectraFeatures\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra_targets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectraTargets\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\spectra\\spectra_dataset.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnirs4all\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra_features\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectraFeatures\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnirs4all\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra_targets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectraTargets\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnirs4all\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra_predictions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpectraPredictions\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnirs4all\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectra_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data_from_config\n",
      "File \u001b[1;32mD:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\spectra\\spectra_targets.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Union, Tuple\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\__init__.py:73\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     __check_build,\n\u001b[0;32m     71\u001b[0m     _distributor_init,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m ]\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[0;32m     19\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\utils\\fixes.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\scipy\\stats\\__init__.py:624\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    619\u001b[0m \n\u001b[0;32m    620\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    623\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 624\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, _get_nan,\n\u001b[0;32m     43\u001b[0m                               _rename_parameter, _contains_nan,\n\u001b[0;32m     44\u001b[0m                               AxisError, _lazywhere)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_positional_args\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\scipy\\optimize\\__init__.py:435\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_nnls\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nnls\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_basinhopping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m basinhopping\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linprog, linprog_verbose_callback\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lsap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_differentialevolution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m differential_evolution\n",
      "File \u001b[1;32md:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\scipy\\optimize\\_linprog.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog_highs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _linprog_highs\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog_ip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _linprog_ip\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog_simplex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _linprog_simplex\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog_rs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _linprog_rs\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1016\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from nirs4all.spectra import SpectraDataset\n",
    "\n",
    "dataset_reg_1_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression\n",
    "dataset_reg_2_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression (first output)\n",
    "dataset_reg_2_2 = SpectraDataset(task_type=\"regression\")  # Single-output regression (second output)\n",
    "dataset_cla_2_1 = SpectraDataset(task_type=\"classification\")  # Single-output classification (first output)\n",
    "dataset_cla_2_2 = SpectraDataset(task_type=\"classification\")  # Single-output classification (second output)\n",
    "dataset_bin_1_1 = SpectraDataset(task_type=\"binary\")  # Binary classification\n",
    "\n",
    "# Features\n",
    "f1_source = np.random.rand(100, 1000) * 2.5 + 1.5\n",
    "f2_source = np.random.rand(100, 500) * 12 + 3.5\n",
    "\n",
    "# Targets\n",
    "reg_target_1 = np.random.rand(100,)  # 1D array for single-output regression\n",
    "reg_target_2_first = np.random.rand(100,)  # First output of multi-output regression\n",
    "reg_target_2_second = np.random.rand(100,)  # Second output of multi-output regression\n",
    "cla_target_2_first = np.random.randint(0, 5, size=(100,))  # First output of multi-output classification\n",
    "cla_target_2_second = np.random.randint(0, 5, size=(100,))  # Second output of multi-output classification\n",
    "bin_target_1 = np.random.randint(0, 2, size=(100,))  # 1D array for binary classification\n",
    "\n",
    "# Add data to datasets\n",
    "dataset_reg_1_1.add_data([f1_source], reg_target_1)\n",
    "dataset_reg_2_1.add_data([f1_source, f2_source], reg_target_2_first)\n",
    "dataset_reg_2_2.add_data([f1_source, f2_source], reg_target_2_second)\n",
    "dataset_cla_2_1.add_data([f1_source, f2_source], cla_target_2_first)\n",
    "dataset_cla_2_2.add_data([f1_source, f2_source], cla_target_2_second)\n",
    "dataset_bin_1_1.add_data([f1_source], bin_target_1)\n",
    "pass\n",
    "# print(\"Dataset for regression 1-1:\", dataset_reg_1_1)\n",
    "# print(\"Dataset for regression 2-1 (first output):\", dataset_reg_2_1)\n",
    "# print(\"Dataset for regression 2-2 (second output):\", dataset_reg_2_2)\n",
    "# print(\"Dataset for classification 2-1 (first output):\", dataset_cla_2_1)\n",
    "# print(\"Dataset for classification 2-2 (second output):\", dataset_cla_2_2)\n",
    "# print(\"Dataset for binary classification 1-1:\", dataset_bin_1_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecc4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nirs4all.pipeline.pipeline_config:Pipeline configuration saved to test_config.yaml\n",
      "INFO:nirs4all.pipeline.pipeline_config:Pipeline configuration saved to test_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ðŸš€ Starting Pipeline Runner\n",
      "ðŸ”„ Running 9 steps in sequential mode\n",
      "ðŸ”¹ Step Dict with 3 keys\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}\n",
      "ðŸ“¦ Deserializing operation: class\n",
      "Executing transformer operation for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: Dict with 3 keys\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step feature_augmentation\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}\n",
      "ðŸ“‹ Workflow operation: feature_augmentation\n",
      "Executing dummy operation for step: {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: feature_augmentation\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step sample_augmentation\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'sample_augmentation': ['nirs4all.presets.transformations._random_augmentation.Rotate_Translate', {'class': 'nirs4all.presets.transformations._random_augmentation.Rotate_Translate', 'params': {'p_range': 3}, '_runtime_instance': Rotate_Translate(p_range=3)}]}\n",
      "ðŸ“‹ Workflow operation: sample_augmentation\n",
      "Executing dummy operation for step: {'sample_augmentation': ['nirs4all.presets.transformations._random_augmentation.Rotate_Translate', {'class': 'nirs4all.presets.transformations._random_augmentation.Rotate_Translate', 'params': {'p_range': 3}, '_runtime_instance': Rotate_Translate(p_range=3)}]}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: sample_augmentation\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step Dict with 2 keys\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.model_selection._split.ShuffleSplit', '_runtime_instance': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)}\n",
      "ðŸ“¦ Deserializing operation: class\n",
      "Executing dummy operation for step: {'class': 'sklearn.model_selection._split.ShuffleSplit', '_runtime_instance': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: Dict with 2 keys\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step cluster\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'cluster': {'class': 'sklearn.cluster._kmeans.KMeans', 'params': {'n_clusters': 5, 'random_state': 42}, '_runtime_instance': KMeans(n_clusters=5, random_state=42)}}\n",
      "ðŸ“‹ Workflow operation: cluster\n",
      "Executing dummy operation for step: {'cluster': {'class': 'sklearn.cluster._kmeans.KMeans', 'params': {'n_clusters': 5, 'random_state': 42}, '_runtime_instance': KMeans(n_clusters=5, random_state=42)}}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: cluster\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step Dict with 3 keys\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.model_selection._split.RepeatedStratifiedKFold', 'params': {'n_repeats': 2, 'random_state': 42}, '_runtime_instance': RepeatedStratifiedKFold(n_repeats=2, n_splits=5, random_state=42)}\n",
      "ðŸ“¦ Deserializing operation: class\n",
      "Executing dummy operation for step: {'class': 'sklearn.model_selection._split.RepeatedStratifiedKFold', 'params': {'n_repeats': 2, 'random_state': 42}, '_runtime_instance': RepeatedStratifiedKFold(n_repeats=2, n_splits=5, random_state=42)}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: Dict with 3 keys\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step uncluster\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: uncluster\n",
      "ðŸ“‹ Workflow operation: uncluster\n",
      "Executing dummy operation for step: uncluster, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: uncluster\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step dispatch\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'dispatch': [[{'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}, {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': 'sklearn.preprocessing._data.StandardScaler'}], {'model': {'function': 'nirs4all.presets.models.ref_models.decon'}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}}, {'model': {'class': 'sklearn.svm._classes.SVC', 'params': {'kernel': 'linear', 'random_state': 42}, '_runtime_instance': SVC(kernel='linear', random_state=42)}, 'y_pipeline': ['sklearn.preprocessing._data.MinMaxScaler', {'class': 'sklearn.preprocessing._data.RobustScaler', 'params': {'with_centering': False}, '_runtime_instance': RobustScaler(with_centering=False)}], 'finetune_params': {'C': [0.1, 1.0, 10.0]}}, {'stack': {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}, 'base_learners': [{'model': {'class': 'sklearn.ensemble._gb.GradientBoostingClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': GradientBoostingClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}}, {'model': {'class': 'sklearn.tree._classes.DecisionTreeClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': DecisionTreeClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, 'finetune_params': {'max_depth': [3, 5, 7]}}]}}]}\n",
      "ðŸ“‹ Workflow operation: dispatch\n",
      "Executing dummy operation for step: {'dispatch': [[{'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}, {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': 'sklearn.preprocessing._data.StandardScaler'}], {'model': {'function': 'nirs4all.presets.models.ref_models.decon'}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}}, {'model': {'class': 'sklearn.svm._classes.SVC', 'params': {'kernel': 'linear', 'random_state': 42}, '_runtime_instance': SVC(kernel='linear', random_state=42)}, 'y_pipeline': ['sklearn.preprocessing._data.MinMaxScaler', {'class': 'sklearn.preprocessing._data.RobustScaler', 'params': {'with_centering': False}, '_runtime_instance': RobustScaler(with_centering=False)}], 'finetune_params': {'C': [0.1, 1.0, 10.0]}}, {'stack': {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}, 'base_learners': [{'model': {'class': 'sklearn.ensemble._gb.GradientBoostingClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': GradientBoostingClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}}, {'model': {'class': 'sklearn.tree._classes.DecisionTreeClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': DecisionTreeClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, 'finetune_params': {'max_depth': [3, 5, 7]}}]}}]}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: dispatch\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step PlotConfusionMatrix\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: PlotConfusionMatrix\n",
      "ðŸ“¦ Deserializing operation: None\n",
      "Executing dummy operation for step: None, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: PlotConfusionMatrix\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "âœ… Pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nirs4all.pipeline.pipeline_config import PipelineConfig\n",
    "from nirs4all.pipeline.pipeline_runner import PipelineRunner\n",
    "import json\n",
    "from sample import config as steps\n",
    "\n",
    "# steps = [ MinMaxScaler(feature_range=(0.2,0.8)) ]\n",
    "config = PipelineConfig(steps)\n",
    "config.save(\"test_config.yaml\")\n",
    "config.save(\"test_config.json\")\n",
    "config_text = config.serializable_steps()\n",
    "\n",
    "config2 = PipelineConfig(\"test_config.yaml\")\n",
    "config2_text = config2.serializable_steps()\n",
    "assert config_text == config2_text, \"Config serialization mismatch\"\n",
    "\n",
    "config3 = PipelineConfig(\"test_config.json\")\n",
    "config3_text = config3.serializable_steps()\n",
    "assert config_text == config3_text, \"Config serialization mismatch\"\n",
    "\n",
    "runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "dataset_res, history, pipeline = runner.run(config, dataset=dataset_reg_1_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
