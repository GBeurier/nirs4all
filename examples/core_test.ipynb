{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304bfbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from nirs4all.spectra import SpectraDataset\n",
    "\n",
    "dataset_reg_1_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression\n",
    "dataset_reg_2_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression (first output)\n",
    "dataset_reg_2_2 = SpectraDataset(task_type=\"regression\")  # Single-output regression (second output)\n",
    "dataset_cla_2_1 = SpectraDataset(task_type=\"classification\")  # Single-output classification (first output)\n",
    "dataset_cla_2_2 = SpectraDataset(task_type=\"classification\")  # Single-output classification (second output)\n",
    "dataset_bin_1_1 = SpectraDataset(task_type=\"binary\")  # Binary classification\n",
    "\n",
    "# Features\n",
    "f1_source = np.random.rand(100, 1000) * 2.5 + 1.5\n",
    "f2_source = np.random.rand(100, 500) * 12 + 3.5\n",
    "\n",
    "# Targets\n",
    "reg_target_1 = np.random.rand(100,)  # 1D array for single-output regression\n",
    "reg_target_2_first = np.random.rand(100,)  # First output of multi-output regression\n",
    "reg_target_2_second = np.random.rand(100,)  # Second output of multi-output regression\n",
    "cla_target_2_first = np.random.randint(0, 5, size=(100,))  # First output of multi-output classification\n",
    "cla_target_2_second = np.random.randint(0, 5, size=(100,))  # Second output of multi-output classification\n",
    "bin_target_1 = np.random.randint(0, 2, size=(100,))  # 1D array for binary classification\n",
    "\n",
    "# Add data to datasets\n",
    "dataset_reg_1_1.add_data([f1_source], reg_target_1)\n",
    "dataset_reg_2_1.add_data([f1_source, f2_source], reg_target_2_first)\n",
    "dataset_reg_2_2.add_data([f1_source, f2_source], reg_target_2_second)\n",
    "dataset_cla_2_1.add_data([f1_source, f2_source], cla_target_2_first)\n",
    "dataset_cla_2_2.add_data([f1_source, f2_source], cla_target_2_second)\n",
    "dataset_bin_1_1.add_data([f1_source], bin_target_1)\n",
    "pass\n",
    "# print(\"Dataset for regression 1-1:\", dataset_reg_1_1)\n",
    "# print(\"Dataset for regression 2-1 (first output):\", dataset_reg_2_1)\n",
    "# print(\"Dataset for regression 2-2 (second output):\", dataset_reg_2_2)\n",
    "# print(\"Dataset for classification 2-1 (first output):\", dataset_cla_2_1)\n",
    "# print(\"Dataset for classification 2-2 (second output):\", dataset_cla_2_2)\n",
    "# print(\"Dataset for binary classification 1-1:\", dataset_bin_1_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ecc4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nirs4all.pipeline.pipeline_config:Pipeline configuration saved to test_config.yaml\n",
      "INFO:nirs4all.pipeline.pipeline_config:Pipeline configuration saved to test_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ðŸš€ Starting Pipeline Runner\n",
      "ðŸ”„ Running 9 steps in sequential mode\n",
      "ðŸ”¹ Step Dict with 3 keys\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}\n",
      "ðŸ“¦ Deserializing operation: class\n",
      "Executing transformer operation for step: {'class': 'sklearn.preprocessing._data.MinMaxScaler', 'params': {'feature_range': [0.2, 0.8]}, '_runtime_instance': MinMaxScaler(feature_range=(0.2, 0.8))}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: Dict with 3 keys\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step feature_augmentation\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}\n",
      "ðŸ“‹ Workflow operation: feature_augmentation\n",
      "Executing dummy operation for step: {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: feature_augmentation\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step sample_augmentation\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'sample_augmentation': ['nirs4all.presets.transformations._random_augmentation.Rotate_Translate', {'class': 'nirs4all.presets.transformations._random_augmentation.Rotate_Translate', 'params': {'p_range': 3}, '_runtime_instance': Rotate_Translate(p_range=3)}]}\n",
      "ðŸ“‹ Workflow operation: sample_augmentation\n",
      "Executing dummy operation for step: {'sample_augmentation': ['nirs4all.presets.transformations._random_augmentation.Rotate_Translate', {'class': 'nirs4all.presets.transformations._random_augmentation.Rotate_Translate', 'params': {'p_range': 3}, '_runtime_instance': Rotate_Translate(p_range=3)}]}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: sample_augmentation\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step Dict with 2 keys\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.model_selection._split.ShuffleSplit', '_runtime_instance': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)}\n",
      "ðŸ“¦ Deserializing operation: class\n",
      "Executing dummy operation for step: {'class': 'sklearn.model_selection._split.ShuffleSplit', '_runtime_instance': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: Dict with 2 keys\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step cluster\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'cluster': {'class': 'sklearn.cluster._kmeans.KMeans', 'params': {'n_clusters': 5, 'random_state': 42}, '_runtime_instance': KMeans(n_clusters=5, random_state=42)}}\n",
      "ðŸ“‹ Workflow operation: cluster\n",
      "Executing dummy operation for step: {'cluster': {'class': 'sklearn.cluster._kmeans.KMeans', 'params': {'n_clusters': 5, 'random_state': 42}, '_runtime_instance': KMeans(n_clusters=5, random_state=42)}}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: cluster\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step Dict with 3 keys\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'class': 'sklearn.model_selection._split.RepeatedStratifiedKFold', 'params': {'n_repeats': 2, 'random_state': 42}, '_runtime_instance': RepeatedStratifiedKFold(n_repeats=2, n_splits=5, random_state=42)}\n",
      "ðŸ“¦ Deserializing operation: class\n",
      "Executing dummy operation for step: {'class': 'sklearn.model_selection._split.RepeatedStratifiedKFold', 'params': {'n_repeats': 2, 'random_state': 42}, '_runtime_instance': RepeatedStratifiedKFold(n_repeats=2, n_splits=5, random_state=42)}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: Dict with 3 keys\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step uncluster\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: uncluster\n",
      "ðŸ“‹ Workflow operation: uncluster\n",
      "Executing dummy operation for step: uncluster, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: uncluster\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step dispatch\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: {'dispatch': [[{'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}, {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': 'sklearn.preprocessing._data.StandardScaler'}], {'model': {'function': 'nirs4all.presets.models.ref_models.decon'}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}}, {'model': {'class': 'sklearn.svm._classes.SVC', 'params': {'kernel': 'linear', 'random_state': 42}, '_runtime_instance': SVC(kernel='linear', random_state=42)}, 'y_pipeline': ['sklearn.preprocessing._data.MinMaxScaler', {'class': 'sklearn.preprocessing._data.RobustScaler', 'params': {'with_centering': False}, '_runtime_instance': RobustScaler(with_centering=False)}], 'finetune_params': {'C': [0.1, 1.0, 10.0]}}, {'stack': {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}, 'base_learners': [{'model': {'class': 'sklearn.ensemble._gb.GradientBoostingClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': GradientBoostingClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}}, {'model': {'class': 'sklearn.tree._classes.DecisionTreeClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': DecisionTreeClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, 'finetune_params': {'max_depth': [3, 5, 7]}}]}}]}\n",
      "ðŸ“‹ Workflow operation: dispatch\n",
      "Executing dummy operation for step: {'dispatch': [[{'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, {'feature_augmentation': [None, 'nirs4all.presets.transformations._nirs.SavitzkyGolay', ['sklearn.preprocessing._data.StandardScaler', 'nirs4all.presets.transformations._standard.Gaussian']]}, {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': 'sklearn.preprocessing._data.StandardScaler'}], {'model': {'function': 'nirs4all.presets.models.ref_models.decon'}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}}, {'model': {'class': 'sklearn.svm._classes.SVC', 'params': {'kernel': 'linear', 'random_state': 42}, '_runtime_instance': SVC(kernel='linear', random_state=42)}, 'y_pipeline': ['sklearn.preprocessing._data.MinMaxScaler', {'class': 'sklearn.preprocessing._data.RobustScaler', 'params': {'with_centering': False}, '_runtime_instance': RobustScaler(with_centering=False)}], 'finetune_params': {'C': [0.1, 1.0, 10.0]}}, {'stack': {'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 10, 'random_state': 42}, '_runtime_instance': RandomForestClassifier(max_depth=10, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.StandardScaler', '_runtime_instance': StandardScaler()}, 'base_learners': [{'model': {'class': 'sklearn.ensemble._gb.GradientBoostingClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': GradientBoostingClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}}, {'model': {'class': 'sklearn.tree._classes.DecisionTreeClassifier', 'params': {'max_depth': 5, 'random_state': 42}, '_runtime_instance': DecisionTreeClassifier(max_depth=5, random_state=42)}, 'y_pipeline': {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}, 'finetune_params': {'max_depth': [3, 5, 7]}}]}}]}, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: dispatch\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ðŸ”¹ Step PlotConfusionMatrix\n",
      "ðŸ”¹ Current context: {'dataset': {'branch': 0}}\n",
      "ðŸ”¹ Step config: PlotConfusionMatrix\n",
      "ðŸ“¦ Deserializing operation: None\n",
      "Executing dummy operation for step: None, keyword: \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Step completed: PlotConfusionMatrix\n",
      "Dataset state after step:\n",
      "\n",
      "Source 0: 100x1000 Mean: 2.75, Std: 0.07\n",
      "\n",
      "Samples: 100, Rows: 100, Features: 1\n",
      "Partitions: ['train']\n",
      "  train: 100 samples\n",
      "Groups: [0] - Branches: [0] - Processing: ['raw']\n",
      "Targets: {'classes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'n_samples': 100}\n",
      "Results: {'n_predictions': 0, 'models': [], 'partitions': [], 'folds': []}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "âœ… Pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nirs4all.pipeline.pipeline_config import PipelineConfig\n",
    "from nirs4all.pipeline.pipeline_runner import PipelineRunner\n",
    "import json\n",
    "from sample import config as steps\n",
    "\n",
    "# steps = [ MinMaxScaler(feature_range=(0.2,0.8)) ]\n",
    "config = PipelineConfig(steps)\n",
    "config.save(\"test_config.yaml\")\n",
    "config.save(\"test_config.json\")\n",
    "config_text = config.serializable_steps()\n",
    "\n",
    "config2 = PipelineConfig(\"test_config.yaml\")\n",
    "config2_text = config2.serializable_steps()\n",
    "assert config_text == config2_text, \"Config serialization mismatch\"\n",
    "\n",
    "config3 = PipelineConfig(\"test_config.json\")\n",
    "config3_text = config3.serializable_steps()\n",
    "assert config_text == config3_text, \"Config serialization mismatch\"\n",
    "\n",
    "runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "dataset_res, history, pipeline = runner.run(config, dataset=dataset_reg_1_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
