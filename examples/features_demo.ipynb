{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96298d26",
   "metadata": {},
   "source": [
    "# NIRS Features Simple Demo\n",
    "\n",
    "Simple test of Features class with single and multi-source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68633d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Single source data: (20, 100)\n",
      "Multi-source data: [(15, 80), (15, 120), (15, 90)]\n",
      "✓ Fake data created\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Setup and create fake data\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "from nirs4all.dataset.features import Features\n",
    "from nirs4all.dataset.dataset import SpectroDataset\n",
    "\n",
    "# Create fake spectral data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Single source: 20 samples, 100 wavelengths\n",
    "single_data = np.random.randn(20, 100) + 2.0\n",
    "print(f\"Single source data: {single_data.shape}\")\n",
    "\n",
    "# Multi-source: 3 sources with different wavelength counts\n",
    "source1 = np.random.randn(15, 80) + 1.5\n",
    "source2 = np.random.randn(15, 120) + 1.8\n",
    "source3 = np.random.randn(15, 90) + 2.2\n",
    "multi_data = [source1, source2, source3]\n",
    "print(f\"Multi-source data: {[s.shape for s in multi_data]}\")\n",
    "\n",
    "print(\"✓ Fake data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0bf700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features single - Sources: 1, Samples: 20, Features: 100\n",
      "Features representation: FeatureBlock with 1 sources and 20 samples\n",
      "Source 0: FeatureSource(shape=(20, 1, 100), dtype=float32, processing_ids=['raw'], mean=2.045084238052368, variance=0.9765757918357849)\n",
      "\n",
      "Dataset single - Sources: 1, Multi-source: False\n",
      "=== SpectroDataset Summary ===\n",
      "\n",
      "📊 Features: 20 samples, 1 source(s)\n",
      "Features: 100, processings: 1\n",
      "Processing IDs: ['raw']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test single source\n",
    "features_single = Features()\n",
    "dataset_single = SpectroDataset()\n",
    "\n",
    "# Add single source data to Features\n",
    "features_single.add_samples(single_data)\n",
    "print(f\"Features single - Sources: {len(features_single.sources)}, Samples: {features_single.num_samples}, Features: {features_single.num_features}\")\n",
    "print(f\"Features representation: {features_single}\")\n",
    "\n",
    "# Add single source data to Dataset\n",
    "dataset_single.add_samples(single_data)\n",
    "print(f\"\\nDataset single - Sources: {dataset_single.n_sources}, Multi-source: {dataset_single.is_multi_source()}\")\n",
    "dataset_single.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b4bc333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features multi - Sources: 3, Samples: 15, Features: [80, 120, 90]\n",
      "Features representation: FeatureBlock with 3 sources and 15 samples\n",
      "Source 0: FeatureSource(shape=(15, 1, 80), dtype=float32, processing_ids=['raw'], mean=1.481632113456726, variance=0.9760969877243042)\n",
      "Source 1: FeatureSource(shape=(15, 1, 120), dtype=float32, processing_ids=['raw'], mean=1.7777127027511597, variance=1.0189955234527588)\n",
      "Source 2: FeatureSource(shape=(15, 1, 90), dtype=float32, processing_ids=['raw'], mean=2.1727027893066406, variance=0.9904318451881409)\n",
      "\n",
      "Dataset multi - Sources: 3, Multi-source: True\n",
      "=== SpectroDataset Summary ===\n",
      "\n",
      "📊 Features: 15 samples, 3 source(s)\n",
      "Features: [80, 120, 90], processings: [1, 1, 1]\n",
      "Processing IDs: [['raw'], ['raw'], ['raw']]\n",
      "\n",
      "Source 0: (15, 1, 80) - 80 features\n",
      "Source 1: (15, 1, 120) - 120 features\n",
      "Source 2: (15, 1, 90) - 90 features\n",
      "shape: (15, 8)\n",
      "┌─────┬────────┬────────┬───────────┬───────┬────────┬─────────────┬──────────────┐\n",
      "│ row ┆ sample ┆ origin ┆ partition ┆ group ┆ branch ┆ processings ┆ augmentation │\n",
      "│ --- ┆ ---    ┆ ---    ┆ ---       ┆ ---   ┆ ---    ┆ ---         ┆ ---          │\n",
      "│ i32 ┆ i32    ┆ i32    ┆ cat       ┆ i8    ┆ i8     ┆ str         ┆ cat          │\n",
      "╞═════╪════════╪════════╪═══════════╪═══════╪════════╪═════════════╪══════════════╡\n",
      "│ 0   ┆ 0      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 1   ┆ 1      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 2   ┆ 2      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 3   ┆ 3      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 4   ┆ 4      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ …   ┆ …      ┆ …      ┆ …         ┆ …     ┆ …      ┆ …           ┆ …            │\n",
      "│ 10  ┆ 10     ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 11  ┆ 11     ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 12  ┆ 12     ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 13  ┆ 13     ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 14  ┆ 14     ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "└─────┴────────┴────────┴───────────┴───────┴────────┴─────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Test multi-source\n",
    "features_multi = Features()\n",
    "dataset_multi = SpectroDataset()\n",
    "\n",
    "# Add multi-source data to Features\n",
    "features_multi.add_samples(multi_data)\n",
    "print(f\"Features multi - Sources: {len(features_multi.sources)}, Samples: {features_multi.num_samples}, Features: {features_multi.num_features}\")\n",
    "print(f\"Features representation: {features_multi}\")\n",
    "\n",
    "# Add multi-source data to Dataset\n",
    "dataset_multi.add_samples(multi_data)\n",
    "print(f\"\\nDataset multi - Sources: {dataset_multi.n_sources}, Multi-source: {dataset_multi.is_multi_source()}\")\n",
    "dataset_multi.print_summary()\n",
    "\n",
    "# Show individual source info\n",
    "for i, source in enumerate(features_multi.sources):\n",
    "    print(f\"Source {i}: {source._array.shape} - {source.num_features} features\")\n",
    "\n",
    "print(dataset_multi._indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ddac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing add_features for single source ===\n",
      "✓ Successfully added new processings to single source\n",
      "Dataset sources: 1\n",
      "Processing IDs: ['raw', 'savgol', 'msc']\n",
      "Number of processings: 3\n",
      "Source array shape: (20, 3, 100) (samples, processings, features)\n",
      "Source processing IDs: ['raw', 'savgol', 'msc']\n",
      "\n",
      "Updated dataset summary:\n",
      "=== SpectroDataset Summary ===\n",
      "\n",
      "📊 Features: 20 samples, 1 source(s)\n",
      "Features: 100, processings: 3\n",
      "Processing IDs: ['raw', 'savgol', 'msc']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test add_features for single source\n",
    "print(\"=== Testing add_features for single source ===\")\n",
    "\n",
    "# Create new processed versions of single source data\n",
    "savgol_data = single_data + np.random.randn(*single_data.shape) * 0.1  # Simulated savgol filtering\n",
    "msc_data = single_data * 0.9 + 0.05  # Simulated MSC correction\n",
    "\n",
    "# Test adding features to existing single source dataset\n",
    "try:\n",
    "    dataset_single.add_features([savgol_data, msc_data], [\"savgol\", \"msc\"])\n",
    "    print(\"✓ Successfully added new processings to single source\")\n",
    "    print(f\"Dataset sources: {dataset_single.n_sources}\")\n",
    "    print(f\"Processing IDs: {dataset_single._features.preprocessing_str}\")\n",
    "    print(f\"Number of processings: {dataset_single._features.num_processings}\")\n",
    "\n",
    "    # Check the source details\n",
    "    source = dataset_single._features.sources[0]\n",
    "    print(f\"Source array shape: {source._array.shape} (samples, processings, features)\")\n",
    "    print(f\"Source processing IDs: {source._processing_ids}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error adding features to single source: {e}\")\n",
    "\n",
    "print(f\"\\nUpdated dataset summary:\")\n",
    "dataset_single.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4877833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing add_features for multi-source ===\n",
      "✓ Successfully added new processings to multi-source\n",
      "Dataset sources: 3\n",
      "Processing IDs: [['raw', 'detrend', 'normalize'], ['raw', 'detrend', 'normalize'], ['raw', 'detrend', 'normalize']]\n",
      "Number of processings: [3, 3, 3]\n",
      "Source 0 array shape: (15, 3, 80) (samples, processings, features)\n",
      "Source 0 processing IDs: ['raw', 'detrend', 'normalize']\n",
      "Source 1 array shape: (15, 3, 120) (samples, processings, features)\n",
      "Source 1 processing IDs: ['raw', 'detrend', 'normalize']\n",
      "Source 2 array shape: (15, 3, 90) (samples, processings, features)\n",
      "Source 2 processing IDs: ['raw', 'detrend', 'normalize']\n",
      "\n",
      "Updated multi-source dataset summary:\n",
      "=== SpectroDataset Summary ===\n",
      "\n",
      "📊 Features: 15 samples, 3 source(s)\n",
      "Features: [80, 120, 90], processings: [3, 3, 3]\n",
      "Processing IDs: [['raw', 'detrend', 'normalize'], ['raw', 'detrend', 'normalize'], ['raw', 'detrend', 'normalize']]\n",
      "\n",
      "\n",
      "Data comparison for source 0:\n",
      "Raw data mean: 1.482\n",
      "Detrend data mean: 1.480\n",
      "Normalize data mean: 1.650\n"
     ]
    }
   ],
   "source": [
    "# Test add_features for multi-source\n",
    "print(\"=== Testing add_features for multi-source ===\")\n",
    "\n",
    "# Create new processed versions for each source\n",
    "processed_source1 = [\n",
    "    source1 + np.random.randn(*source1.shape) * 0.05,  # Simulated detrend\n",
    "    source1 * 1.1 + 0.02  # Simulated normalization\n",
    "]\n",
    "processed_source2 = [\n",
    "    source2 + np.random.randn(*source2.shape) * 0.08,  # Simulated detrend\n",
    "    source2 * 0.95 - 0.01  # Simulated normalization\n",
    "]\n",
    "processed_source3 = [\n",
    "    source3 + np.random.randn(*source3.shape) * 0.06,  # Simulated detrend\n",
    "    source3 * 1.05 + 0.03  # Simulated normalization\n",
    "]\n",
    "\n",
    "multi_processed_data = [processed_source1, processed_source2, processed_source3]\n",
    "processing_names = [\"detrend\", \"normalize\"]\n",
    "\n",
    "# Test adding features to existing multi-source dataset\n",
    "try:\n",
    "    dataset_multi.add_features(multi_processed_data, processing_names)\n",
    "    print(\"✓ Successfully added new processings to multi-source\")\n",
    "    print(f\"Dataset sources: {dataset_multi.n_sources}\")\n",
    "    print(f\"Processing IDs: {dataset_multi._features.preprocessing_str}\")\n",
    "    print(f\"Number of processings: {dataset_multi._features.num_processings}\")\n",
    "\n",
    "    # Check each source details\n",
    "    for i, source in enumerate(dataset_multi._features.sources):\n",
    "        print(f\"Source {i} array shape: {source._array.shape} (samples, processings, features)\")\n",
    "        print(f\"Source {i} processing IDs: {source._processing_ids}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error adding features to multi-source: {e}\")\n",
    "\n",
    "print(f\"\\nUpdated multi-source dataset summary:\")\n",
    "dataset_multi.print_summary()\n",
    "\n",
    "# Compare original vs processed data for one source\n",
    "print(f\"\\nData comparison for source 0:\")\n",
    "s0 = dataset_multi._features.sources[0]\n",
    "print(f\"Raw data mean: {np.mean(s0._array[:, 0, :]):.3f}\")  # First processing (raw)\n",
    "print(f\"Detrend data mean: {np.mean(s0._array[:, 1, :]):.3f}\")  # Second processing (detrend)\n",
    "print(f\"Normalize data mean: {np.mean(s0._array[:, 2, :]):.3f}\")  # Third processing (normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing new update_features API ===\n",
      "Initial: ['raw']\n",
      "After add_features: ['raw', 'savgol', 'msc']\n",
      "After replace_features: ['normalized', 'savgol', 'msc']\n",
      "After update_features: ['normalized', 'savgol', 'msc_v2', 'detrend', 'baseline']\n",
      "Final shape: (10, 5, 50) (samples, processings, features)\n",
      "✓ New API working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test the new update_features functionality\n",
    "print(\"=== Testing new update_features API ===\")\n",
    "\n",
    "# Create a fresh feature source for testing\n",
    "from nirs4all.dataset.feature_source import FeatureSource\n",
    "test_source = FeatureSource()\n",
    "\n",
    "# Add initial data\n",
    "initial_data = np.random.randn(10, 50) + 1.0\n",
    "test_source.add_samples(initial_data)\n",
    "print(f\"Initial: {test_source._processing_ids}\")\n",
    "\n",
    "# Test 1: Add new features using simplified add_features\n",
    "new_data1 = initial_data + np.random.randn(*initial_data.shape) * 0.1\n",
    "new_data2 = initial_data * 0.9\n",
    "test_source.update_features([\"\", \"\"], [new_data1, new_data2], [\"savgol\", \"msc\"])\n",
    "print(f\"After add_features: {test_source._processing_ids}\")\n",
    "\n",
    "# Test 2: Replace features using simplified replace_features\n",
    "replacement_data = initial_data * 1.1 + 0.05\n",
    "test_source.update_features([\"raw\"], [replacement_data], [\"normalized\"])\n",
    "print(f\"After replace_features: {test_source._processing_ids}\")\n",
    "\n",
    "# Test 3: Mixed add/replace using update_features\n",
    "mixed_data1 = initial_data + 0.1  # New processing\n",
    "mixed_data2 = initial_data * 0.8  # Replace existing\n",
    "mixed_data3 = initial_data - 0.05 # New processing\n",
    "\n",
    "test_source.update_features(\n",
    "    [\"\", \"msc\", \"\"],  # \"\" = add new, \"msc\" = replace existing\n",
    "    [mixed_data1, mixed_data2, mixed_data3],\n",
    "    [\"detrend\", \"msc_v2\", \"baseline\"]\n",
    ")\n",
    "print(f\"After update_features: {test_source._processing_ids}\")\n",
    "print(f\"Final shape: {test_source._array.shape} (samples, processings, features)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
