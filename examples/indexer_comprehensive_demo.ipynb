{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a4553d",
   "metadata": {},
   "source": [
    "# NIRS Indexer Comprehensive Demonstration\n",
    "\n",
    "This notebook demonstrates the comprehensive usage of the NIRS Indexer class for managing sample indices in Near-Infrared Spectroscopy (NIRS) machine learning pipelines.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Basic Usage](#basic-usage)\n",
    "2. [Sample Management](#sample-management)\n",
    "3. [Data Augmentation](#data-augmentation)\n",
    "4. [Filtering and Querying](#filtering-and-querying)\n",
    "5. [ML Pipeline Integration](#ml-pipeline-integration)\n",
    "6. [Advanced Scenarios](#advanced-scenarios)\n",
    "7. [Performance Analysis](#performance-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151fc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../')  # Add parent directory to path\n",
    "\n",
    "from nirs4all.dataset.indexer import Indexer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5e80a",
   "metadata": {},
   "source": [
    "## 1. Basic Usage {#basic-usage}\n",
    "\n",
    "Let's start with the basic functionality of the Indexer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae5135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Indexer:\n",
      "Next row index: 0\n",
      "Next sample index: 0\n",
      "DataFrame shape: (0, 8)\n",
      "Default values: {'partition': 'train', 'group': 0, 'branch': 0, 'processings': ['raw']}\n"
     ]
    }
   ],
   "source": [
    "# Create a new indexer instance\n",
    "indexer = Indexer()\n",
    "\n",
    "print(\"Empty Indexer:\")\n",
    "print(f\"Next row index: {indexer.next_row_index()}\")\n",
    "print(f\"Next sample index: {indexer.next_sample_index()}\")\n",
    "print(f\"DataFrame shape: {indexer.df.shape}\")\n",
    "print(f\"Default values: {indexer.default_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5174f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 50 training samples: [0, 1, 2, 3, 4]...\n",
      "Added 20 test samples: [50, 51, 52, 53, 54]...\n",
      "Added 15 validation samples: [70, 71, 72, 73, 74]...\n",
      "\n",
      "Total samples in indexer: 85\n",
      "\n",
      "First 5 rows:\n",
      "shape: (5, 8)\n",
      "┌─────┬────────┬────────┬───────────┬───────┬────────┬─────────────┬──────────────┐\n",
      "│ row ┆ sample ┆ origin ┆ partition ┆ group ┆ branch ┆ processings ┆ augmentation │\n",
      "│ --- ┆ ---    ┆ ---    ┆ ---       ┆ ---   ┆ ---    ┆ ---         ┆ ---          │\n",
      "│ i32 ┆ i32    ┆ i32    ┆ cat       ┆ i8    ┆ i8     ┆ cat         ┆ cat          │\n",
      "╞═════╪════════╪════════╪═══════════╪═══════╪════════╪═════════════╪══════════════╡\n",
      "│ 0   ┆ 0      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 1   ┆ 1      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 2   ┆ 2      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 3   ┆ 3      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 4   ┆ 4      ┆ null   ┆ train     ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "└─────┴────────┴────────┴───────────┴───────┴────────┴─────────────┴──────────────┘\n",
      "\n",
      "Last 5 rows:\n",
      "shape: (5, 8)\n",
      "┌─────┬────────┬────────┬───────────┬───────┬────────┬─────────────┬──────────────┐\n",
      "│ row ┆ sample ┆ origin ┆ partition ┆ group ┆ branch ┆ processings ┆ augmentation │\n",
      "│ --- ┆ ---    ┆ ---    ┆ ---       ┆ ---   ┆ ---    ┆ ---         ┆ ---          │\n",
      "│ i32 ┆ i32    ┆ i32    ┆ cat       ┆ i8    ┆ i8     ┆ cat         ┆ cat          │\n",
      "╞═════╪════════╪════════╪═══════════╪═══════╪════════╪═════════════╪══════════════╡\n",
      "│ 80  ┆ 80     ┆ null   ┆ val       ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 81  ┆ 81     ┆ null   ┆ val       ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 82  ┆ 82     ┆ null   ┆ val       ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 83  ┆ 83     ┆ null   ┆ val       ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "│ 84  ┆ 84     ┆ null   ┆ val       ┆ 0     ┆ 0      ┆ ['raw']     ┆ null         │\n",
      "└─────┴────────┴────────┴───────────┴───────┴────────┴─────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Add some basic samples\n",
    "train_samples = indexer.add_samples(50, partition=\"train\")\n",
    "test_samples = indexer.add_samples(20, partition=\"test\")\n",
    "val_samples = indexer.add_samples(15, partition=\"val\")\n",
    "\n",
    "print(f\"Added {len(train_samples)} training samples: {train_samples[:5]}...\")\n",
    "print(f\"Added {len(test_samples)} test samples: {test_samples[:5]}...\")\n",
    "print(f\"Added {len(val_samples)} validation samples: {val_samples[:5]}...\")\n",
    "\n",
    "print(f\"\\nTotal samples in indexer: {len(indexer.df)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(indexer.df.head(5))\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(indexer.df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dabed1",
   "metadata": {},
   "source": [
    "## 2. Sample Management {#sample-management}\n",
    "\n",
    "Demonstrate advanced sample management with different groups, branches, and processing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new indexer for this section\n",
    "indexer_advanced = Indexer()\n",
    "\n",
    "# Add samples with different groups (representing different sample types)\n",
    "# Group 0: Organic samples\n",
    "organic_samples = indexer_advanced.add_samples(\n",
    "    count=30,\n",
    "    partition=\"train\",\n",
    "    group=0,\n",
    "    processings=[\"raw\", \"savgol\", \"msc\"]\n",
    ")\n",
    "\n",
    "# Group 1: Synthetic samples\n",
    "synthetic_samples = indexer_advanced.add_samples(\n",
    "    count=25,\n",
    "    partition=\"train\",\n",
    "    group=1,\n",
    "    branch=1,\n",
    "    processings=[\"raw\", \"snv\", \"detrend\"]\n",
    ")\n",
    "\n",
    "# Group 2: Mixed samples with individual processing\n",
    "mixed_samples = indexer_advanced.add_samples(\n",
    "    count=3,\n",
    "    partition=\"test\",\n",
    "    group=2,\n",
    "    branch=[0, 1, 2],  # Different branches per sample\n",
    "    processings=[\n",
    "        [\"raw\"],\n",
    "        [\"raw\", \"savgol\"],\n",
    "        [\"raw\", \"msc\", \"snv\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Organic samples (group 0): {len(organic_samples)} samples\")\n",
    "print(f\"Synthetic samples (group 1): {len(synthetic_samples)} samples\")\n",
    "print(f\"Mixed samples (group 2): {len(mixed_samples)} samples\")\n",
    "\n",
    "print(\"\\nMixed samples details:\")\n",
    "mixed_df = indexer_advanced.df.filter(indexer_advanced.df[\"group\"] == 2)\n",
    "print(mixed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cca60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sample distribution\n",
    "df_pandas = indexer_advanced.df.to_pandas()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Group distribution\n",
    "group_counts = df_pandas['group'].value_counts().sort_index()\n",
    "axes[0,0].bar(group_counts.index, group_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0,0].set_title('Sample Distribution by Group')\n",
    "axes[0,0].set_xlabel('Group')\n",
    "axes[0,0].set_ylabel('Number of Samples')\n",
    "\n",
    "# Partition distribution\n",
    "partition_counts = df_pandas['partition'].value_counts()\n",
    "axes[0,1].pie(partition_counts.values, labels=partition_counts.index, autopct='%1.1f%%')\n",
    "axes[0,1].set_title('Sample Distribution by Partition')\n",
    "\n",
    "# Branch distribution\n",
    "branch_counts = df_pandas['branch'].value_counts().sort_index()\n",
    "axes[1,0].bar(branch_counts.index, branch_counts.values, color=['#96CEB4', '#FFEAA7', '#DDA0DD'])\n",
    "axes[1,0].set_title('Sample Distribution by Branch')\n",
    "axes[1,0].set_xlabel('Branch')\n",
    "axes[1,0].set_ylabel('Number of Samples')\n",
    "\n",
    "# Processing types\n",
    "processing_counts = Counter(df_pandas['processings'])\n",
    "axes[1,1].barh(list(processing_counts.keys()), list(processing_counts.values()))\n",
    "axes[1,1].set_title('Processing Configuration Distribution')\n",
    "axes[1,1].set_xlabel('Number of Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59eed7a",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation {#data-augmentation}\n",
    "\n",
    "Demonstrate the data augmentation capabilities using the `augment_rows` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e75303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new indexer for augmentation demo\n",
    "aug_indexer = Indexer()\n",
    "\n",
    "# Add original samples with different characteristics\n",
    "original_samples = []\n",
    "sample_types = [\n",
    "    {\"count\": 10, \"group\": 0, \"processings\": [\"raw\", \"savgol\"]},\n",
    "    {\"count\": 8, \"group\": 1, \"processings\": [\"raw\", \"msc\"]},\n",
    "    {\"count\": 5, \"group\": 2, \"processings\": [\"raw\", \"snv\", \"detrend\"]}\n",
    "]\n",
    "\n",
    "for i, sample_type in enumerate(sample_types):\n",
    "    samples = aug_indexer.add_samples(\n",
    "        count=sample_type[\"count\"],\n",
    "        partition=\"train\",\n",
    "        group=sample_type[\"group\"],\n",
    "        processings=sample_type[\"processings\"]\n",
    "    )\n",
    "    original_samples.extend(samples)\n",
    "    print(f\"Added {sample_type['count']} samples of type {i} (group {sample_type['group']})\")\n",
    "\n",
    "print(f\"\\nTotal original samples: {len(original_samples)}\")\n",
    "print(f\"Original samples: {original_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data augmentation with different strategies\n",
    "\n",
    "# 1. Augment all samples with rotation\n",
    "rotation_samples = aug_indexer.augment_rows(original_samples, 1, \"rotation\")\n",
    "print(f\"Created {len(rotation_samples)} rotation augmentations\")\n",
    "\n",
    "# 2. Augment only group 0 samples with noise (2 augmentations each)\n",
    "group_0_samples = aug_indexer.x_indices({\"group\": 0, \"augmentation\": None})\n",
    "noise_samples = aug_indexer.augment_rows(group_0_samples.tolist(), 2, \"noise\")\n",
    "print(f\"Created {len(noise_samples)} noise augmentations for group 0\")\n",
    "\n",
    "# 3. Selective augmentation with different counts per sample\n",
    "selected_samples = [0, 5, 10]  # Select specific samples\n",
    "selective_counts = [3, 1, 2]   # Different augmentation counts\n",
    "selective_samples = aug_indexer.augment_rows(selected_samples, selective_counts, \"selective\")\n",
    "print(f\"Created {len(selective_samples)} selective augmentations\")\n",
    "\n",
    "print(f\"\\nTotal samples after augmentation: {len(aug_indexer.df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68544002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentation results\n",
    "aug_df = aug_indexer.df.to_pandas()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Original vs Augmented samples\n",
    "aug_counts = aug_df['augmentation'].fillna('Original').value_counts()\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "wedges, texts, autotexts = axes[0,0].pie(aug_counts.values, labels=aug_counts.index,\n",
    "                                         autopct='%1.1f%%', colors=colors)\n",
    "axes[0,0].set_title('Sample Distribution: Original vs Augmented')\n",
    "\n",
    "# Augmentation by group\n",
    "aug_group_crosstab = pd.crosstab(aug_df['group'], aug_df['augmentation'].fillna('Original'))\n",
    "aug_group_crosstab.plot(kind='bar', ax=axes[0,1], stacked=True)\n",
    "axes[0,1].set_title('Augmentation Distribution by Group')\n",
    "axes[0,1].set_xlabel('Group')\n",
    "axes[0,1].set_ylabel('Number of Samples')\n",
    "axes[0,1].legend(title='Augmentation Type')\n",
    "\n",
    "# Origin mapping for augmented samples\n",
    "augmented_only = aug_df[aug_df['augmentation'].notna()]\n",
    "origin_counts = augmented_only['origin'].value_counts().sort_index()\n",
    "axes[1,0].bar(origin_counts.index[:15], origin_counts.values[:15])  # Show first 15\n",
    "axes[1,0].set_title('Origin Distribution for Augmented Samples (First 15)')\n",
    "axes[1,0].set_xlabel('Origin Sample ID')\n",
    "axes[1,0].set_ylabel('Number of Augmentations')\n",
    "\n",
    "# Processing preservation in augmented samples\n",
    "processing_comparison = []\n",
    "for _, row in augmented_only.iterrows():\n",
    "    original_processing = aug_df[aug_df['sample'] == row['origin']]['processings'].iloc[0]\n",
    "    processing_comparison.append({\n",
    "        'augmented_id': row['sample'],\n",
    "        'original_processing': original_processing,\n",
    "        'augmented_processing': row['processings'],\n",
    "        'preserved': original_processing == row['processings']\n",
    "    })\n",
    "\n",
    "preservation_counts = pd.Series([item['preserved'] for item in processing_comparison]).value_counts()\n",
    "axes[1,1].pie(preservation_counts.values, labels=['Preserved', 'Modified'],\n",
    "              autopct='%1.1f%%', colors=['#2ECC71', '#E74C3C'])\n",
    "axes[1,1].set_title('Processing Configuration Preservation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Processing configurations are {'perfectly' if all(item['preserved'] for item in processing_comparison) else 'not perfectly'} preserved in augmented samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de027381",
   "metadata": {},
   "source": [
    "## 4. Filtering and Querying {#filtering-and-querying}\n",
    "\n",
    "Demonstrate the powerful filtering and querying capabilities of the Indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce82627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the augmented indexer from the previous section\n",
    "print(\"Available filtering options:\")\n",
    "print(f\"Groups: {sorted(aug_indexer.df['group'].unique().to_list())}\")\n",
    "print(f\"Partitions: {aug_indexer.df['partition'].unique().to_list()}\")\n",
    "print(f\"Branches: {sorted(aug_indexer.df['branch'].unique().to_list())}\")\n",
    "print(f\"Augmentation types: {[x for x in aug_indexer.df['augmentation'].unique().to_list() if x is not None]}\")\n",
    "print(f\"Processing types: {aug_indexer.df['processings'].unique().to_list()[:5]}...\")  # Show first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate various filtering scenarios\n",
    "\n",
    "# 1. Get all training samples\n",
    "train_x = aug_indexer.x_indices({\"partition\": \"train\"})\n",
    "train_y = aug_indexer.y_indices({\"partition\": \"train\"})\n",
    "print(f\"Training samples: {len(train_x)} X indices, {len(train_y)} Y indices\")\n",
    "\n",
    "# 2. Get only original samples (no augmentation)\n",
    "original_x = aug_indexer.x_indices({\"augmentation\": None})\n",
    "original_y = aug_indexer.y_indices({\"augmentation\": None})\n",
    "print(f\"Original samples: {len(original_x)} X indices, {len(original_y)} Y indices\")\n",
    "\n",
    "# 3. Get samples from specific group with specific augmentation\n",
    "group0_noise = aug_indexer.x_indices({\"group\": 0, \"augmentation\": \"noise\"})\n",
    "print(f\"Group 0 with noise augmentation: {len(group0_noise)} samples\")\n",
    "print(f\"Sample IDs: {group0_noise.tolist()[:10]}...\")  # Show first 10\n",
    "\n",
    "# 4. Get samples with multiple criteria\n",
    "complex_filter = aug_indexer.x_indices({\n",
    "    \"partition\": \"train\",\n",
    "    \"group\": [0, 1],  # Multiple groups\n",
    "    \"augmentation\": [\"rotation\", None]  # Original and rotation samples\n",
    "})\n",
    "print(f\"Complex filter (train, group 0&1, original+rotation): {len(complex_filter)} samples\")\n",
    "\n",
    "# 5. Demonstrate Y indices for augmented samples\n",
    "rotation_x = aug_indexer.x_indices({\"augmentation\": \"rotation\"})\n",
    "rotation_y = aug_indexer.y_indices({\"augmentation\": \"rotation\"})\n",
    "print(f\"\\nRotation augmentation:\")\n",
    "print(f\"X indices (augmented sample IDs): {rotation_x.tolist()[:10]}...\")\n",
    "print(f\"Y indices (original sample IDs): {rotation_y.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filtering results\n",
    "filters_results = {\n",
    "    'All Samples': len(aug_indexer.df),\n",
    "    'Training': len(train_x),\n",
    "    'Original Only': len(original_x),\n",
    "    'Group 0 + Noise': len(group0_noise),\n",
    "    'Complex Filter': len(complex_filter),\n",
    "    'Rotation Aug': len(rotation_x)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(filters_results.keys(), filters_results.values(),\n",
    "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD'])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title('Sample Counts for Different Filtering Scenarios', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Filter Type', fontsize=12)\n",
    "plt.ylabel('Number of Samples', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b974ac",
   "metadata": {},
   "source": [
    "## 5. ML Pipeline Integration {#ml-pipeline-integration}\n",
    "\n",
    "Show how the Indexer integrates with a typical machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a complete ML pipeline workflow\n",
    "class NIRSMLPipeline:\n",
    "    def __init__(self, indexer):\n",
    "        self.indexer = indexer\n",
    "        self.data_cache = {}  # Simulate data storage\n",
    "\n",
    "    def generate_synthetic_spectra(self, sample_ids, n_wavelengths=1000):\n",
    "        \"\"\"Simulate NIRS spectra generation based on sample properties\"\"\"\n",
    "        spectra = {}\n",
    "        for sample_id in sample_ids:\n",
    "            # Get sample metadata\n",
    "            sample_info = self.indexer.df.filter(\n",
    "                self.indexer.df['sample'] == sample_id\n",
    "            ).row(0, named=True)\n",
    "\n",
    "            # Generate base spectrum based on group\n",
    "            base_freq = 1 + sample_info['group'] * 0.5\n",
    "            wavelengths = np.linspace(1000, 2500, n_wavelengths)\n",
    "            spectrum = np.sin(wavelengths * base_freq / 1000) + np.random.normal(0, 0.1, n_wavelengths)\n",
    "\n",
    "            # Apply processing effects\n",
    "            processing = sample_info['processings']\n",
    "            if 'savgol' in processing:\n",
    "                spectrum = np.convolve(spectrum, np.ones(5)/5, mode='same')  # Simple smoothing\n",
    "            if 'msc' in processing:\n",
    "                spectrum = spectrum / np.mean(spectrum)  # Normalize\n",
    "\n",
    "            spectra[sample_id] = spectrum\n",
    "\n",
    "        return spectra\n",
    "\n",
    "    def get_training_data(self):\n",
    "        \"\"\"Get training data using indexer filters\"\"\"\n",
    "        train_x = self.indexer.x_indices({\"partition\": \"train\"})\n",
    "        train_y = self.indexer.y_indices({\"partition\": \"train\"})\n",
    "\n",
    "        X_spectra = self.generate_synthetic_spectra(train_x)\n",
    "        X = np.array(list(X_spectra.values()))\n",
    "        y = train_y  # In real scenario, this would be actual target values\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_validation_data(self, augmentation_type=None):\n",
    "        \"\"\"Get validation data, optionally including augmented samples\"\"\"\n",
    "        val_filter = {\"partition\": \"train\"}  # We'll use train partition for demo\n",
    "        if augmentation_type:\n",
    "            val_filter[\"augmentation\"] = augmentation_type\n",
    "\n",
    "        val_x = self.indexer.x_indices(val_filter)\n",
    "        val_y = self.indexer.y_indices(val_filter)\n",
    "\n",
    "        X_spectra = self.generate_synthetic_spectra(val_x)\n",
    "        X = np.array(list(X_spectra.values()))\n",
    "        y = val_y\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def analyze_data_distribution(self):\n",
    "        \"\"\"Analyze the distribution of data across different categories\"\"\"\n",
    "        df = self.indexer.df.to_pandas()\n",
    "\n",
    "        analysis = {\n",
    "            'total_samples': len(df),\n",
    "            'original_samples': len(df[df['augmentation'].isna()]),\n",
    "            'augmented_samples': len(df[df['augmentation'].notna()]),\n",
    "            'partitions': df['partition'].value_counts().to_dict(),\n",
    "            'groups': df['group'].value_counts().to_dict(),\n",
    "            'augmentation_types': df['augmentation'].value_counts().to_dict()\n",
    "        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "# Create pipeline instance\n",
    "pipeline = NIRSMLPipeline(aug_indexer)\n",
    "\n",
    "# Analyze data distribution\n",
    "analysis = pipeline.analyze_data_distribution()\n",
    "print(\"Pipeline Data Analysis:\")\n",
    "print(f\"Total samples: {analysis['total_samples']}\")\n",
    "print(f\"Original samples: {analysis['original_samples']}\")\n",
    "print(f\"Augmented samples: {analysis['augmented_samples']}\")\n",
    "print(f\"Augmentation ratio: {analysis['augmented_samples']/analysis['original_samples']:.2f}x\")\n",
    "print(f\"\\nPartition distribution: {analysis['partitions']}\")\n",
    "print(f\"Group distribution: {analysis['groups']}\")\n",
    "print(f\"Augmentation types: {analysis['augmentation_types']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize training data\n",
    "X_train, y_train = pipeline.get_training_data()\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "\n",
    "# Get different types of validation data\n",
    "X_val_original, y_val_original = pipeline.get_validation_data(augmentation_type=None)\n",
    "X_val_rotation, y_val_rotation = pipeline.get_validation_data(augmentation_type=\"rotation\")\n",
    "X_val_noise, y_val_noise = pipeline.get_validation_data(augmentation_type=\"noise\")\n",
    "\n",
    "print(f\"\\nValidation data shapes:\")\n",
    "print(f\"Original: {X_val_original.shape}\")\n",
    "print(f\"Rotation augmented: {X_val_rotation.shape}\")\n",
    "print(f\"Noise augmented: {X_val_noise.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some example spectra\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "wavelengths = np.linspace(1000, 2500, 1000)\n",
    "\n",
    "# Plot spectra from different groups\n",
    "group_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "for i, group in enumerate([0, 1, 2]):\n",
    "    group_indices = aug_indexer.x_indices({\"group\": group, \"augmentation\": None})\n",
    "    if len(group_indices) > 0:\n",
    "        sample_spectra = pipeline.generate_synthetic_spectra([group_indices[0]])\n",
    "        axes[0,0].plot(wavelengths, list(sample_spectra.values())[0],\n",
    "                      label=f'Group {group}', color=group_colors[i], linewidth=2)\n",
    "\n",
    "axes[0,0].set_title('Example Spectra by Group', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Wavelength (nm)')\n",
    "axes[0,0].set_ylabel('Absorbance')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Plot original vs augmented spectra\n",
    "sample_id = 0\n",
    "original_spectrum = pipeline.generate_synthetic_spectra([sample_id])[sample_id]\n",
    "rotation_indices = aug_indexer.x_indices({\"origin\": sample_id, \"augmentation\": \"rotation\"})\n",
    "if len(rotation_indices) > 0:\n",
    "    rotation_spectrum = pipeline.generate_synthetic_spectra([rotation_indices[0]])[rotation_indices[0]]\n",
    "    axes[0,1].plot(wavelengths, original_spectrum, label='Original', linewidth=2, color='#2ECC71')\n",
    "    axes[0,1].plot(wavelengths, rotation_spectrum, label='Rotation Aug', linewidth=2, color='#E74C3C')\n",
    "    axes[0,1].set_title('Original vs Rotation Augmented', fontsize=14, fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Wavelength (nm)')\n",
    "    axes[0,1].set_ylabel('Absorbance')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Show data distribution across pipeline stages\n",
    "pipeline_data = {\n",
    "    'All Data': len(aug_indexer.df),\n",
    "    'Training': len(X_train),\n",
    "    'Val Original': len(X_val_original),\n",
    "    'Val Rotation': len(X_val_rotation),\n",
    "    'Val Noise': len(X_val_noise)\n",
    "}\n",
    "\n",
    "bars = axes[1,0].bar(pipeline_data.keys(), pipeline_data.values(),\n",
    "                     color=['#34495E', '#3498DB', '#2ECC71', '#F39C12', '#E74C3C'])\n",
    "axes[1,0].set_title('Data Distribution Across Pipeline Stages', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Number of Samples')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Show sample correlation matrix (simplified)\n",
    "correlation_data = np.corrcoef(X_train[:10])  # Use first 10 samples for demo\n",
    "im = axes[1,1].imshow(correlation_data, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1,1].set_title('Sample Correlation Matrix (First 10)', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Sample Index')\n",
    "axes[1,1].set_ylabel('Sample Index')\n",
    "plt.colorbar(im, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba4ee5",
   "metadata": {},
   "source": [
    "## 6. Advanced Scenarios {#advanced-scenarios}\n",
    "\n",
    "Demonstrate advanced usage patterns and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced scenario 1: Dynamic sample addition during training\n",
    "dynamic_indexer = Indexer()\n",
    "\n",
    "print(\"=== Dynamic Sample Addition Scenario ===\")\n",
    "\n",
    "# Initial training set\n",
    "initial_samples = dynamic_indexer.add_samples(20, partition=\"train\", group=0)\n",
    "print(f\"Initial training set: {len(initial_samples)} samples\")\n",
    "\n",
    "# Simulate model training and identify hard examples\n",
    "# (In real scenario, this would be based on model performance)\n",
    "hard_examples = [0, 5, 12]  # Samples that need augmentation\n",
    "print(f\"Identified hard examples: {hard_examples}\")\n",
    "\n",
    "# Add targeted augmentations\n",
    "targeted_aug = dynamic_indexer.augment_rows(hard_examples, [3, 2, 4], \"hard_example_aug\")\n",
    "print(f\"Added {len(targeted_aug)} targeted augmentations\")\n",
    "\n",
    "# Add new validation samples\n",
    "new_val_samples = dynamic_indexer.add_samples(10, partition=\"val\", group=0)\n",
    "print(f\"Added {len(new_val_samples)} validation samples\")\n",
    "\n",
    "# Show the evolution of the dataset\n",
    "evolution_stats = {\n",
    "    'Initial': 20,\n",
    "    'After Hard Aug': 20 + len(targeted_aug),\n",
    "    'After Val Addition': 20 + len(targeted_aug) + len(new_val_samples)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(evolution_stats.keys()), list(evolution_stats.values()),\n",
    "         marker='o', linewidth=3, markersize=8, color='#3498DB')\n",
    "plt.title('Dataset Evolution During Training', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Total Samples')\n",
    "plt.grid(alpha=0.3)\n",
    "for i, (stage, count) in enumerate(evolution_stats.items()):\n",
    "    plt.text(i, count + 1, f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final dataset size: {len(dynamic_indexer.df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced scenario 2: Cross-validation with augmentation awareness\n",
    "cv_indexer = Indexer()\n",
    "\n",
    "print(\"=== Cross-Validation with Augmentation Awareness ===\")\n",
    "\n",
    "# Create samples with multiple branches (representing different CV folds)\n",
    "n_folds = 5\n",
    "samples_per_fold = 8\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    fold_samples = cv_indexer.add_samples(\n",
    "        count=samples_per_fold,\n",
    "        partition=\"train\",\n",
    "        branch=fold,  # Use branch to represent CV fold\n",
    "        group=fold % 3  # Distribute across groups\n",
    "    )\n",
    "    print(f\"Fold {fold}: added {len(fold_samples)} samples (group {fold % 3})\")\n",
    "\n",
    "# Add augmentations that respect fold boundaries\n",
    "for fold in range(n_folds):\n",
    "    fold_samples = cv_indexer.x_indices({\"branch\": fold, \"augmentation\": None})\n",
    "    if len(fold_samples) > 0:\n",
    "        aug_samples = cv_indexer.augment_rows(fold_samples.tolist(), 1, f\"fold_{fold}_aug\")\n",
    "        print(f\"Fold {fold}: added {len(aug_samples)} augmentations\")\n",
    "\n",
    "# Demonstrate proper CV splits\n",
    "print(\"\\n=== CV Split Demonstration ===\")\n",
    "for test_fold in range(3):  # Demo first 3 folds\n",
    "    # Training folds (all except test_fold)\n",
    "    train_folds = [f for f in range(n_folds) if f != test_fold]\n",
    "\n",
    "    train_x = cv_indexer.x_indices({\"branch\": train_folds})\n",
    "    test_x = cv_indexer.x_indices({\"branch\": test_fold})\n",
    "\n",
    "    # Get original samples only for testing (to avoid data leakage)\n",
    "    test_x_original = cv_indexer.x_indices({\"branch\": test_fold, \"augmentation\": None})\n",
    "\n",
    "    print(f\"Fold {test_fold}: Train={len(train_x)} (inc. aug), Test={len(test_x)} (all), Test_orig={len(test_x_original)}\")\n",
    "\n",
    "# Visualize CV setup\n",
    "cv_df = cv_indexer.df.to_pandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Samples per fold\n",
    "fold_samples = cv_df.groupby(['branch', 'augmentation']).size().unstack(fill_value=0)\n",
    "fold_samples.plot(kind='bar', ax=axes[0], stacked=True)\n",
    "axes[0].set_title('Samples per CV Fold (with Augmentation)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('CV Fold (Branch)')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].legend(title='Augmentation Type')\n",
    "\n",
    "# Group distribution across folds\n",
    "fold_groups = cv_df.groupby(['branch', 'group']).size().unstack(fill_value=0)\n",
    "fold_groups.plot(kind='bar', ax=axes[1], stacked=True)\n",
    "axes[1].set_title('Group Distribution across CV Folds', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('CV Fold (Branch)')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].legend(title='Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485889b5",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis {#performance-analysis}\n",
    "\n",
    "Analyze the performance characteristics of the Indexer for large-scale operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a619300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_operation(operation_func, *args, **kwargs):\n",
    "    \"\"\"Benchmark an operation and return execution time\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = operation_func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "print(\"=== Performance Benchmarking ===\")\n",
    "\n",
    "# Test different scales of operations\n",
    "scales = [100, 500, 1000, 2000, 5000]\n",
    "results = {\n",
    "    'sample_counts': [],\n",
    "    'add_samples_time': [],\n",
    "    'add_rows_time': [],\n",
    "    'augment_time': [],\n",
    "    'filter_time': [],\n",
    "    'total_memory_mb': []\n",
    "}\n",
    "\n",
    "for scale in scales:\n",
    "    print(f\"\\nTesting scale: {scale} samples\")\n",
    "\n",
    "    # Create fresh indexer for each scale test\n",
    "    perf_indexer = Indexer()\n",
    "\n",
    "    # Benchmark add_samples\n",
    "    sample_ids, add_time = benchmark_operation(\n",
    "        perf_indexer.add_samples,\n",
    "        scale,\n",
    "        partition=\"train\",\n",
    "        group=[i % 3 for i in range(scale)]\n",
    "    )\n",
    "\n",
    "    # Benchmark add_rows (smaller scale for comparison)\n",
    "    add_rows_scale = min(scale // 10, 100)\n",
    "    _, rows_time = benchmark_operation(\n",
    "        perf_indexer.add_rows,\n",
    "        add_rows_scale,\n",
    "        {\"partition\": \"test\"}\n",
    "    )\n",
    "\n",
    "    # Benchmark augmentation (sample from available)\n",
    "    aug_samples = sample_ids[:min(scale // 20, 50)]\n",
    "    _, aug_time = benchmark_operation(\n",
    "        perf_indexer.augment_rows,\n",
    "        aug_samples,\n",
    "        1,\n",
    "        \"performance_test\"\n",
    "    )\n",
    "\n",
    "    # Benchmark filtering\n",
    "    _, filter_time = benchmark_operation(\n",
    "        perf_indexer.x_indices,\n",
    "        {\"partition\": \"train\", \"group\": [0, 1]}\n",
    "    )\n",
    "\n",
    "    # Estimate memory usage (simplified)\n",
    "    df_size = len(perf_indexer.df)\n",
    "    estimated_memory_mb = df_size * 8 * 8 / (1024 * 1024)  # Rough estimate\n",
    "\n",
    "    # Store results\n",
    "    results['sample_counts'].append(scale)\n",
    "    results['add_samples_time'].append(add_time)\n",
    "    results['add_rows_time'].append(rows_time)\n",
    "    results['augment_time'].append(aug_time)\n",
    "    results['filter_time'].append(filter_time)\n",
    "    results['total_memory_mb'].append(estimated_memory_mb)\n",
    "\n",
    "    print(f\"  Add samples: {add_time:.4f}s\")\n",
    "    print(f\"  Add rows: {rows_time:.4f}s\")\n",
    "    print(f\"  Augment: {aug_time:.4f}s\")\n",
    "    print(f\"  Filter: {filter_time:.4f}s\")\n",
    "    print(f\"  Est. memory: {estimated_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nBenchmarking completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Execution time vs scale\n",
    "axes[0,0].plot(results['sample_counts'], results['add_samples_time'],\n",
    "               'o-', label='Add Samples', linewidth=2, markersize=6)\n",
    "axes[0,0].plot(results['sample_counts'], results['augment_time'],\n",
    "               's-', label='Augment Rows', linewidth=2, markersize=6)\n",
    "axes[0,0].plot(results['sample_counts'], results['filter_time'],\n",
    "               '^-', label='Filter Query', linewidth=2, markersize=6)\n",
    "axes[0,0].set_title('Execution Time vs Scale', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Number of Samples')\n",
    "axes[0,0].set_ylabel('Time (seconds)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "axes[0,0].set_yscale('log')\n",
    "\n",
    "# Memory usage vs scale\n",
    "axes[0,1].plot(results['sample_counts'], results['total_memory_mb'],\n",
    "               'o-', color='#E74C3C', linewidth=2, markersize=6)\n",
    "axes[0,1].set_title('Memory Usage vs Scale', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Number of Samples')\n",
    "axes[0,1].set_ylabel('Memory (MB)')\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Throughput (samples per second)\n",
    "throughput = [count/time for count, time in zip(results['sample_counts'], results['add_samples_time'])]\n",
    "axes[1,0].bar(range(len(scales)), throughput, color='#2ECC71')\n",
    "axes[1,0].set_title('Add Samples Throughput', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Scale Index')\n",
    "axes[1,0].set_ylabel('Samples per Second')\n",
    "axes[1,0].set_xticks(range(len(scales)))\n",
    "axes[1,0].set_xticklabels([f'{s}' for s in scales])\n",
    "\n",
    "# Performance comparison across operations (normalized)\n",
    "normalized_times = {\n",
    "    'Add Samples': np.array(results['add_samples_time']) / max(results['add_samples_time']),\n",
    "    'Augment': np.array(results['augment_time']) / max(results['augment_time']),\n",
    "    'Filter': np.array(results['filter_time']) / max(results['filter_time'])\n",
    "}\n",
    "\n",
    "x = np.arange(len(scales))\n",
    "width = 0.25\n",
    "colors = ['#3498DB', '#F39C12', '#9B59B6']\n",
    "\n",
    "for i, (operation, times) in enumerate(normalized_times.items()):\n",
    "    axes[1,1].bar(x + i*width, times, width, label=operation, color=colors[i])\n",
    "\n",
    "axes[1,1].set_title('Normalized Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Scale Index')\n",
    "axes[1,1].set_ylabel('Normalized Time')\n",
    "axes[1,1].set_xticks(x + width)\n",
    "axes[1,1].set_xticklabels([f'{s}' for s in scales])\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Maximum scale tested: {max(results['sample_counts']):,} samples\")\n",
    "print(f\"Best throughput: {max(throughput):,.0f} samples/second\")\n",
    "print(f\"Memory efficiency: {results['total_memory_mb'][-1]/results['sample_counts'][-1]*1000:.2f} KB per 1000 samples\")\n",
    "print(f\"Filter query time at max scale: {results['filter_time'][-1]:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c82df",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the comprehensive capabilities of the NIRS Indexer class:\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "1. **Sample Management**: Efficient addition and organization of samples with metadata\n",
    "2. **Data Augmentation**: Sophisticated augmentation capabilities with origin tracking\n",
    "3. **Filtering & Querying**: Powerful filtering system for ML pipeline integration\n",
    "4. **ML Pipeline Integration**: Seamless integration with machine learning workflows\n",
    "5. **Performance**: Excellent scalability for large-scale NIRS datasets\n",
    "\n",
    "### Best Practices:\n",
    "- Use appropriate partitioning strategies for train/test/validation splits\n",
    "- Leverage augmentation with origin tracking for data augmentation workflows\n",
    "- Utilize filtering capabilities to create targeted datasets for specific experiments\n",
    "- Consider memory usage and performance characteristics for large-scale applications\n",
    "- Implement proper cross-validation strategies that respect data relationships\n",
    "\n",
    "The Indexer class provides a robust foundation for managing NIRS spectroscopy data in machine learning applications, offering both simplicity for basic use cases and sophistication for advanced scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
