{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f8ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet files to analyze\n",
    "filenames = [\n",
    "    # \"digestibility_custom2\",\n",
    "    # \"digestibility_custom3\",\n",
    "    # \"digestibility_custom5\",\n",
    "    \"digestibility_0_8\",\n",
    "    # \"hardness_custom2\",\n",
    "    # \"hardness_custom4\",\n",
    "    # \"hardness_0_8\",\n",
    "    # \"tannin_custom2\",\n",
    "    # \"tannin_custom3\",\n",
    "    # \"tannin_0_8\"\n",
    "]\n",
    "\n",
    "# folders = [\"local\", \"_denis\", \"_wsl\"]\n",
    "\n",
    "# for file in filenames:\n",
    "#     input_files = [\n",
    "#         f\"workspace/{folders[0]}/{file}.meta.parquet\",\n",
    "#         f\"workspace/{folders[1]}/{file}.meta.parquet\",\n",
    "#         f\"workspace/{folders[2]}/{file}.meta.parquet\"\n",
    "#     ]\n",
    "#     output_file = f\"{file}.meta.parquet\"\n",
    "#     Predictions.merge_parquet_files(\n",
    "#         input_files=input_files,\n",
    "#         output_file=f\"workspace/{output_file}\",\n",
    "#         deduplicate=True  # Remove duplicate prediction IDs (default)\n",
    "#     )\n",
    "\n",
    "WORKSPACE_PATH = \"workspace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d215e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from nirs4all.data.predictions import Predictions\n",
    "from nirs4all.visualization.charts import ChartConfig\n",
    "from nirs4all.visualization.predictions import PredictionAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609d2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions_file(predictions_path: str, save_dir: str = \"charts\", exclude_models: list = None):\n",
    "    \"\"\"Analyze a single predictions parquet file with all visualizations.\n",
    "\n",
    "    Args:\n",
    "        predictions_path: Path to the predictions parquet file.\n",
    "        save_dir: Directory to save chart images.\n",
    "        exclude_models: List of model names to exclude from analysis.\n",
    "    \"\"\"\n",
    "    import polars as pl\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(predictions_path).exists():\n",
    "        print(f\"‚ö†Ô∏è  File not found: {predictions_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load predictions\n",
    "    predictions = Predictions()\n",
    "    predictions.load_from_file(predictions_path)\n",
    "\n",
    "    # Exclude specified models by filtering the internal DataFrame\n",
    "    if exclude_models:\n",
    "        original_count = len(predictions)\n",
    "        df = predictions._storage._df\n",
    "        predictions._storage._df = df.filter(~pl.col(\"model_name\").is_in(exclude_models))\n",
    "        excluded_count = original_count - len(predictions)\n",
    "        if excluded_count > 0:\n",
    "            print(f\"üö´ Excluded {excluded_count} predictions from models: {exclude_models}\")\n",
    "\n",
    "    file_name = Path(predictions_path).stem.replace('.meta', '')\n",
    "\n",
    "    # Create output directory for this file\n",
    "    output_dir = Path(save_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä Analyzing: {file_name}\")\n",
    "    print(f\"üìÅ Saving charts to: {output_dir}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    def save_figure(fig, name: str):\n",
    "        \"\"\"Helper to save figure with meaningful name.\"\"\"\n",
    "        filepath = output_dir / f\"{file_name}_{name}.png\"\n",
    "        fig.savefig(filepath, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üíæ Saved: {filepath.name}\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    # =========================================================================\n",
    "    # SYNTHETIC DATAVIZ: Summary statistics\n",
    "    # =========================================================================\n",
    "    print(f\"\\nüìà Summary Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Number of predictions: {len(predictions)}\")\n",
    "    print(f\"   ‚Ä¢ Models: {len(predictions.get_models())} unique ({', '.join(predictions.get_models()[:5])}{'...' if len(predictions.get_models()) > 5 else ''})\")\n",
    "    print(f\"   ‚Ä¢ Datasets: {predictions.get_datasets()}\")\n",
    "    print(f\"   ‚Ä¢ Partitions: {predictions.get_partitions()}\")\n",
    "    print(f\"   ‚Ä¢ Configs: {len(predictions.get_configs())} unique\")\n",
    "    print(f\"   ‚Ä¢ Folds: {predictions.get_folds()}\")\n",
    "\n",
    "    # Determine task type\n",
    "    sample_preds = predictions.filter_predictions(partition='test', load_arrays=False)\n",
    "    if sample_preds:\n",
    "        task_type = sample_preds[0].get('task_type', 'regression')\n",
    "        is_classification = 'classif' in task_type.lower()\n",
    "    else:\n",
    "        task_type = 'regression'\n",
    "        is_classification = False\n",
    "\n",
    "    print(f\"   ‚Ä¢ Task type: {task_type}\")\n",
    "\n",
    "    # Select metrics based on task type\n",
    "    if is_classification:\n",
    "        rank_metric = 'balanced_accuracy'\n",
    "        display_metrics = ['accuracy', 'balanced_accuracy', 'f1']\n",
    "    else:\n",
    "        rank_metric = 'rmse'\n",
    "        display_metrics = ['rmse', 'r2', 'mae']\n",
    "\n",
    "    # =========================================================================\n",
    "    # TOP MODELS\n",
    "    # =========================================================================\n",
    "    print(f\"\\nüèÜ Top 5 models by {rank_metric} (val):\")\n",
    "    top_models = predictions.top(n=5, rank_metric=rank_metric, rank_partition='val')\n",
    "    for idx, model in enumerate(top_models, 1):\n",
    "        summary = Predictions.pred_short_string(model, metrics=display_metrics, partition=['val', 'test'])\n",
    "        print(f\"   {idx}. {summary}\")\n",
    "\n",
    "    print(f\"\\nüèÜ Top 5 models by {rank_metric} (val) - Agg reps:\")\n",
    "    top_models = predictions.top(n=5, rank_metric=rank_metric, rank_partition='val', aggregate=\"ID\")\n",
    "    for idx, model in enumerate(top_models, 1):\n",
    "        summary = Predictions.pred_short_string(model, metrics=display_metrics, partition=['val', 'test'])\n",
    "        print(f\"   {idx}. {summary}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # VISUALIZATIONS\n",
    "    # =========================================================================\n",
    "    analyzer = PredictionAnalyzer(predictions, output_dir=None)\n",
    "\n",
    "    # --- Classification: Top 6 Confusion Matrices (val and test) ---\n",
    "    if is_classification:\n",
    "        print(f\"\\nüìä Confusion Matrices (Top 6 by {rank_metric}):\")\n",
    "\n",
    "        cm_config = ChartConfig(\n",
    "            # title_fontsize=24,        # Chart titles\n",
    "            # label_fontsize=14,        # Axis labels (X/Y labels)\n",
    "            # tick_fontsize=12,         # Tick labels on axes\n",
    "            # legend_fontsize=11,       # Legend text\n",
    "            annotation_fontsize=32    # Text inside charts (heatmap cells, etc.)\n",
    "        )\n",
    "\n",
    "        # Confusion matrices ranked by test, displayed on test, aggregated\n",
    "        fig_cm = analyzer.plot_confusion_matrix(\n",
    "            k=2,\n",
    "            rank_metric='balanced_accuracy',\n",
    "            rank_partition='test',\n",
    "            display_partition='test',\n",
    "            display_metric=['balanced_accuracy', 'accuracy'],\n",
    "            aggregate=\"ID\",\n",
    "            config=cm_config\n",
    "        )\n",
    "        plt.suptitle(f\"{file_name} - Confusion Matrices (ranked by test, display test) - Agg reps\", y=1.02)\n",
    "        save_figure(fig_cm, \"confusion_matrix_rank_test_display_test_agg\")\n",
    "\n",
    "        # Confusion matrices ranked by val, displayed on test, aggregated\n",
    "        fig_cm = analyzer.plot_confusion_matrix(\n",
    "            k=6,\n",
    "            rank_metric='balanced_accuracy',\n",
    "            rank_partition='val',\n",
    "            display_partition='test',\n",
    "            display_metric=['balanced_accuracy', 'accuracy'],\n",
    "            aggregate=\"ID\",\n",
    "            config=cm_config\n",
    "        )\n",
    "        plt.suptitle(f\"{file_name} - Confusion Matrices (ranked by val, display test) - Agg reps\", y=1.02)\n",
    "        save_figure(fig_cm, \"confusion_matrix_rank_val_display_test_agg\")\n",
    "\n",
    "        # Confusion matrices ranked by test, displayed on test\n",
    "        fig_cm = analyzer.plot_confusion_matrix(\n",
    "            k=2,\n",
    "            rank_metric='balanced_accuracy',\n",
    "            rank_partition='test',\n",
    "            display_partition='test',\n",
    "            display_metric=['balanced_accuracy', 'accuracy'],\n",
    "            config=cm_config\n",
    "        )\n",
    "        plt.suptitle(f\"{file_name} - Confusion Matrices (ranked by test, display test)\", y=1.02)\n",
    "        save_figure(fig_cm, \"confusion_matrix_rank_test_display_test\")\n",
    "\n",
    "        # Confusion matrices ranked by val, displayed on test\n",
    "        fig_cm = analyzer.plot_confusion_matrix(\n",
    "            k=6,\n",
    "            rank_metric='balanced_accuracy',\n",
    "            rank_partition='val',\n",
    "            display_partition='test',\n",
    "            display_metric=['balanced_accuracy', 'accuracy'],\n",
    "            config=cm_config\n",
    "        )\n",
    "        plt.suptitle(f\"{file_name} - Confusion Matrices (ranked by val, display test)\", y=1.02)\n",
    "        save_figure(fig_cm, \"confusion_matrix_rank_val_display_test\")\n",
    "\n",
    "    # --- Regression: Top 3 ---\n",
    "    else:\n",
    "        print(f\"\\nüìä Top 3 Model Comparison:\")\n",
    "        fig_top3 = analyzer.plot_top_k(\n",
    "            k=3,\n",
    "            rank_metric='rmse',\n",
    "            rank_partition='val',\n",
    "            aggregate=\"ID\"\n",
    "        )\n",
    "        plt.suptitle(f\"{file_name} - Top 3 Models (ranked by val) - Agg reps\", y=1.02)\n",
    "        save_figure(fig_top3, \"top3_models_rank_val_agg\")\n",
    "\n",
    "        fig_top3 = analyzer.plot_top_k(\n",
    "            k=3,\n",
    "            rank_metric='rmse',\n",
    "            rank_partition='val',\n",
    "        )\n",
    "        plt.suptitle(f\"{file_name} - Top 3 Models (ranked by val)\", y=1.02)\n",
    "        save_figure(fig_top3, \"top3_models_rank_val\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Heatmap ranked by VAL ---\n",
    "    hm_config = ChartConfig(\n",
    "        # title_fontsize=18,        # Chart titles\n",
    "        # label_fontsize=14,        # Axis labels (X/Y labels)\n",
    "        # tick_fontsize=12,         # Tick labels on axes\n",
    "        # legend_fontsize=11,       # Legend text\n",
    "        annotation_fontsize=15    # Text inside charts (heatmap cells, etc.)\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"\\nüó∫Ô∏è  Heatmaps:\")\n",
    "    fig_heatmap = analyzer.plot_heatmap(\n",
    "        x_var=\"partition\",\n",
    "        y_var=\"model_name\",\n",
    "        rank_metric=rank_metric,\n",
    "        display_metric=rank_metric,\n",
    "        show_counts=False,\n",
    "        rank_partition='val',\n",
    "        column_scale = True,\n",
    "        top_k=20,\n",
    "        sort_by='borda',\n",
    "        config=hm_config\n",
    "    )\n",
    "    plt.suptitle(f\"{file_name} - Heatmap (ranked by val)\", y=1.02)\n",
    "    save_figure(fig_heatmap, f\"heatmap_{rank_metric}_rank_val\")\n",
    "\n",
    "    # --- Heatmap ranked by TEST ---\n",
    "    fig_heatmap = analyzer.plot_heatmap(\n",
    "        x_var=\"partition\",\n",
    "        y_var=\"model_name\",\n",
    "        rank_metric=rank_metric,\n",
    "        display_metric=rank_metric,\n",
    "        show_counts=False,\n",
    "        rank_partition='test',\n",
    "        column_scale = True,\n",
    "        top_k=4,\n",
    "        sort_by_value=True,\n",
    "        config=hm_config\n",
    "    )\n",
    "    plt.suptitle(f\"{file_name} - Heatmap (ranked by test)\", y=1.02)\n",
    "    save_figure(fig_heatmap, f\"heatmap_{rank_metric}_rank_test\")\n",
    "\n",
    "    # --- Heatmap ranked by VAL - Aggregated ---\n",
    "    fig_heatmap = analyzer.plot_heatmap(\n",
    "        x_var=\"partition\",\n",
    "        y_var=\"model_name\",\n",
    "        rank_metric=rank_metric,\n",
    "        display_metric=rank_metric,\n",
    "        show_counts=False,\n",
    "        rank_partition='val',\n",
    "        aggregate=\"ID\",\n",
    "        column_scale = True,\n",
    "        top_k=20,\n",
    "        sort_by='borda',\n",
    "        config=hm_config\n",
    "    )\n",
    "    plt.suptitle(f\"{file_name} - Heatmap (ranked by val) - Agg reps\", y=1.02)\n",
    "    save_figure(fig_heatmap, f\"heatmap_{rank_metric}_rank_val_agg\")\n",
    "\n",
    "    # --- Heatmap ranked by TEST - Aggregated ---\n",
    "    fig_heatmap = analyzer.plot_heatmap(\n",
    "        x_var=\"partition\",\n",
    "        y_var=\"model_name\",\n",
    "        rank_metric=rank_metric,\n",
    "        display_metric=rank_metric,\n",
    "        show_counts=False,\n",
    "        rank_partition='test',\n",
    "        aggregate=\"ID\",\n",
    "        column_scale = True,\n",
    "        top_k=4,\n",
    "        sort_by_value=True,\n",
    "        config=hm_config\n",
    "    )\n",
    "    plt.suptitle(f\"{file_name} - Heatmap (ranked by test) - Agg reps\", y=1.02)\n",
    "    save_figure(fig_heatmap, f\"heatmap_{rank_metric}_rank_test_agg\")\n",
    "\n",
    "    # --- Candlestick plots ---\n",
    "    print(f\"\\nüìä Candlestick plots:\")\n",
    "    # fig_candlestick = analyzer.plot_candlestick(\n",
    "    #     variable=\"model_classname\",\n",
    "    #     display_metric=rank_metric,\n",
    "    #     display_partition='test'\n",
    "    # )\n",
    "    # plt.suptitle(f\"{file_name} - Score Distribution by Model\", y=1.02)\n",
    "    # save_figure(fig_candlestick, f\"candlestick_{rank_metric}_test\")\n",
    "\n",
    "    fig_candlestick = analyzer.plot_candlestick(\n",
    "        variable=\"model_classname\",\n",
    "        display_metric=rank_metric,\n",
    "        display_partition='test',\n",
    "        aggregate=\"ID\"\n",
    "    )\n",
    "    plt.suptitle(f\"{file_name} - Score Distribution by Model - Agg reps\", y=1.02)\n",
    "    save_figure(fig_candlestick, f\"candlestick_{rank_metric}_test_agg\")\n",
    "\n",
    "    # --- Histograms ---\n",
    "    # print(f\"\\nüìä Histograms:\")\n",
    "    # fig_histogram = analyzer.plot_histogram(\n",
    "    #     display_metric=rank_metric,\n",
    "    #     display_partition='test'\n",
    "    # )\n",
    "    # plt.suptitle(f\"{file_name} - Score Histogram\", y=1.02)\n",
    "    # save_figure(fig_histogram, f\"histogram_{rank_metric}_test\")\n",
    "\n",
    "    fig_histogram = analyzer.plot_histogram(\n",
    "        display_metric=rank_metric,\n",
    "        display_partition='test',\n",
    "        aggregate=\"ID\"\n",
    "    )\n",
    "    plt.suptitle(f\"{file_name} - Score Histogram - Agg reps\", y=1.02)\n",
    "    save_figure(fig_histogram, f\"histogram_{rank_metric}_test_agg\")\n",
    "\n",
    "    print(f\"\\n‚úÖ All charts saved to: {output_dir}\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecee50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Excluded 15 predictions from models: ['KernelPLS']\n",
      "\n",
      "================================================================================\n",
      " Analyzing: digestibility_0_8\n",
      " Saving charts to: charts\n",
      "================================================================================\n",
      "\n",
      " Summary Statistics:\n",
      "    Number of predictions: 3585\n",
      "    Models: 40 unique (opls_2_6, FCKPLS, ridge, opls_1_5, pls_14...)\n",
      "    Datasets: ['digestibility_0_8']\n",
      "    Partitions: ['val', 'test', 'train']\n",
      "    Configs: 54 unique\n",
      "    Folds: ['1', 'avg', '2', '0', 'w_avg']\n",
      "    Task type: regression\n",
      "\n",
      " Top 5 models by rmse (val):\n",
      "   1. LWPLS_5_components - mse [test: 218.0761], [val: 31.6120],  [val]: [rmse:14.7674], [r2:0.2427], [mae:11.8765] [test]: [rmse:14.7674], [r2:0.2427], [mae:11.8765], (fold: w_avg, id: 5, step: 8) - [3d0428a77da45101]\n",
      "   2. LWPLS_5_components - mse [test: 218.0648], [val: 31.8298],  [val]: [rmse:14.7670], [r2:0.2428], [mae:11.8784] [test]: [rmse:14.7670], [r2:0.2428], [mae:11.8784], (fold: avg, id: 4, step: 8) - [1484c8c70075df4d]\n",
      "   3. LWPLS_5_components - mse [test: 206.1026], [val: 32.1629],  [val]: [rmse:14.3563], [r2:0.2843], [mae:11.4103] [test]: [rmse:14.3563], [r2:0.2843], [mae:11.4103], (fold: w_avg, id: 5, step: 8) - [75801b37e4afe013]\n",
      "   4. LWPLS_5_components - mse [test: 206.0165], [val: 32.1889],  [val]: [rmse:14.3533], [r2:0.2846], [mae:11.4085] [test]: [rmse:14.3533], [r2:0.2846], [mae:11.4085], (fold: avg, id: 4, step: 8) - [0b071db66f8ecbe3]\n",
      "   5. LWPLS_5_components - mse [test: 233.8544], [val: 33.1338],  [val]: [rmse:15.2923], [r2:0.1880], [mae:12.1574] [test]: [rmse:15.2923], [r2:0.1880], [mae:12.1574], (fold: w_avg, id: 5, step: 8) - [4c9e38c305371b21]\n",
      "\n",
      " Top 5 models by rmse (val) - Agg reps:\n",
      "   1. LWPLS_5_components - mse [test: 218.0761], [val: 31.6120],  [val]: [rmse:14.7674], [r2:0.2427], [mae:11.8765] [test]: [rmse:14.7674], [r2:0.2427], [mae:11.8765], (fold: w_avg, id: 5, step: 8) - [3d0428a77da45101]\n",
      "   2. LWPLS_5_components - mse [test: 218.0648], [val: 31.8298],  [val]: [rmse:14.7670], [r2:0.2428], [mae:11.8784] [test]: [rmse:14.7670], [r2:0.2428], [mae:11.8784], (fold: avg, id: 4, step: 8) - [1484c8c70075df4d]\n",
      "   3. LWPLS_5_components - mse [test: 206.1026], [val: 32.1629],  [val]: [rmse:14.3563], [r2:0.2843], [mae:11.4103] [test]: [rmse:14.3563], [r2:0.2843], [mae:11.4103], (fold: w_avg, id: 5, step: 8) - [75801b37e4afe013]\n",
      "   4. LWPLS_5_components - mse [test: 206.0165], [val: 32.1889],  [val]: [rmse:14.3533], [r2:0.2846], [mae:11.4085] [test]: [rmse:14.3533], [r2:0.2846], [mae:11.4085], (fold: avg, id: 4, step: 8) - [0b071db66f8ecbe3]\n",
      "   5. LWPLS_5_components - mse [test: 233.8544], [val: 33.1338],  [val]: [rmse:15.2923], [r2:0.1880], [mae:12.1574] [test]: [rmse:15.2923], [r2:0.1880], [mae:12.1574], (fold: w_avg, id: 5, step: 8) - [4c9e38c305371b21]\n",
      "\n",
      " Top 3 Model Comparison:\n",
      " Saved: digestibility_0_8_top3_models_rank_val_agg.png\n",
      " Saved: digestibility_0_8_top3_models_rank_val.png\n",
      "\n",
      "  Heatmaps:\n",
      "Data wrangling time: 0.0190 seconds\n",
      "Matplotlib render time: 0.1084 seconds\n",
      " Saved: digestibility_0_8_heatmap_rmse_rank_val.png\n",
      "Data wrangling time: 0.0170 seconds\n",
      "Matplotlib render time: 0.0300 seconds\n",
      " Saved: digestibility_0_8_heatmap_rmse_rank_test.png\n",
      "Data wrangling time (with aggregation): 1.0654 seconds\n",
      "Matplotlib render time: 0.0521 seconds\n",
      " Saved: digestibility_0_8_heatmap_rmse_rank_val_agg.png\n",
      "Data wrangling time (with aggregation): 1.0660 seconds\n",
      "Matplotlib render time: 0.0280 seconds\n",
      " Saved: digestibility_0_8_heatmap_rmse_rank_test_agg.png\n",
      "\n",
      " Candlestick plots:\n",
      "Candlestick data wrangling time (with aggregation): 9.1118 seconds\n",
      "Matplotlib render time: 0.0465 seconds\n",
      " Saved: digestibility_0_8_candlestick_rmse_test_agg.png\n",
      "Histogram data wrangling time (with aggregation): 8.7673 seconds\n",
      "Matplotlib render time: 0.0350 seconds\n",
      " Saved: digestibility_0_8_histogram_rmse_test_agg.png\n",
      "\n",
      " All charts saved to: charts\n",
      "\n",
      "================================================================================\n",
      " Analysis complete! Processed 1/1 files.\n",
      " All charts saved to: charts/\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Process all parquet files\n",
    "results = {}\n",
    "\n",
    "# Output directory for all charts\n",
    "CHARTS_OUTPUT_DIR = \"charts\"\n",
    "Path(CHARTS_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Models to exclude from analysis (e.g., buggy models)\n",
    "EXCLUDE_MODELS = [\"KernelPLS\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    predictions_path = f\"{WORKSPACE_PATH}/{filename}.meta.parquet\"\n",
    "    result = analyze_predictions_file(\n",
    "        predictions_path,\n",
    "        save_dir=CHARTS_OUTPUT_DIR,\n",
    "        exclude_models=EXCLUDE_MODELS\n",
    "    )\n",
    "    if result is not None:\n",
    "        results[filename] = result\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Analysis complete! Processed {len(results)}/{len(filenames)} files.\")\n",
    "print(f\"üìÅ All charts saved to: {CHARTS_OUTPUT_DIR}/\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106eb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
