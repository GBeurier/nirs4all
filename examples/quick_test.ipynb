{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4280f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "import time\n",
    "from nirs4all.presets.ref_models import decon, nicon, customizable_nicon, nicon_classification\n",
    "from nirs4all.presets.preprocessings import decon_set, nicon_set\n",
    "from nirs4all.data_splitters import KennardStoneSplitter\n",
    "from nirs4all.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG, Gaussian as GS, Derivate as  Dv\n",
    "from nirs4all.transformations import Rotate_Translate as RT, Spline_X_Simplification as SXS, Random_X_Operation as RXO\n",
    "from nirs4all.transformations import CropTransformer\n",
    "from nirs4all.core.runner import ExperimentRunner\n",
    "from nirs4all.core.config import Config\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold, ShuffleSplit, GroupKFold, StratifiedShuffleSplit, BaseCrossValidator, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "model_sklearn = {\n",
    "    \"class\": \"sklearn.cross_decomposition.PLSRegression\",\n",
    "    \"model_params\": {\n",
    "        \"n_components\": 21,\n",
    "    }\n",
    "}\n",
    "    \n",
    "finetune_pls_experiment = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_components': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "bacon_train = {\"action\": \"train\", \"training_params\": {\"epochs\": 2000, \"batch_size\": 500, \"patience\": 200, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 400}}\n",
    "bacon_train_short = {\"action\": \"train\", \"training_params\": {\"epochs\": 10, \"batch_size\": 500, \"patience\": 20, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 40}}\n",
    "bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 10,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "full_bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 500,\n",
    "        \"patience\": 100,\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        \"nb_trials\": 150,\n",
    "        \"model_params\": {\n",
    "            'spatial_dropout': (float, 0.01, 0.5),\n",
    "            'filters1': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'kernel_size1': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides1': [1, 2, 3, 4, 5],\n",
    "            # 'activation1': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'dropout_rate': (float, 0.01, 0.5),\n",
    "            'filters2': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size2': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides2': [1, 2, 3, 4, 5],\n",
    "            'activation2': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'normalization_method1': ['BatchNormalization', 'LayerNormalization'],\n",
    "            'filters3': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size3': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides3': [1, 2, 3, 4, 5],\n",
    "            'activation3': ['relu', 'selu', 'elu', 'swish'],\n",
    "            # 'normalization_method2': ['BatchNormalization', 'LayerNormalization'],\n",
    "            # 'dense_units': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'dense_activation': ['relu', 'selu', 'elu', 'swish'],\n",
    "        },\n",
    "        # \"training_params\": {\n",
    "        #     \"batch_size\": [32, 64, 128, 256, 512],\n",
    "        #     \"cyclic_lr\": [True, False],\n",
    "        #     \"base_lr\": (float, 1e-6, 1e-2),\n",
    "        #     \"max_lr\": (float, 1e-3, 1e-1),\n",
    "        #     \"step_size\": (int, 500, 5000),\n",
    "        # },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "x_pipeline_full = [\n",
    "    RobustScaler(quantile_range=(0.05, 0.95)),\n",
    "    {\"samples\": [None, None, None, None, SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(3,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler(feature_range=(0, 0.9), clip=False)\n",
    "]\n",
    "\n",
    "\n",
    "bacon_finetune_classif = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 5,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "finetune_randomForestclassifier = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_estimators': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "x_pipeline_PLS = [\n",
    "    RobustScaler(),\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "            \n",
    "            \n",
    "x_pipeline = [\n",
    "    RobustScaler(), \n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    # bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipelineb = [\n",
    "    RobustScaler(), \n",
    "    {\"samples\": [RT(6)], \"balance\": True},\n",
    "    # {\"samples\": [None, RT]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    # {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "\n",
    "y_pipeline = MinMaxScaler()\n",
    "\n",
    "seed = 123459456\n",
    "\n",
    "# processing only\n",
    "config1 = Config(\"../sample_data/regression\", x_pipeline_full, y_pipeline, None, None, seed)\n",
    "## TRAINING\n",
    "# regression\n",
    "config2 = Config(\"../sample_data/regression\", x_pipeline, y_pipeline, nicon, bacon_train_short, seed)\n",
    "config3 = Config(\"../sample_data/regression\", x_pipeline_PLS, y_pipeline, PLSRegression(n_components=10), None, seed)\n",
    "# classification\n",
    "config4 = Config(\"../sample_data/classification\", x_pipeline, None, nicon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config4b = Config(\"../sample_data/binary\", x_pipelineb, None, nicon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config5 = Config(\"../sample_data/binary\", x_pipeline, None, nicon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}, \"verbose\":0}, seed*2)\n",
    "config6 = Config(\"../sample_data/classification\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "config7 = Config(\"../sample_data/binary\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "## FINETUNING\n",
    "# regression\n",
    "config8 = Config(\"../sample_data/regression\", x_pipeline, y_pipeline, nicon, bacon_finetune, seed)\n",
    "config9 = Config(\"../sample_data/regression\", x_pipeline, y_pipeline, model_sklearn, finetune_pls_experiment, seed)\n",
    "# classification\n",
    "config10 = Config(\"../sample_data/classification\", x_pipeline, None, nicon_classification, bacon_finetune_classif, seed*2)\n",
    "config10b = Config(\"../sample_data/binary\", x_pipeline, None, nicon_classification, bacon_finetune_classif, seed*2)\n",
    "config11 = Config(\"../sample_data/classification\", x_pipelineb, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "config11b = Config(\"../sample_data/binary\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "\n",
    "\n",
    "configs = [config1, config2, config3, config4, config4b, config5, config6, config7, config8, config9, config10, config10b, config11, config11b]\n",
    "# configs = [config10b, config11, config11b]\n",
    "# configs = [config3]\n",
    "config_names = [\"config1\", \"config2\", \"config3\", \"config4\", \"config4b\", \"config5\", \"config6\", \"config7\", \"config8\", \"config9\", \"config10\", \"config10b\", \"config11\", \"config11b\"]\n",
    "# for i, config in enumerate(configs):\n",
    "#     print(\"#\" * 20)\n",
    "#     print(f\"Config {i}: {config_names[i]}\")\n",
    "#     print(\"#\" * 20)\n",
    "#     start = time.time()\n",
    "#     runner = ExperimentRunner([config], resume_mode=\"restart\")\n",
    "#     datasets, predictions, scores, best_params = runner.run()\n",
    "#     end = time.time()\n",
    "#     print(f\"Time elapsed: {end-start} seconds\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15c4e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Type mismatch: <class 'int'> != <class 'str'>\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "config9\n",
      "Config and deserialized config are not equal\n",
      "OConfig: Config(dataset='../sample_data/regression', x_pipeline=[RobustScaler(), {'split': RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)}, MinMaxScaler()], y_pipeline=MinMaxScaler(), model={'class': 'sklearn.cross_decomposition.PLSRegression', 'model_params': {'n_components': 21}}, experiment={'action': 'finetune', 'finetune_params': {'model_params': {'n_components': ('int', 5, 20)}, 'training_params': {}, 'tuner': 'sklearn'}}, seed=123459456)\n",
      "--------------------\n",
      "SConfig: {'dataset': '../sample_data/regression', 'x_pipeline': [{'instance': 'sklearn.preprocessing._data.RobustScaler'}, {'split': {'instance': 'sklearn.model_selection._split.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'instance': 'sklearn.preprocessing._data.MinMaxScaler'}], 'y_pipeline': {'instance': 'sklearn.preprocessing._data.MinMaxScaler'}, 'model': {'class': 'sklearn.cross_decomposition.PLSRegression', 'model_params': {'n_components': 21}}, 'experiment': {'action': 'finetune', 'finetune_params': {'model_params': {'n_components': ['int', 5, 20]}, 'training_params': {}, 'tuner': 'sklearn'}}, 'seed': 123459456}\n",
      "--------------------\n",
      "DConfig: Config(dataset='../sample_data/regression', x_pipeline=[RobustScaler(), {'split': RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)}, MinMaxScaler()], y_pipeline=MinMaxScaler(), model=<class 'sklearn.cross_decomposition._pls.PLSRegression'>, experiment={'action': 'finetune', 'finetune_params': {'model_params': {'n_components': ['int', 5, 20]}, 'training_params': {}, 'tuner': 'sklearn'}}, seed=123459456)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Type mismatch: <class 'int'> != <class 'str'>\n",
      "--------------------------------------------------------------------------------\n",
      "config11\n",
      "Config and deserialized config are not equal\n",
      "OConfig: Config(dataset='../sample_data/classification', x_pipeline=[RobustScaler(), {'samples': [Rotate_Translate(apply_on=6)], 'balance': True}, {'split': RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)}, MinMaxScaler()], y_pipeline=None, model=<class 'sklearn.ensemble._forest.RandomForestClassifier'>, experiment={'action': 'finetune', 'task': 'classification', 'finetune_params': {'model_params': {'n_estimators': ('int', 5, 20)}, 'training_params': {}, 'tuner': 'sklearn'}}, seed=246918912)\n",
      "--------------------\n",
      "SConfig: {'dataset': '../sample_data/classification', 'x_pipeline': [{'instance': 'sklearn.preprocessing._data.RobustScaler'}, {'samples': [{'instance': 'nirs4all.transformations._random_augmentation.Rotate_Translate', 'params': {'apply_on': 6}}], 'balance': True}, {'split': {'instance': 'sklearn.model_selection._split.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'instance': 'sklearn.preprocessing._data.MinMaxScaler'}], 'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier'}, 'experiment': {'action': 'finetune', 'task': 'classification', 'finetune_params': {'model_params': {'n_estimators': ['int', 5, 20]}, 'training_params': {}, 'tuner': 'sklearn'}}, 'seed': 246918912}\n",
      "--------------------\n",
      "DConfig: Config(dataset='../sample_data/classification', x_pipeline=[RobustScaler(), {'samples': [Rotate_Translate(apply_on=6)], 'balance': True}, {'split': RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)}, MinMaxScaler()], y_pipeline=None, model=<class 'sklearn.ensemble._forest.RandomForestClassifier'>, experiment={'action': 'finetune', 'task': 'classification', 'finetune_params': {'model_params': {'n_estimators': ['int', 5, 20]}, 'training_params': {}, 'tuner': 'sklearn'}}, seed=246918912)\n",
      "--------------------------------------------------------------------------------\n",
      "config11b\n",
      "Config and deserialized config are not equal\n",
      "OConfig: Config(dataset='../sample_data/binary', x_pipeline=[RobustScaler(), {'split': RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)}, MinMaxScaler()], y_pipeline=None, model=<class 'sklearn.ensemble._forest.RandomForestClassifier'>, experiment={'action': 'finetune', 'task': 'classification', 'finetune_params': {'model_params': {'n_estimators': ('int', 5, 20)}, 'training_params': {}, 'tuner': 'sklearn'}}, seed=246918912)\n",
      "--------------------\n",
      "SConfig: {'dataset': '../sample_data/binary', 'x_pipeline': [{'instance': 'sklearn.preprocessing._data.RobustScaler'}, {'split': {'instance': 'sklearn.model_selection._split.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'instance': 'sklearn.preprocessing._data.MinMaxScaler'}], 'model': {'class': 'sklearn.ensemble._forest.RandomForestClassifier'}, 'experiment': {'action': 'finetune', 'task': 'classification', 'finetune_params': {'model_params': {'n_estimators': ['int', 5, 20]}, 'training_params': {}, 'tuner': 'sklearn'}}, 'seed': 246918912}\n",
      "--------------------\n",
      "DConfig: Config(dataset='../sample_data/binary', x_pipeline=[RobustScaler(), {'split': RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)}, MinMaxScaler()], y_pipeline=None, model=<class 'sklearn.ensemble._forest.RandomForestClassifier'>, experiment={'action': 'finetune', 'task': 'classification', 'finetune_params': {'model_params': {'n_estimators': ['int', 5, 20]}, 'training_params': {}, 'tuner': 'sklearn'}}, seed=246918912)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nirs4all.utils.serialization import _serialize_component, _deserialize_component\n",
    "for config, name in zip(configs, config_names):\n",
    "    # print(\" \")\n",
    "    # print(\"--\" * 20)\n",
    "    # print(\"#\" * 20)\n",
    "    # print(config)\n",
    "    # print(\"#\" * 20)\n",
    "    serialized_config = config.to_dict()\n",
    "    # print(serialized_config)\n",
    "    # print(\"#\" * 20)\n",
    "    deserialized_config = Config.from_dict(serialized_config)\n",
    "    # print(deserialized_config)\n",
    "    print(\"--\" * 40)\n",
    "    if str(config) != str(deserialized_config):\n",
    "        print(f\"{name}\")\n",
    "        print(\"Config and deserialized config are not equal\")\n",
    "        print(f\"OConfig: {config}\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"SConfig: {serialized_config}\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"DConfig: {deserialized_config}\")\n",
    "        \n",
    "    \n",
    "#     # start = time.time()\n",
    "#     # runner = ExperimentRunner(deserialized_config, resume_mode=\"restart\")\n",
    "#     # datasets, predictions, scores, best_params = runner.run()\n",
    "#     # end = time.time()\n",
    "#     # print(f\"Time elapsed: {end-start} seconds\")\n",
    "# import json\n",
    "# # json load ../nirs4all/presets/configs/fast_train.json\n",
    "# json_config = json.load(open(\"../nirs4all/presets/configs/fast_train.json\"))\n",
    "# print(json_config)\n",
    "# deserialized_config = Config.from_dict(json_config)\n",
    "# print(deserialized_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dad55f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "====================\n",
      "custom_obj(a=1, b=test, c=(0.25, 0))\n",
      "(0.25, 0)\n",
      "{'instance': '__main__.custom_obj', 'params': {'a': 1, 'b': 'test', 'c': [0.25, 0]}}\n",
      "custom_obj(a=1, b=test, c=(0.25, 0))\n",
      "====================\n",
      "<function test_func at 0x0000029E4EA43D00>\n",
      "{'function': '__main__.test_func'}\n",
      "<function test_func at 0x0000029E4EA43D00>\n",
      "====================\n",
      "====================\n",
      "RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)\n",
      "{'instance': 'sklearn.model_selection._split.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1}}\n",
      "RepeatedKFold(n_repeats=1, n_splits=3, random_state=None)\n",
      "====================\n",
      "Foo(coords=(1, 2))\n",
      "{'instance': '__main__.Foo', 'params': {'coords': [1, 2]}}\n",
      "Foo(coords=(1, 2))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import inspect\n",
    "from typing import Any, Union, Callable, Dict, Tuple\n",
    "from typing import Type\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold, ShuffleSplit, GroupKFold, StratifiedShuffleSplit, BaseCrossValidator, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from nirs4all.utils.serialization import _serialize_component, _deserialize_component\n",
    "\n",
    "\n",
    "print(\"=\"*20) \n",
    "class custom_obj:\n",
    "    def __init__(self, a, b: str, c = (0.0, 0.1)): \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"custom_obj(a={self.a}, b={self.b}, c={self.c})\"\n",
    "    def __str__(self):\n",
    "        return f\"custom_obj(a={self.a}, b={self.b}, c={self.c})\"\n",
    "        \n",
    "a = custom_obj(1, \"test\", (0.25,0))\n",
    "print(a)\n",
    "print(a.c)\n",
    "ser = _serialize_component(a)\n",
    "print(ser)\n",
    "deser = _deserialize_component(ser)\n",
    "print(deser)\n",
    "\n",
    "\n",
    "print(\"=\"*20)\n",
    "def test_func(a: int, b: str, c = (0.0, 0.1)):\n",
    "    return a + b + c[0] + c[1]\n",
    "\n",
    "ser_func = _serialize_component(test_func)\n",
    "print(test_func)\n",
    "print(ser_func)\n",
    "deser_func = _deserialize_component(ser_func)\n",
    "print(deser_func)\n",
    "\n",
    "\n",
    "print(\"=\"*20)\n",
    "obj = RepeatedKFold(n_splits=3, n_repeats=1)\n",
    "ser_obj = _serialize_component(obj)\n",
    "print(\"=\"*20)\n",
    "print(obj)\n",
    "print(ser_obj)\n",
    "deser_obj = _deserialize_component(ser_obj)\n",
    "print(deser_obj)\n",
    "\n",
    "\n",
    "print(\"=\"*20)\n",
    "class Foo:\n",
    "    def __init__(self, coords: Tuple[int, int]):\n",
    "        self.coords = coords\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Foo(coords={self.coords})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Foo(coords={self.coords})\"\n",
    "\n",
    "f = Foo((1, 2))\n",
    "print(f)\n",
    "blob = _serialize_component(f)\n",
    "print(blob)\n",
    "g = _deserialize_component(blob)\n",
    "print(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70ed1eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Config(dataset='../sample_data/regression', x_pipeline=['sklearn.preprocessing.RobustScaler', {'samples': ['nirs4all.transformations.Rotate_Translate', {'class': <class 'nirs4all.transformations._random_augmentation.Rotate_Translate'>, 'params': {'p_range': 3, 'y_factor': 5}}]}, {'split': {'class': <class 'sklearn.model_selection._split.RepeatedKFold'>, 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'features': [None, {'class': <class 'nirs4all.transformations._standard.Gaussian'>, 'params': {'order': 2, 'sigma': 2}}, 'nirs4all.transformations.SavitzkyGolay', 'nirs4all.transformations.StandardNormalVariate', 'nirs4all.transformations.Derivate', 'nirs4all.transformations.Haar']}, 'sklearn.preprocessing.MinMaxScaler'], y_pipeline='sklearn.preprocessing.MinMaxScaler', model=<class 'sklearn.cross_decomposition._pls.PLSRegression'>, experiment={'action': 'train'}, seed=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 16:23:41,735 - INFO - ================================================================================\n",
      "2025-05-16 16:23:41,735 - INFO - Running config: Config(dataset='../sample_data/regression', x_pipeline=['sklearn.preprocessing.RobustScaler', {'samples': ['nirs4all.transformations.Rotate_Translate', {'class': <class 'nirs4all.transformations._random_augmentation.Rotate_Translate'>, 'params': {'p_range': 3, 'y_factor': 5}}]}, {'split': {'class': <class 'sklearn.model_selection._split.RepeatedKFold'>, 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'features': [None, {'class': <class 'nirs4all.transformations._standard.Gaussian'>, 'params': {'order': 2, 'sigma': 2}}, 'nirs4all.transformations.SavitzkyGolay', 'nirs4all.transformations.StandardNormalVariate', 'nirs4all.transformations.Derivate', 'nirs4all.transformations.Haar']}, 'sklearn.preprocessing.MinMaxScaler'], y_pipeline='sklearn.preprocessing.MinMaxScaler', model=<class 'sklearn.cross_decomposition._pls.PLSRegression'>, experiment={'action': 'train'}, seed=None)\n",
      "2025-05-16 16:23:41,736 - INFO - ================================================================================\n",
      "2025-05-16 16:23:41,736 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Browsing ../sample_data/regression\n",
      "No train_group file found for ../sample_data/regression.\n",
      "No test_group file found for ../sample_data/regression.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 16:23:42,514 - INFO - Dataset(x_train:(130, 2151) - y_train:(130, 1), x_test:(59, 2151) - y_test:(59, 1))\n",
      "2025-05-16 16:23:42,515 - INFO - ### PROCESSING DATASET ###\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Class <class 'nirs4all.transformations._random_augmentation.Rotate_Translate'> not found: type object 'Rotate_Translate' has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:34\u001b[0m, in \u001b[0;36minstantiate_class\u001b[1;34m(class_name, params)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     components \u001b[38;5;241m=\u001b[39m \u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(components[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Rotate_Translate' has no attribute 'split'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[0;32m     10\u001b[0m runner \u001b[38;5;241m=\u001b[39m ExperimentRunner(config, resume_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m datasets, predictions, scores, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\runner.py:71\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning config: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, config)\n\u001b[1;32m---> 71\u001b[0m dataset_, preds_, scores_, best_params_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m datasets\u001b[38;5;241m.\u001b[39mappend(dataset_)\n\u001b[0;32m     74\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(preds_)\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\runner.py:31\u001b[0m, in \u001b[0;36mExperimentRunner._run_config\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(dataset)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### PROCESSING DATASET ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(dataset)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# len of unique classes for y_train merged with y_test\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:98\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m(dataset, x_pipeline, y_pipeline, logger, cache)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_pipeline\u001b[39m(\n\u001b[0;32m     80\u001b[0m     dataset: Dataset,\n\u001b[0;32m     81\u001b[0m     x_pipeline: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     cache: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     85\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    Run the pipeline on the dataset.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m        Dataset: The processed dataset.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mexec_step_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m exec_step_y(dataset, y_pipeline, fit_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:321\u001b[0m, in \u001b[0;36mexec_step_x\u001b[1;34m(dataset, step, logger, indent, fit_test, skip_test, cache)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# Sequential pipeline\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# logger.info(indent + \"Sequential pipeline\")\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m step:\n\u001b[1;32m--> 321\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mexec_step_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, TransformerMixin) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:256\u001b[0m, in \u001b[0;36mexec_step_x\u001b[1;34m(dataset, step, logger, indent, fit_test, skip_test, cache)\u001b[0m\n\u001b[0;32m    251\u001b[0m     augmented_dataset \u001b[38;5;241m=\u001b[39m exec_step_x(\n\u001b[0;32m    252\u001b[0m         augmented_dataset, st, logger, indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, fit_test, \u001b[38;5;28;01mTrue\u001b[39;00m, cache\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (aug_range, augmented_dataset\u001b[38;5;241m.\u001b[39mraw_x_train)\n\u001b[1;32m--> 256\u001b[0m results \u001b[38;5;241m=\u001b[39m [process_sample_transformer((i, st)) \u001b[38;5;28;01mfor\u001b[39;00m i, st \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_transformers)]\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m aug_range, aug_train_data \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    259\u001b[0m     train_data[aug_range, :, :, :] \u001b[38;5;241m=\u001b[39m aug_train_data\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:256\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m     augmented_dataset \u001b[38;5;241m=\u001b[39m exec_step_x(\n\u001b[0;32m    252\u001b[0m         augmented_dataset, st, logger, indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, fit_test, \u001b[38;5;28;01mTrue\u001b[39;00m, cache\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (aug_range, augmented_dataset\u001b[38;5;241m.\u001b[39mraw_x_train)\n\u001b[1;32m--> 256\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43mprocess_sample_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, st \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_transformers)]\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m aug_range, aug_train_data \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    259\u001b[0m     train_data[aug_range, :, :, :] \u001b[38;5;241m=\u001b[39m aug_train_data\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:251\u001b[0m, in \u001b[0;36mexec_step_x.<locals>.process_sample_transformer\u001b[1;34m(i_st)\u001b[0m\n\u001b[0;32m    249\u001b[0m aug_data \u001b[38;5;241m=\u001b[39m train_data[aug_range, :, :, :]\n\u001b[0;32m    250\u001b[0m augmented_dataset \u001b[38;5;241m=\u001b[39m Dataset(_x_train\u001b[38;5;241m=\u001b[39maug_data)\n\u001b[1;32m--> 251\u001b[0m augmented_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mexec_step_x\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (aug_range, augmented_dataset\u001b[38;5;241m.\u001b[39mraw_x_train)\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:311\u001b[0m, in \u001b[0;36mexec_step_x\u001b[1;34m(dataset, step, logger, indent, fit_test, skip_test, cache)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m step:\n\u001b[1;32m--> 311\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m \u001b[43mget_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_step_x(dataset, transformer, logger, indent, fit_test, skip_test, cache)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:72\u001b[0m, in \u001b[0;36mget_transformer\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instantiate_class(config, {})\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstantiate_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(config):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config()\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\core\\processor.py:42\u001b[0m, in \u001b[0;36minstantiate_class\u001b[1;34m(class_name, params)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m class_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Class <class 'nirs4all.transformations._random_augmentation.Rotate_Translate'> not found: type object 'Rotate_Translate' has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nirs4all.core.config import Config\n",
    "from nirs4all.core.runner import ExperimentRunner\n",
    "\n",
    "config = Config.from_json_file(\"../nirs4all/presets/configs/fast_train.json\")\n",
    "config.dataset = \"../sample_data/regression\"\n",
    "print(config)\n",
    "runner = ExperimentRunner(config, resume_mode=\"restart\")\n",
    "datasets, predictions, scores, best_params = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pipeline_full = [\n",
    "    RobustScaler(quantile_range=(0.05, 0.95)),\n",
    "    {\"samples\": [None, None, None, None, SXS(), RXO()]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(3,1), SG(), SNV(), Dv(), [GS(), SNV()], [GS(), GS()],[GS(), SG()],[SG(), SNV()], [GS(), Dv()], [SG(), Dv()]]},\n",
    "    MinMaxScaler(feature_range=(0, 0.9), clip=False)\n",
    "]\n",
    "\n",
    "print(x_pipeline_full)\n",
    "# save x_pipeline_full to pickle file\n",
    "import pickle\n",
    "with open(\"x_pipeline_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(x_pipeline_full, f)\n",
    "# load x_pipeline_full from pickle file\n",
    "with open(\"x_pipeline_full.pkl\", \"rb\") as f:\n",
    "    x_pipeline_full = pickle.load(f)\n",
    "print(x_pipeline_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc58e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"train\": {\n",
      "      \"X\": {\n",
      "        \"path\": \"/STnh6Foy.npy\"\n",
      "      },\n",
      "      \"Y\": {\n",
      "        \"to\": 51\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"data\": \"/2_mUp/B4/\"\n",
      "}\n",
      "{\n",
      "  \"data\": {\n",
      "    \"train\": {\n",
      "      \"X\": \"/0.csv\",\n",
      "      \"Y\": \"/0.csv\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"data\": {\n",
      "    \"train\": {\n",
      "      \"X\": \"/4g-e9MNQLYT1kObN.csv.gz\",\n",
      "      \"Y\": {\n",
      "        \"from\": 160\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"data\": {\n",
      "    \"train\": {\n",
      "      \"X\": \"/Id9-R/o.csv.gz\",\n",
      "      \"Y\": {\n",
      "        \"path\": \"/yMm/gqyT/hKdpY/N3/KRE/F.mY0/21unRqqI.csv\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"data\": {\n",
      "    \"path\": \"/JQ/MFW/\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"data\": {\n",
      "    \"path\": \"/0/\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"data\": {\n",
      "    \"path\": \"/P08nm/\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"data\": \"/-8b/iSUpby/n/F/wWx3C/ft5j/lH2ay/\"\n",
      "}\n",
      "{\n",
      "  \"data\": \"/jgY/HoUgaM/HwgqhpHC0-XfkLZbZ0aD_/t.7-u/rc/ss9kO5vcWHVaS.mWT/3D99/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from hypothesis_jsonschema import from_schema\n",
    "import json\n",
    "from nirs4all.data.schema import schema\n",
    "\n",
    "# Créez la stratégie une seule fois\n",
    "strategy = from_schema(schema)\n",
    "\n",
    "# Puis générez des exemples « concrets »\n",
    "for _ in range(10):\n",
    "    sample = strategy.example()\n",
    "    print(json.dumps(sample, ensure_ascii=False, indent=2))\n",
    "    print(\"=\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
