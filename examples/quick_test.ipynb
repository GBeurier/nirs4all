{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "import time\n",
    "from nirs4all.presets.ref_models import decon, nicon, customizable_nicon, nicon_classification\n",
    "from nirs4all.presets.preprocessings import decon_set, nicon_set\n",
    "from nirs4all.data_splitters import KennardStoneSplitter\n",
    "from nirs4all.transformations import StandardNormalVariate as SNV, SavitzkyGolay as SG, Gaussian as GS, Derivate as  Dv\n",
    "from nirs4all.transformations import Rotate_Translate as RT, Spline_X_Simplification as SXS, Random_X_Operation as RXO\n",
    "from nirs4all.transformations import CropTransformer\n",
    "from nirs4all.core.runner import ExperimentRunner\n",
    "from nirs4all.core.config import Config\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFold, ShuffleSplit, GroupKFold, StratifiedShuffleSplit, BaseCrossValidator, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "model_sklearn = {\n",
    "    \"class\": \"sklearn.cross_decomposition.PLSRegression\",\n",
    "    \"model_params\": {\n",
    "        \"n_components\": 21,\n",
    "    }\n",
    "}\n",
    "    \n",
    "finetune_pls_experiment = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_components': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "bacon_train = {\"action\": \"train\", \"training_params\": {\"epochs\": 2000, \"batch_size\": 500, \"patience\": 200, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 400}}\n",
    "bacon_train_short = {\"action\": \"train\", \"training_params\": {\"epochs\": 10, \"batch_size\": 500, \"patience\": 20, \"cyclic_lr\": True, \"base_lr\": 1e-6, \"max_lr\": 1e-3, \"step_size\": 40}}\n",
    "bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 10,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "full_bacon_finetune = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 500,\n",
    "        \"patience\": 100,\n",
    "    },\n",
    "    \"finetune_params\": {\n",
    "        \"nb_trials\": 150,\n",
    "        \"model_params\": {\n",
    "            'spatial_dropout': (float, 0.01, 0.5),\n",
    "            'filters1': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'kernel_size1': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides1': [1, 2, 3, 4, 5],\n",
    "            # 'activation1': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'dropout_rate': (float, 0.01, 0.5),\n",
    "            'filters2': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size2': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides2': [1, 2, 3, 4, 5],\n",
    "            'activation2': ['relu', 'selu', 'elu', 'swish'],\n",
    "            'normalization_method1': ['BatchNormalization', 'LayerNormalization'],\n",
    "            'filters3': [4, 8, 16, 32, 64, 128, 256],\n",
    "            # 'kernel_size3': [3, 5, 7, 9, 11, 13, 15],\n",
    "            # 'strides3': [1, 2, 3, 4, 5],\n",
    "            'activation3': ['relu', 'selu', 'elu', 'swish'],\n",
    "            # 'normalization_method2': ['BatchNormalization', 'LayerNormalization'],\n",
    "            # 'dense_units': [4, 8, 16, 32, 64, 128, 256],\n",
    "            'dense_activation': ['relu', 'selu', 'elu', 'swish'],\n",
    "        },\n",
    "        # \"training_params\": {\n",
    "        #     \"batch_size\": [32, 64, 128, 256, 512],\n",
    "        #     \"cyclic_lr\": [True, False],\n",
    "        #     \"base_lr\": (float, 1e-6, 1e-2),\n",
    "        #     \"max_lr\": (float, 1e-3, 1e-1),\n",
    "        #     \"step_size\": (int, 500, 5000),\n",
    "        # },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "x_pipeline_full = [\n",
    "    RobustScaler(),\n",
    "    {\"samples\": [None, None, None, None, SXS, RXO]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "\n",
    "bacon_finetune_classif = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        \"n_trials\": 5,\n",
    "        \"model_params\": {\n",
    "            \"filters_1\": [8, 16, 32, 64], \n",
    "            \"filters_2\": [8, 16, 32, 64], \n",
    "            \"filters_3\": [8, 16, 32, 64]\n",
    "        }\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 5,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "finetune_randomForestclassifier = {\n",
    "    \"action\": \"finetune\",\n",
    "    \"task\": \"classification\",\n",
    "    \"finetune_params\": {\n",
    "        'model_params': {\n",
    "            'n_estimators': ('int', 5, 20),\n",
    "        },\n",
    "        'training_params': {},\n",
    "        'tuner': 'sklearn'\n",
    "    }\n",
    "}\n",
    "\n",
    "x_pipeline_PLS = [\n",
    "    RobustScaler(),\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)},\n",
    "    {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "            \n",
    "            \n",
    "x_pipeline = [\n",
    "    RobustScaler(), \n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    # bacon_set(),\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "x_pipelineb = [\n",
    "    RobustScaler(), \n",
    "    {\"samples\": [RT(6)], \"balance\": True},\n",
    "    # {\"samples\": [None, RT]},\n",
    "    {\"split\": RepeatedKFold(n_splits=3, n_repeats=1)}, \n",
    "    # {\"features\": [None, GS(2,1), SG, SNV, Dv, [GS, SNV], [GS, GS],[GS, SG],[SG, SNV], [GS, Dv], [SG, Dv]]},\n",
    "    MinMaxScaler()\n",
    "]\n",
    "\n",
    "\n",
    "y_pipeline = MinMaxScaler()\n",
    "\n",
    "seed = 123459456\n",
    "\n",
    "# processing only\n",
    "config1 = Config(\"../sample_data/regression\", x_pipeline_full, y_pipeline, None, None, seed)\n",
    "## TRAINING\n",
    "# regression\n",
    "config2 = Config(\"../sample_data/regression\", x_pipeline, y_pipeline, nicon, bacon_train_short, seed)\n",
    "config3 = Config(\"../sample_data/regression\", x_pipeline_PLS, y_pipeline, PLSRegression(n_components=10), None, seed)\n",
    "# classification\n",
    "config4 = Config(\"../sample_data/classification\", x_pipeline, None, nicon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config4b = Config(\"../sample_data/binary\", x_pipelineb, None, nicon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":10, \"patience\": 100, \"verbose\":0}}, seed*2)\n",
    "config5 = Config(\"../sample_data/binary\", x_pipeline, None, nicon_classification, {\"task\":\"classification\", \"training_params\":{\"epochs\":5}, \"verbose\":0}, seed*2)\n",
    "config6 = Config(\"../sample_data/classification\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "config7 = Config(\"../sample_data/binary\", x_pipeline, None, RandomForestClassifier, {\"task\":\"classification\"}, seed*2)\n",
    "## FINETUNING\n",
    "# regression\n",
    "config8 = Config(\"../sample_data/regression\", x_pipeline, y_pipeline, nicon, bacon_finetune, seed)\n",
    "config9 = Config(\"../sample_data/regression\", x_pipeline, y_pipeline, model_sklearn, finetune_pls_experiment, seed)\n",
    "# classification\n",
    "config10 = Config(\"../sample_data/classification\", x_pipeline, None, nicon_classification, bacon_finetune_classif, seed*2)\n",
    "config10b = Config(\"../sample_data/binary\", x_pipeline, None, nicon_classification, bacon_finetune_classif, seed*2)\n",
    "config11 = Config(\"../sample_data/classification\", x_pipelineb, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "config11b = Config(\"../sample_data/binary\", x_pipeline, None, RandomForestClassifier, finetune_randomForestclassifier, seed*2)\n",
    "\n",
    "\n",
    "# configs = [config1, config2, config3, config4, config4b, config5, config6, config7, config8, config9, config10, config10b, config11, config11b]\n",
    "# configs = [config10b, config11, config11b]\n",
    "configs = [config3]\n",
    "config_names = [\"config1\", \"config2\", \"config3\", \"config4\", \"config4b\", \"config5\", \"config6\", \"config7\", \"config8\", \"config9\", \"config10\", \"config10b\", \"config11\", \"config11b\"]\n",
    "for i, config in enumerate(configs):\n",
    "    print(\"#\" * 20)\n",
    "    print(f\"Config {i}: {config_names[i]}\")\n",
    "    print(\"#\" * 20)\n",
    "    start = time.time()\n",
    "    runner = ExperimentRunner([config], resume_mode=\"restart\")\n",
    "    datasets, predictions, scores, best_params = runner.run()\n",
    "    end = time.time()\n",
    "    print(f\"Time elapsed: {end-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ed1eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:28:22,406 - INFO - ================================================================================\n",
      "2025-05-14 00:28:22,406 - INFO - Running config: Config(dataset='../sample_data/regression', x_pipeline=['sklearn.preprocessing.RobustScaler', {'samples': ['nirs4all.transformations.Rotate_Translate', {'class': 'nirs4all.transformations.Rotate_Translate', 'params': {'p_range': 3, 'y_factor': 5}}]}, {'split': {'class': 'sklearn.model_selection.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'features': [None, {'class': 'nirs4all.transformations.Gaussian', 'params': {'order': 2, 'sigma': 2}}, 'nirs4all.transformations.SavitzkyGolay', 'nirs4all.transformations.StandardNormalVariate', 'nirs4all.transformations.Derivate', 'nirs4all.transformations.Haar']}, 'sklearn.preprocessing.MinMaxScaler'], y_pipeline='sklearn.preprocessing.MinMaxScaler', model={'class': 'sklearn.cross_decomposition.PLSRegression', 'model_params': {'n_components': 21}}, experiment={'action': 'train'}, seed=None)\n",
      "2025-05-14 00:28:22,407 - INFO - ================================================================================\n",
      "2025-05-14 00:28:22,407 - INFO - ### LOADING DATASET ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Config(dataset='../sample_data/regression', x_pipeline=['sklearn.preprocessing.RobustScaler', {'samples': ['nirs4all.transformations.Rotate_Translate', {'class': 'nirs4all.transformations.Rotate_Translate', 'params': {'p_range': 3, 'y_factor': 5}}]}, {'split': {'class': 'sklearn.model_selection.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'features': [None, {'class': 'nirs4all.transformations.Gaussian', 'params': {'order': 2, 'sigma': 2}}, 'nirs4all.transformations.SavitzkyGolay', 'nirs4all.transformations.StandardNormalVariate', 'nirs4all.transformations.Derivate', 'nirs4all.transformations.Haar']}, 'sklearn.preprocessing.MinMaxScaler'], y_pipeline='sklearn.preprocessing.MinMaxScaler', model={'class': 'sklearn.cross_decomposition.PLSRegression', 'model_params': {'n_components': 21}}, experiment={'action': 'train'}, seed=None)\n",
      ">> Browsing ../sample_data/regression\n",
      "No train_group file found for ../sample_data/regression.\n",
      "No test_group file found for ../sample_data/regression.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:28:23,674 - INFO - Dataset(x_train:(130, 2151) - y_train:(130, 1), x_test:(59, 2151) - y_test:(59, 1))\n",
      "2025-05-14 00:28:23,674 - INFO - ### PROCESSING DATASET ###\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "d:\\Workspace\\ML\\NIRS\\nirs4all\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "2025-05-14 00:28:24,923 - INFO - Dataset(x_train:(260, 12906) - y_train:(260, 1), x_test:(59, 12906) - y_test:(59, 1))\n",
      "Folds size: 86-44, 87-43, 87-43\n",
      "2025-05-14 00:28:24,924 - INFO - ### PREPARING MODEL ###\n",
      "2025-05-14 00:28:24,927 - INFO - Running config > {'dataset': '../sample_data/regression', 'x_pipeline': ['sklearn.preprocessing.RobustScaler', {'samples': ['nirs4all.transformations.Rotate_Translate', {'class': 'nirs4all.transformations.Rotate_Translate', 'params': {'p_range': 3, 'y_factor': 5}}]}, {'split': {'class': 'sklearn.model_selection.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1}}}, {'features': [None, {'class': 'nirs4all.transformations.Gaussian', 'params': {'order': 2, 'sigma': 2}}, 'nirs4all.transformations.SavitzkyGolay', 'nirs4all.transformations.StandardNormalVariate', 'nirs4all.transformations.Derivate', 'nirs4all.transformations.Haar']}, 'sklearn.preprocessing.MinMaxScaler'], 'y_pipeline': 'sklearn.preprocessing.MinMaxScaler', 'model': {'class': 'sklearn.cross_decomposition.PLSRegression', 'model_params': {'n_components': 21}}, 'experiment': {'action': 'train', 'metrics': ['mse', 'mae'], 'task': 'regression'}, 'seed': None}\n",
      "2025-05-14 00:28:24,928 - INFO - Starting new experiment experiment_72ea00aa.\n",
      "2025-05-14 00:28:24,929 - INFO - Experiment prepared at results\\sample_dataregression\\PLSRegression\\experiment_72ea00aa\n",
      "2025-05-14 00:28:24,930 - INFO - Training the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Splitter method: sklearn.model_selection.RepeatedKFold\n",
      "Splitter params: {'n_splits': 3, 'n_repeats': 1}\n",
      "=========================\n",
      "Loaded splitter class: <class 'sklearn.model_selection._split.RepeatedKFold'>\n",
      "Using framework: sklearn\n",
      "Training fold 1, with shapes: (172, 12906) (172, 1) (88, 12906) (88, 1)\n",
      "Training fold 2, with shapes: (174, 12906) (174, 1) (86, 12906) (86, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:28:25,100 - INFO - Saved model to results\\sample_dataregression\\PLSRegression\\experiment_72ea00aa\n",
      "2025-05-14 00:28:25,118 - INFO - Evaluation Metrics fold_0: {'mse': 399.68833504057557, 'mae': 16.08065514648115}\n",
      "2025-05-14 00:28:25,120 - INFO - Evaluation Metrics fold_1: {'mse': 407.0605190522869, 'mae': 16.487930924229502}\n",
      "2025-05-14 00:28:25,121 - INFO - Evaluation Metrics fold_2: {'mse': 427.2875456387469, 'mae': 16.258452634014162}\n",
      "2025-05-14 00:28:25,122 - INFO - Evaluation Metrics mean: {'mse': 398.55595761149385, 'mae': 15.878677111684443}\n",
      "2025-05-14 00:28:25,124 - INFO - Evaluation Metrics best: {'mse': 399.68833504057557, 'mae': 16.08065514648115}\n",
      "2025-05-14 00:28:25,125 - INFO - Evaluation Metrics weighted: {'mse': 398.2234778021958, 'mae': 15.878048109731408}\n",
      "2025-05-14 00:28:25,126 - INFO - Metrics saved to results\\sample_dataregression\\PLSRegression\\experiment_72ea00aa\\metrics.json\n",
      "2025-05-14 00:28:25,129 - INFO - Predictions saved to results\\sample_dataregression\\PLSRegression\\experiment_72ea00aa\\predictions.csv\n",
      "2025-05-14 00:28:25,141 - INFO - Updated experiments at results\\sample_dataregression\\PLSRegression\\experiments.json\n",
      "2025-05-14 00:28:25,153 - INFO - Updated experiments at results\\sample_dataregression\\experiments.json\n",
      "2025-05-14 00:28:25,154 - INFO - Updated experiments at results\\sample_dataregression\\PLSRegression\\experiments.json and results\\sample_dataregression\\experiments.json\n",
      "2025-05-14 00:28:25,155 - INFO - All experiments completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3, with shapes: (174, 12906) (174, 1) (86, 12906) (86, 1)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nirs4all.core.config import Config\n",
    "from nirs4all.core.runner import ExperimentRunner\n",
    "\n",
    "config = Config.from_json_file(\"../sample_data/fast_train.json\")\n",
    "config.dataset = \"../sample_data/regression\"\n",
    "print(config)\n",
    "runner = ExperimentRunner(config, resume_mode=\"restart\")\n",
    "datasets, predictions, scores, best_params = runner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
