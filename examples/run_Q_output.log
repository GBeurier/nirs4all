=== Q Examples Run Started: 2025-10-17 10:16:16 ===
Launch: Q1_groupsplit.py
########################################
2025-10-17 10:16:17.727467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:16:18.420801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Dataset: classification (classification)
Features (samples=208, sources=1):
- Source 0: (208, 1, 234), processings=['raw'], min=-0.157, max=0.437, mean=0.196, var=0.014)
Targets: (samples=208, targets=1, processings=['numeric'])
- numeric: min=8.0, max=18.0, mean=12.486
Indexes:
- partition - "train", processings - ['raw']: 178 samples
- partition - "test", processings - ['raw']: 30 samples
Metadata(rows=208, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
 Starting pipeline Q1_groupsplit_a497c1 on dataset classification
------------------------------------------------------------------------------------------------------------------------
 Step 1: fold_Sample_ID
 Executing controller FoldChartController with operator str
 Using metadata column 'Sample_ID' for color coding
 No CV folds found. Creating visualization from train/test partition.
  Using train (178 samples including augmented) and test (30 samples) partitions.
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'split': {'class': 'sklearn.model_selection._split.GroupKFold', 'params': {'n_splits': 3, 'shuffle': True, 'random_state': 42}, '_runtime_instance': GroupKFold(n_splits=3, random_state=42, shuffle=True)}, 'group': 'Sample_ID'}
 Executing controller CrossValidatorController with operator GroupKFold
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification (classification)
Features (samples=208, sources=1):
- Source 0: (208, 1, 234), processings=['raw'], min=-0.157, max=0.437, mean=0.196, var=0.014)
Targets: (samples=208, targets=1, processings=['numeric'])
- numeric: min=8.0, max=18.0, mean=12.486
Indexes:
- partition - "train", processings - ['raw']: 178 samples
- partition - "test", processings - ['raw']: 30 samples
Metadata(rows=208, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
Folds: [(104, 74), (132, 46), (120, 58)]
------------------------------------------------------------------------------------------------------------------------
 Step 3: fold_Sample_ID
 Executing controller FoldChartController with operator str
 Using metadata column 'Sample_ID' for color coding
------------------------------------------------------------------------------------------------------------------------
========================================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Dataset: classification (classification)
Features (samples=208, sources=1):
- Source 0: (208, 1, 234), processings=['raw'], min=-0.157, max=0.437, mean=0.196, var=0.014)
Targets: (samples=208, targets=1, processings=['numeric'])
- numeric: min=8.0, max=18.0, mean=12.486
Indexes:
- partition - "train", processings - ['raw']: 178 samples
- partition - "test", processings - ['raw']: 30 samples
Metadata(rows=208, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
 Starting pipeline Q1_groupsplit_58aef4 on dataset classification
------------------------------------------------------------------------------------------------------------------------
 Step 1: fold_Sample_ID
 Executing controller FoldChartController with operator str
 Using metadata column 'Sample_ID' for color coding
 No CV folds found. Creating visualization from train/test partition.
  Using train (178 samples including augmented) and test (30 samples) partitions.
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'split': {'class': 'sklearn.model_selection._split.StratifiedGroupKFold', 'params': {'n_splits': 3, 'shuffle': True, 'random_state': 42}, '_runtime_instance': StratifiedGroupKFold(n_splits=3, random_state=42, shuffle=True)}, 'group': 'Sample_ID'}
 Executing controller CrossValidatorController with operator StratifiedGroupKFold
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification (classification)
Features (samples=208, sources=1):
- Source 0: (208, 1, 234), processings=['raw'], min=-0.157, max=0.437, mean=0.196, var=0.014)
Targets: (samples=208, targets=1, processings=['numeric'])
- numeric: min=8.0, max=18.0, mean=12.486
Indexes:
- partition - "train", processings - ['raw']: 178 samples
- partition - "test", processings - ['raw']: 30 samples
Metadata(rows=208, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
Folds: [(124, 54), (120, 58), (112, 66)]
------------------------------------------------------------------------------------------------------------------------
 Step 3: fold_Sample_ID
 Executing controller FoldChartController with operator str
 Using metadata column 'Sample_ID' for color coding
------------------------------------------------------------------------------------------------------------------------
 Step 4: fold_chart
 Executing controller FoldChartController without operator
------------------------------------------------------------------------------------------------------------------------
========================================================================================================================
########################################
Finished running: Q1_groupsplit.py
########################################
Launch: Q1_regression.py
########################################
2025-10-17 10:16:25.010316: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:16:25.855689: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
========================================================================================================================
 Starting Nirs4all run(s) with 2 pipeline on 1 dataset (2 total runs).
========================================================================================================================
 Starting pipeline Q1_c38e2a on dataset regression
------------------------------------------------------------------------------------------------------------------------
 PLS-1_components rmse  [test: 20.4376], [val: 29.5690], (fold: 0, id: 1) - [48pzhl]
 PLS-1_components rmse  [test: 19.9193], [val: 30.0184], (fold: 1, id: 2) - [n6k3md]
 PLS-1_components rmse  [test: 18.0984], [val: 20.7493], (fold: 2, id: 3) - [odxh3w]
 PLS-1_components rmse  [test: 18.9024], [val: 24.5484], (avg, id: 4) - [896vgd]
 PLS-1_components rmse  [test: 18.6666], [val: 23.7916], (w_avg, id: 5) - [am1oen]
 PLS-11_components rmse  [test: 15.2294], [val: 16.0602], (fold: 0, id: 1) - [pj7ey2]
 PLS-11_components rmse  [test: 16.5923], [val: 19.5953], (fold: 1, id: 2) - [te2r2c]
 PLS-11_components rmse  [test: 14.7274], [val: 16.8848], (fold: 2, id: 3) - [gtrpyb]
 PLS-11_components rmse  [test: 15.0098], [val: 11.3872], (avg, id: 4) - [40pse7]
 PLS-11_components rmse  [test: 14.9466], [val: 11.2802], (w_avg, id: 5) - [t8lnmr]
 PLS-21_components rmse  [test: 14.8236], [val: 15.9128], (fold: 0, id: 1) - [yu7c3v]
 PLS-21_components rmse  [test: 16.5802], [val: 20.2005], (fold: 1, id: 2) - [t43zrd]
 PLS-21_components rmse  [test: 14.5122], [val: 17.6738], (fold: 2, id: 3) - [fe5f49]
 PLS-21_components rmse  [test: 14.6518], [val: 11.1784], (avg, id: 4) - [plynoy]
 PLS-21_components rmse  [test: 14.5748], [val: 11.0275], (w_avg, id: 5) - [fq95fq]
 Pipeline Best: PLS-21_components - rmse [test: 14.5748], [val: 11.0275], , (fold: w_avg, id: 5, step: 8) - [fq95fq]
========================================================================================================================
 Starting pipeline Q1_afa8c9 on dataset regression
------------------------------------------------------------------------------------------------------------------------
 PLS-1_components rmse  [test: 20.2817], [val: 24.5134], (fold: 0, id: 1) - [z83tpu]
 PLS-1_components rmse  [test: 19.8926], [val: 26.3329], (fold: 1, id: 2) - [ggtawb]
 PLS-1_components rmse  [test: 19.8647], [val: 31.8260], (fold: 2, id: 3) - [v24ca0]
 PLS-1_components rmse  [test: 19.9852], [val: 27.3844], (avg, id: 4) - [hc26iz]
 PLS-1_components rmse  [test: 20.0020], [val: 27.3834], (w_avg, id: 5) - [bkjh72]
 PLS-11_components rmse  [test: 12.1114], [val: 14.4111], (fold: 0, id: 1) - [89t6zd]
 PLS-11_components rmse  [test: 12.2851], [val: 14.0843], (fold: 1, id: 2) - [1k9klm]
 PLS-11_components rmse  [test: 10.7347], [val: 17.1865], (fold: 2, id: 3) - [cxjza2]
 PLS-11_components rmse  [test: 11.4296], [val: 13.7725], (avg, id: 4) - [wyglqo]
 PLS-11_components rmse  [test: 11.4971], [val: 13.7840], (w_avg, id: 5) - [uebbfn]
 PLS-21_components rmse  [test: 12.6198], [val: 14.6881], (fold: 0, id: 1) - [y85w1k]
 PLS-21_components rmse  [test: 12.6206], [val: 16.0024], (fold: 1, id: 2) - [x2itel]
 PLS-21_components rmse  [test: 12.0939], [val: 15.9600], (fold: 2, id: 3) - [tlc49t]
 PLS-21_components rmse  [test: 11.2358], [val: 9.8187], (avg, id: 4) - [t35yfr]
 PLS-21_components rmse  [test: 11.2486], [val: 9.8152], (w_avg, id: 5) - [pf6asc]
 Pipeline Best: PLS-21_components - rmse [test: 11.2486], [val: 9.8152], , (fold: w_avg, id: 5, step: 8) - [pf6asc]
========================================================================================================================
 Best prediction in run for dataset 'regression': PLS-21_components - rmse [test: 11.2486], [val: 9.8152], , (fold: w_avg, id: 5, step: 8) - [pf6asc] | [Q1_afa8c9]
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max     | SD     | CV     | R     | RMSE   | MSE     | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
| Cros Val | 99      | 6453     | 32.175 | 21.380 | 4.180  | 128.310 | 27.585 | 0.857  | 0.873  | 9.815  | 96.339  | 9.811  | 7.301  | 2.81   | 0.304  | 96.0        |
| Train    | 130     | 6453     | 30.333 | 23.140 | 2.050  | 128.310 | 23.575 | 0.777  | 0.901  | 7.419  | 55.040  | 7.419  | 5.225  | 3.18   | -0.014 | 98.5        |
| Test     | 59      | 6453     | 31.762 | 27.110 | 1.330  | 84.570  | 19.805 | 0.624  | 0.677  | 11.249 | 126.532 | 11.216 | 8.656  | 1.77   | 0.860  | 89.8        |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
========================================================================================================================
Top 5 models by rmse:
1. PLS-21_components - rmse [test: 11.2486], [val: 9.8152], , (fold: w_avg, id: 5, step: 8) - [pf6asc] - MinMax|MinMax>Detr|MinMax>SG
2. PLS-21_components - rmse [test: 11.2358], [val: 9.8187], , (fold: avg, id: 4, step: 8) - [t35yfr] - MinMax|MinMax>Detr|MinMax>SG
3. PLS-21_components - rmse [test: 14.5748], [val: 11.0275], , (fold: w_avg, id: 5, step: 8) - [fq95fq] - MinMax|MinMax>1stDer|MinMax>Haar
4. PLS-21_components - rmse [test: 14.6518], [val: 11.1784], , (fold: avg, id: 4, step: 8) - [plynoy] - MinMax|MinMax>1stDer|MinMax>Haar
5. PLS-11_components - rmse [test: 14.9466], [val: 11.2802], , (fold: w_avg, id: 5, step: 7) - [t8lnmr] - MinMax|MinMax>1stDer|MinMax>Haar
 Saved prediction result to Q1_regression_best_model.csv\regression\pf6asc.csv
PredictionResult(id=pf6asc, model=PLS-21_components, dataset=regression, fold=w_avg, step=8, op=5)
PredictionResult(id=cxjza2, model=PLS-11_components, dataset=regression, fold=2, step=7, op=3)
PredictionResult(id=fe5f49, model=PLS-21_components, dataset=regression, fold=2, step=8, op=3)
########################################
Finished running: Q1_regression.py
########################################
Launch: Q1_classif.py
########################################
2025-10-17 10:16:35.106583: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:16:35.788442: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 1, 234), processings=['raw'], min=-0.157, max=0.437, mean=0.197, var=0.014)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw']: 178 samples
 Starting pipeline Q1_classification_c3abeb on dataset classification_Xtrain
------------------------------------------------------------------------------------------------------------------------
 Step 1: chart_2d
 Executing controller SpectraChartController without operator
   Source 0: 1 processings: ['raw']
   Data shape: (178, 1, 234)
   Headers available: 234, features: 234
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'feature_augmentation': ['nirs4all.operators.transformations.signal.Detrend', 'nirs4all.operators.transformations.nirs.FirstDerivative', 'nirs4all.operators.transformations.nirs.SecondDerivative', 'nirs4all.operators.transformations.signal.Gaussian', 'nirs4all.operators.transformations.scalers.StandardNormalVariate', 'nirs4all.operators.transformations.nirs.SavitzkyGolay', 'nirs4all.operators.transformations.nirs.Haar', 'nirs4all.operators.transformations.nirs.MultiplicativeScatterCorrection']}
 Executing controller FeatureAugmentationController with operator list
    Sub-step 2.1: nirs4all.operators.transformations.signal.Detrend
 Executing controller TransformerMixinController with operator Detrend
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 2, 234), processings=['raw', 'raw_Detrend_1'], min=-0.336, max=0.437, mean=0.098, var=0.023)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1']: 178 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 2.2: nirs4all.operators.transformations.nirs.FirstDerivative
 Executing controller TransformerMixinController with operator FirstDerivative
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 3, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2'], min=-0.336, max=0.437, mean=0.066, var=0.018)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2']: 178 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 2.3: nirs4all.operators.transformations.nirs.SecondDerivative
 Executing controller TransformerMixinController with operator SecondDerivative
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 4, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3'], min=-0.336, max=0.437, mean=0.049, var=0.014)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3']: 178 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 2.4: nirs4all.operators.transformations.signal.Gaussian
 Executing controller TransformerMixinController with operator Gaussian
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 5, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4'], min=-0.336, max=0.437, mean=0.04, var=0.012)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4']: 178 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 2.5: nirs4all.operators.transformations.scalers.StandardNormalVariate
 Executing controller TransformerMixinController with operator StandardNormalVariate
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 6, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5'], min=-2.915, max=1.868, mean=0.033, var=0.177)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5']: 178 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 2.6: nirs4all.operators.transformations.nirs.SavitzkyGolay
 Executing controller TransformerMixinController with operator SavitzkyGolay
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 7, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6'], min=-2.915, max=1.868, mean=0.056, var=0.157)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6']: 178 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 2.7: nirs4all.operators.transformations.nirs.Haar
 Executing controller TransformerMixinController with operator Haar
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 8, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7'], min=-2.915, max=1.868, mean=0.049, var=0.137)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7']: 178 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 2.8: nirs4all.operators.transformations.nirs.MultiplicativeScatterCorrection
 Executing controller TransformerMixinController with operator MultiplicativeScatterCorrection
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 9, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8'], min=-2.915, max=1.868, mean=0.044, var=0.122)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8']: 178 samples
------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 9, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8'], min=-2.915, max=1.868, mean=0.044, var=0.122)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8']: 178 samples
------------------------------------------------------------------------------------------------------------------------
 Step 3: chart_2d
 Executing controller SpectraChartController without operator
   Source 0: 9 processings: ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8']
   Data shape: (178, 9, 234)
   Headers available: 234, features: 234
------------------------------------------------------------------------------------------------------------------------
 Step 4: fold_chart
 Executing controller FoldChartController without operator
 No CV folds found. Creating visualization from train/test partition.
  Only train partition available (178 samples including augmented).
------------------------------------------------------------------------------------------------------------------------
 Step 5: {'class': 'nirs4all.operators.splitters.splitters.SPXYSplitter', 'params': {'test_size': 0.25}, '_runtime_instance': SPXYSplitter(metric='euclidean', pca_components=None, random_state=None,
       test_size=0.25)}
 Executing controller CrossValidatorController with operator SPXYSplitter
 No test partition found; using first fold as test set.
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 9, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8'], min=-2.915, max=1.868, mean=0.044, var=0.122)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8']: 133 samples
- partition - "test", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8']: 45 samples
Folds: [(133, 45)]
------------------------------------------------------------------------------------------------------------------------
 Step 6: fold_chart
 Executing controller FoldChartController without operator
------------------------------------------------------------------------------------------------------------------------
 Step 7: {'class': 'sklearn.model_selection._split.ShuffleSplit', 'params': {'n_splits': 3, 'test_size': 0.25}, '_runtime_instance': ShuffleSplit(n_splits=3, random_state=None, test_size=0.25, train_size=None)}
 Executing controller CrossValidatorController with operator ShuffleSplit
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: classification_Xtrain (classification)
Features (samples=178, sources=1):
- Source 0: (178, 9, 234), processings=['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8'], min=-2.915, max=1.868, mean=0.044, var=0.122)
Targets: (samples=178, targets=1, processings=['numeric'])
- numeric: min=10.0, max=17.0, mean=12.612
Indexes:
- partition - "train", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8']: 133 samples
- partition - "test", processings - ['raw', 'raw_Detrend_1', 'raw_FirstDerivative_2', 'raw_SecondDerivative_3', 'raw_Gaussian_4', 'raw_StandardNormalVariate_5', 'raw_SavitzkyGolay_6', 'raw_Haar_7', 'raw_MultiplicativeScatterCorrection_8']: 45 samples
Folds: [(99, 34), (99, 34), (99, 34)]
------------------------------------------------------------------------------------------------------------------------
 Step 8: fold_chart
 Executing controller FoldChartController without operator
------------------------------------------------------------------------------------------------------------------------
 Step 9: {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 30}, '_runtime_instance': RandomForestClassifier(max_depth=30)}
 Executing controller SklearnModelController with operator RandomForestClassifier
 Model config: {'class': 'sklearn.ensemble._forest.RandomForestClassifier', 'params': {'max_depth': 30}, '_runtime_instance': RandomForestClassifier(max_depth=30), 'model_instance': RandomForestClassifier(max_depth=30)}
 RandomForestClassifier accuracy  [test: 0.3333], [val: 0.2647], (fold: 0, id: 1) - [ddefa9]
 RandomForestClassifier accuracy  [test: 0.3778], [val: 0.2647], (fold: 1, id: 2) - [2e3nwn]
 RandomForestClassifier accuracy  [test: 0.3556], [val: 0.2353], (fold: 2, id: 3) - [l9jnnw]
------------------------------------------------------------------------------------------------------------------------
 Pipeline Best: RandomForestClassifier - accuracy [test: 0.3333], [val: 0.2647], , (fold: 0, id: 1, step: 9) - [ddefa9]
 Pipeline Q1_classification_c3abeb completed successfully on dataset classification_Xtrain
========================================================================================================================
 Best prediction in run for dataset 'classification_Xtrain': RandomForestClassifier - accuracy [test: 0.3333], [val: 0.2647], , (fold: 0, id: 1, step: 9) - [ddefa9] | [Q1_classification_c3abeb]
|----------|---------|-----------|----------|-----------|--------|----------|-------------|
|          | Nsample | Nfeatures | Accuracy | Precision | Recall | F1-score | Specificity |
|----------|---------|-----------|----------|-----------|--------|----------|-------------|
| Cros Val | 34      | 2106      | 0.265    | 0.175     | 0.265  | 0.207    | 0.822       |
| Train    | 99      | 2106      | 1.000    | 1.000     | 1.000  | 1.000    | 1.000       |
| Test     | 45      | 2106      | 0.333    | 0.195     | 0.333  | 0.224    | 0.795       |
|----------|---------|-----------|----------|-----------|--------|----------|-------------|
========================================================================================================================
Top 5 models by accuracy:
1. RandomForestClassifier - accuracy [test: 0.3556], [val: 0.2353], [accuracy:0.2353], (fold: 2, id: 3, step: 9) - [l9jnnw] - raw|Detr|1stDer|2ndDer|Gauss|SNV|SG|Haar|MSC
2. RandomForestClassifier - accuracy [test: 0.3333], [val: 0.2647], [accuracy:0.2647], (fold: 0, id: 1, step: 9) - [ddefa9] - raw|Detr|1stDer|2ndDer|Gauss|SNV|SG|Haar|MSC
3. RandomForestClassifier - accuracy [test: 0.3778], [val: 0.2647], [accuracy:0.2647], (fold: 1, id: 2, step: 9) - [2e3nwn] - raw|Detr|1stDer|2ndDer|Gauss|SNV|SG|Haar|MSC
########################################
Finished running: Q1_classif.py
########################################
Launch: Q2_multimodel.py
########################################
2025-10-17 10:16:44.096506: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:16:44.772729: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:17:23.182349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline Q2_b9f52b on dataset regression
------------------------------------------------------------------------------------------------------------------------
 PLSRegression rmse  [test: 10.7834], [val: 16.3323], (fold: 0, id: 1) - [wmvg7q]
 PLSRegression rmse  [test: 11.2178], [val: 10.4307], (fold: 1, id: 2) - [grond5]
 PLSRegression rmse  [test: 10.6660], [val: 9.1765], (fold: 2, id: 3) - [fpl8ch]
 PLSRegression rmse  [test: 10.6714], [val: 9.5400], (avg, id: 4) - [lmhlr1]
 PLSRegression rmse  [test: 10.6898], [val: 9.3330], (w_avg, id: 5) - [sw7x9v]
 PLSRegression rmse  [test: 13.4538], [val: 16.4129], (fold: 0, id: 1) - [d2gvlp]
 PLSRegression rmse  [test: 13.4032], [val: 10.8238], (fold: 1, id: 2) - [9tdey4]
 PLSRegression rmse  [test: 13.2928], [val: 11.0878], (fold: 2, id: 3) - [vaddov]
 PLSRegression rmse  [test: 13.2662], [val: 11.4039], (avg, id: 4) - [i07u35]
 PLSRegression rmse  [test: 13.2616], [val: 11.3436], (w_avg, id: 5) - [p9cqel]
 RandomForestRegressor rmse  [test: 24.3359], [val: 24.9315], (fold: 0, id: 1) - [f1k7r6]
 RandomForestRegressor rmse  [test: 24.4011], [val: 21.7246], (fold: 1, id: 2) - [mq2inr]
 RandomForestRegressor rmse  [test: 25.6336], [val: 18.0383], (fold: 2, id: 3) - [l84ekl]
 RandomForestRegressor rmse  [test: 24.6713], [val: 13.1904], (avg, id: 4) - [n6jjzk]
 RandomForestRegressor rmse  [test: 24.7468], [val: 13.0522], (w_avg, id: 5) - [gzx4c8]
 ElasticNet rmse  [test: 19.8803], [val: 21.1568], (fold: 0, id: 1) - [mdvj5r]
 ElasticNet rmse  [test: 19.9234], [val: 22.3106], (fold: 1, id: 2) - [abcnt4]
 ElasticNet rmse  [test: 19.8209], [val: 11.8307], (fold: 2, id: 3) - [6nx69c]
 ElasticNet rmse  [test: 19.8667], [val: 18.8566], (avg, id: 4) - [599m1f]
 ElasticNet rmse  [test: 19.8540], [val: 18.8448], (w_avg, id: 5) - [gw1cia]
 SVR_Custom_Model rmse  [test: 20.9062], [val: 23.9048], (fold: 0, id: 1) - [ezai5h]
 SVR_Custom_Model rmse  [test: 20.4233], [val: 21.0805], (fold: 1, id: 2) - [rbsrrb]
 SVR_Custom_Model rmse  [test: 21.3006], [val: 13.2168], (fold: 2, id: 3) - [ny2373]
 SVR_Custom_Model rmse  [test: 20.8347], [val: 18.1579], (avg, id: 4) - [arjngi]
 SVR_Custom_Model rmse  [test: 20.9073], [val: 18.0659], (w_avg, id: 5) - [xcf66n]
 MLP_Custom_Model rmse  [test: 23.6283], [val: 25.7932], (fold: 0, id: 1) - [4nhgzy]
 MLP_Custom_Model rmse  [test: 24.4891], [val: 19.6735], (fold: 1, id: 2) - [syue3n]
 MLP_Custom_Model rmse  [test: 26.3600], [val: 16.6993], (fold: 2, id: 3) - [oecxjw]
 MLP_Custom_Model rmse  [test: 24.4178], [val: 17.2724], (avg, id: 4) - [xj6hi0]
 MLP_Custom_Model rmse  [test: 24.6342], [val: 17.1479], (w_avg, id: 5) - [yjn6jt]
 GradientBoostingRegressor rmse  [test: 24.2895], [val: 26.5072], (fold: 0, id: 1) - [jp07nu]
 GradientBoostingRegressor rmse  [test: 24.6613], [val: 22.5680], (fold: 1, id: 2) - [h74zvw]
 GradientBoostingRegressor rmse  [test: 24.8245], [val: 19.7498], (fold: 2, id: 3) - [55n9k4]
 GradientBoostingRegressor rmse  [test: 23.9913], [val: 8.7691], (avg, id: 4) - [6zop1h]
 GradientBoostingRegressor rmse  [test: 24.0213], [val: 8.5888], (w_avg, id: 5) - [gahxeq]
 nicon rmse  [test: 32.6522], [val: 33.1059], (fold: 0, id: 1) - [t3art8]
 nicon rmse  [test: 48.1110], [val: 46.7899], (fold: 1, id: 2) - [0pq9l5]
 nicon rmse  [test: 33.7920], [val: 35.6613], (fold: 2, id: 3) - [bomcqu]
 nicon rmse  [test: 37.9455], [val: 38.0804], (avg, id: 4) - [3cqnsn]
 nicon rmse  [test: 36.9826], [val: 37.0837], (w_avg, id: 5) - [j7gfx3]
 Pipeline Best: GradientBoostingRegressor - rmse [test: 24.0213], [val: 8.5888], , (fold: w_avg, id: 5, step: 14) - [gahxeq]
========================================================================================================================
 Best prediction in run for dataset 'regression': GradientBoostingRegressor - rmse [test: 24.0213], [val: 8.5888], , (fold: w_avg, id: 5, step: 14) - [gahxeq] | [Q2_b9f52b]
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max     | SD     | CV     | R     | RMSE   | MSE     | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
| Cros Val | 39      | 2151     | 31.592 | 26.880 | 9.360  | 96.120  | 18.805 | 0.595  | 0.791  | 8.589  | 73.768  | 8.586  | 7.116  | 2.19   | 0.219  | 97.4        |
| Train    | 130     | 2151     | 30.333 | 23.140 | 2.050  | 128.310 | 23.575 | 0.777  | 0.955  | 5.014  | 25.138  | 5.012  | 3.350  | 4.70   | 0.141  | 100.0       |
| Test     | 59      | 2151     | 31.762 | 27.110 | 1.330  | 84.570  | 19.805 | 0.624  | -0.471 | 24.021 | 577.021 | 23.967 | 20.029 | 0.83   | -1.612 | 57.6        |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
========================================================================================================================

Top 3 models by rmse:
1. GradientBoostingRegressor - rmse [test: 24.0213], [val: 8.5888], , (fold: w_avg, id: 5, step: 14) - [gahxeq] - MinMax>MSC
2. GradientBoostingRegressor - rmse [test: 23.9913], [val: 8.7691], , (fold: avg, id: 4, step: 14) - [6zop1h] - MinMax>MSC
3. PLSRegression - rmse [test: 10.6660], [val: 9.1765], , (fold: 2, id: 3, step: 8) - [fpl8ch] - MinMax>MSC
PredictionResult(id=gahxeq, model=GradientBoostingRegressor, dataset=regression, fold=w_avg, step=14, op=5)
########################################
Finished running: Q2_multimodel.py
########################################
Launch: Q3_finetune.py
########################################
2025-10-17 10:17:32.519999: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:17:33.178499: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[I 2025-10-17 10:17:36,016] A new study created in memory with name: no-name-5b1c8894-75ee-4fae-b093-ec15ff2e3c9f
[I 2025-10-17 10:17:36,044] Trial 0 finished with value: 0.03403331874775629 and parameters: {'n_components': 2}. Best is trial 0 with value: 0.03403331874775629.
[I 2025-10-17 10:17:36,104] Trial 1 finished with value: 0.016984171723909244 and parameters: {'n_components': 8}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,158] Trial 2 finished with value: 0.01727878671404051 and parameters: {'n_components': 7}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,180] Trial 3 finished with value: 0.036776010015780035 and parameters: {'n_components': 1}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,229] Trial 4 finished with value: 0.017332493479439295 and parameters: {'n_components': 6}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,289] Trial 5 finished with value: 0.016984171723909244 and parameters: {'n_components': 8}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,331] Trial 6 finished with value: 0.019327506488654552 and parameters: {'n_components': 5}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,359] Trial 7 finished with value: 0.03403331874775629 and parameters: {'n_components': 2}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,385] Trial 8 finished with value: 0.03403331874775629 and parameters: {'n_components': 2}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,432] Trial 9 finished with value: 0.017332493479439295 and parameters: {'n_components': 6}. Best is trial 1 with value: 0.016984171723909244.
[I 2025-10-17 10:17:36,550] A new study created in memory with name: no-name-8e50f477-6e30-4d16-8918-206f86adcaa7
2025-10-17 10:17:36.553667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[I 2025-10-17 10:17:38,144] Trial 0 finished with value: 0.09463611990213394 and parameters: {'filters_1': 32, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.09463611990213394.
[I 2025-10-17 10:17:39,751] Trial 1 finished with value: 0.06288411468267441 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 16}. Best is trial 1 with value: 0.06288411468267441.
[I 2025-10-17 10:17:41,472] Trial 2 finished with value: 0.06059924140572548 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 64}. Best is trial 2 with value: 0.06059924140572548.
[I 2025-10-17 10:17:43,011] Trial 3 finished with value: 0.048318732529878616 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 16}. Best is trial 3 with value: 0.048318732529878616.
[I 2025-10-17 10:17:44,571] Trial 4 finished with value: 0.03462968394160271 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}. Best is trial 4 with value: 0.03462968394160271.
[I 2025-10-17 10:17:46,182] Trial 5 finished with value: 0.052692439407110214 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}. Best is trial 4 with value: 0.03462968394160271.
[I 2025-10-17 10:17:47,916] Trial 6 finished with value: 0.04373189061880112 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 64}. Best is trial 4 with value: 0.03462968394160271.
[I 2025-10-17 10:17:49,485] Trial 7 finished with value: 0.053111691027879715 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 8}. Best is trial 4 with value: 0.03462968394160271.
[I 2025-10-17 10:17:51,006] Trial 8 finished with value: 0.03736397996544838 and parameters: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}. Best is trial 4 with value: 0.03462968394160271.
[I 2025-10-17 10:17:52,757] Trial 9 finished with value: 0.04189688339829445 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}. Best is trial 4 with value: 0.03462968394160271.
[I 2025-10-17 10:18:05,007] A new study created in memory with name: no-name-5af40f4d-a58d-4099-93e3-fd5b96d64d86
[I 2025-10-17 10:18:05,059] Trial 0 finished with value: 0.03286202873271029 and parameters: {'n_components': 7}. Best is trial 0 with value: 0.03286202873271029.
[I 2025-10-17 10:18:05,090] Trial 1 finished with value: 0.03535492988681746 and parameters: {'n_components': 3}. Best is trial 0 with value: 0.03286202873271029.
[I 2025-10-17 10:18:05,147] Trial 2 finished with value: 0.033976235596111924 and parameters: {'n_components': 8}. Best is trial 0 with value: 0.03286202873271029.
[I 2025-10-17 10:18:05,170] Trial 3 finished with value: 0.038632073046937415 and parameters: {'n_components': 1}. Best is trial 0 with value: 0.03286202873271029.
[I 2025-10-17 10:18:05,192] Trial 4 finished with value: 0.038632073046937415 and parameters: {'n_components': 1}. Best is trial 0 with value: 0.03286202873271029.
[I 2025-10-17 10:18:05,233] Trial 5 finished with value: 0.03215184637931497 and parameters: {'n_components': 5}. Best is trial 5 with value: 0.03215184637931497.
[I 2025-10-17 10:18:05,255] Trial 6 finished with value: 0.038632073046937415 and parameters: {'n_components': 1}. Best is trial 5 with value: 0.03215184637931497.
[I 2025-10-17 10:18:05,322] Trial 7 finished with value: 0.034656845384341196 and parameters: {'n_components': 10}. Best is trial 5 with value: 0.03215184637931497.
[I 2025-10-17 10:18:05,344] Trial 8 finished with value: 0.038632073046937415 and parameters: {'n_components': 1}. Best is trial 5 with value: 0.03215184637931497.
[I 2025-10-17 10:18:05,381] Trial 9 finished with value: 0.03559159653965931 and parameters: {'n_components': 4}. Best is trial 5 with value: 0.03215184637931497.
[I 2025-10-17 10:18:05,494] A new study created in memory with name: no-name-4a7f5c10-fa7b-4a85-8f27-26af749d44ae
[I 2025-10-17 10:18:07,030] Trial 0 finished with value: 0.036014411598443985 and parameters: {'filters_1': 32, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.036014411598443985.
[I 2025-10-17 10:18:08,535] Trial 1 finished with value: 0.05818570405244827 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.036014411598443985.
[I 2025-10-17 10:18:10,413] Trial 2 finished with value: 0.04248809441924095 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 64}. Best is trial 0 with value: 0.036014411598443985.
[I 2025-10-17 10:18:11,947] Trial 3 finished with value: 0.09098131209611893 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.036014411598443985.
[I 2025-10-17 10:18:13,500] Trial 4 finished with value: 0.03454089164733887 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}. Best is trial 4 with value: 0.03454089164733887.
[I 2025-10-17 10:18:15,014] Trial 5 finished with value: 0.04091498255729675 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}. Best is trial 4 with value: 0.03454089164733887.
[I 2025-10-17 10:18:16,543] Trial 6 finished with value: 0.03524807095527649 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 64}. Best is trial 4 with value: 0.03454089164733887.
[I 2025-10-17 10:18:18,037] Trial 7 finished with value: 0.10420215129852295 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 8}. Best is trial 4 with value: 0.03454089164733887.
[I 2025-10-17 10:18:19,829] Trial 8 finished with value: 0.0347672663629055 and parameters: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}. Best is trial 4 with value: 0.03454089164733887.
[I 2025-10-17 10:18:21,307] Trial 9 finished with value: 0.03460041433572769 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}. Best is trial 4 with value: 0.03454089164733887.
[I 2025-10-17 10:18:32,518] A new study created in memory with name: no-name-2578c736-65ee-441b-a1a2-7ea0b666b85b
[I 2025-10-17 10:18:32,580] Trial 0 finished with value: 0.015508750366391225 and parameters: {'n_components': 9}. Best is trial 0 with value: 0.015508750366391225.
[I 2025-10-17 10:18:32,625] Trial 1 finished with value: 0.01440387339724595 and parameters: {'n_components': 6}. Best is trial 1 with value: 0.01440387339724595.
[I 2025-10-17 10:18:32,680] Trial 2 finished with value: 0.014316963246469224 and parameters: {'n_components': 8}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:32,706] Trial 3 finished with value: 0.03054004119696518 and parameters: {'n_components': 2}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:32,752] Trial 4 finished with value: 0.01440387339724595 and parameters: {'n_components': 6}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:32,806] Trial 5 finished with value: 0.014316963246469224 and parameters: {'n_components': 8}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:32,862] Trial 6 finished with value: 0.014316963246469224 and parameters: {'n_components': 8}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:32,892] Trial 7 finished with value: 0.024702845064472834 and parameters: {'n_components': 3}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:32,939] Trial 8 finished with value: 0.01440387339724595 and parameters: {'n_components': 6}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:32,994] Trial 9 finished with value: 0.014316963246469224 and parameters: {'n_components': 8}. Best is trial 2 with value: 0.014316963246469224.
[I 2025-10-17 10:18:33,116] A new study created in memory with name: no-name-f6808e6e-7981-4ff1-9a65-78af02920a92
[I 2025-10-17 10:18:34,621] Trial 0 finished with value: 0.039624232798814774 and parameters: {'filters_1': 32, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.039624232798814774.
[I 2025-10-17 10:18:36,433] Trial 1 finished with value: 0.03836381062865257 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 16}. Best is trial 1 with value: 0.03836381062865257.
[I 2025-10-17 10:18:38,031] Trial 2 finished with value: 0.12610267102718353 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 64}. Best is trial 1 with value: 0.03836381062865257.
[I 2025-10-17 10:18:39,522] Trial 3 finished with value: 0.03565359488129616 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 16}. Best is trial 3 with value: 0.03565359488129616.
[I 2025-10-17 10:18:41,020] Trial 4 finished with value: 0.055364396423101425 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}. Best is trial 3 with value: 0.03565359488129616.
[I 2025-10-17 10:18:42,551] Trial 5 finished with value: 0.04850561544299126 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}. Best is trial 3 with value: 0.03565359488129616.
[I 2025-10-17 10:18:44,073] Trial 6 finished with value: 0.06154128536581993 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 64}. Best is trial 3 with value: 0.03565359488129616.
[I 2025-10-17 10:18:45,574] Trial 7 finished with value: 0.14694635570049286 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 8}. Best is trial 3 with value: 0.03565359488129616.
[I 2025-10-17 10:18:47,067] Trial 8 finished with value: 0.03476257249712944 and parameters: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}. Best is trial 8 with value: 0.03476257249712944.
[I 2025-10-17 10:18:49,017] Trial 9 finished with value: 0.07908425480127335 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}. Best is trial 8 with value: 0.03476257249712944.
========================================================================================================================
 Starting Nirs4all run(s) with 5 pipeline on 1 dataset (5 total runs).
========================================================================================================================
 Starting pipeline Q1_finetune_07fc8f on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0170
 Best parameters: {'n_components': 8}
 Best parameters: {'n_components': 8}
Training model PLS-Finetuned with: {'n_components': 8}...
Training model PLS-Finetuned with: {'n_components': 8}...
Training model PLS-Finetuned with: {'n_components': 8}...
 PLS-Finetuned rmse  [test: 13.1719], [val: 11.3584], (fold: 0, id: 1) - [ri1551]
 PLS-Finetuned rmse  [test: 13.0032], [val: 13.0318], (fold: 1, id: 2) - [t75jro]
 PLS-Finetuned rmse  [test: 14.3439], [val: 12.7481], (fold: 2, id: 3) - [3h7vxv]
 PLS-Finetuned rmse  [test: 13.2282], [val: 6.7771], (avg, id: 4) - [4az9rk]
 PLS-Finetuned rmse  [test: 13.2206], [val: 6.7835], (w_avg, id: 5) - [s9f7ve]
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0346
 Best parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}
 Best parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}...
 PLS-Default rmse  [test: 20.1608], [val: 19.8157], (fold: 0, id: 1) - [pok4m2]
 PLS-Default rmse  [test: 20.8806], [val: 19.6473], (fold: 1, id: 2) - [kh83x2]
 PLS-Default rmse  [test: 20.2130], [val: 26.1796], (fold: 2, id: 3) - [vk9k6t]
 PLS-Default rmse  [test: 20.3043], [val: 21.2627], (avg, id: 4) - [n6ac53]
 PLS-Default rmse  [test: 20.3333], [val: 21.1760], (w_avg, id: 5) - [m7zsu7]
 PLS-1_components rmse  [test: 20.1060], [val: 20.9298], (fold: 0, id: 1) - [q5gk6j]
 PLS-1_components rmse  [test: 19.9133], [val: 17.7333], (fold: 1, id: 2) - [c2ig5g]
 PLS-1_components rmse  [test: 20.0772], [val: 22.4946], (fold: 2, id: 3) - [mzggjq]
 PLS-1_components rmse  [test: 19.9548], [val: 19.4429], (avg, id: 4) - [0b5cw6]
 PLS-1_components rmse  [test: 19.9494], [val: 19.3712], (w_avg, id: 5) - [mio2g4]
 PLS-4_components rmse  [test: 14.4737], [val: 14.0559], (fold: 0, id: 1) - [h331lq]
 PLS-4_components rmse  [test: 13.5657], [val: 13.8837], (fold: 1, id: 2) - [ff1grq]
 PLS-4_components rmse  [test: 13.8619], [val: 14.4294], (fold: 2, id: 3) - [88znh4]
 PLS-4_components rmse  [test: 13.8330], [val: 11.3010], (avg, id: 4) - [0dg8tr]
 PLS-4_components rmse  [test: 13.8327], [val: 11.3084], (w_avg, id: 5) - [ltborj]
 PLS-7_components rmse  [test: 13.2919], [val: 11.8136], (fold: 0, id: 1) - [g09b6o]
 PLS-7_components rmse  [test: 12.7392], [val: 12.8600], (fold: 1, id: 2) - [eh2zg7]
 PLS-7_components rmse  [test: 13.6640], [val: 12.6108], (fold: 2, id: 3) - [6y3woz]
 PLS-7_components rmse  [test: 12.9887], [val: 7.4501], (avg, id: 4) - [5af8vx]
 PLS-7_components rmse  [test: 12.9933], [val: 7.4545], (w_avg, id: 5) - [f5fzaj]
 PLS-10_components rmse  [test: 13.1751], [val: 11.4276], (fold: 0, id: 1) - [b1nwsg]
 PLS-10_components rmse  [test: 13.0341], [val: 13.2515], (fold: 1, id: 2) - [bt5zws]
 PLS-10_components rmse  [test: 14.0811], [val: 13.1471], (fold: 2, id: 3) - [dtkdts]
 PLS-10_components rmse  [test: 13.1167], [val: 6.6208], (avg, id: 4) - [6voerj]
 PLS-10_components rmse  [test: 13.1095], [val: 6.6333], (w_avg, id: 5) - [ydrty2]
 PLS-13_components rmse  [test: 13.1732], [val: 11.3888], (fold: 0, id: 1) - [o2ehgg]
 PLS-13_components rmse  [test: 12.9118], [val: 13.0881], (fold: 1, id: 2) - [ru1acu]
 PLS-13_components rmse  [test: 13.9945], [val: 13.1705], (fold: 2, id: 3) - [5dcu4l]
 PLS-13_components rmse  [test: 13.0337], [val: 6.4349], (avg, id: 4) - [td5soe]
 PLS-13_components rmse  [test: 13.0273], [val: 6.4501], (w_avg, id: 5) - [oep39e]
 PLS-16_components rmse  [test: 13.2102], [val: 11.3529], (fold: 0, id: 1) - [ieu8fy]
 PLS-16_components rmse  [test: 12.8748], [val: 13.0942], (fold: 1, id: 2) - [bn4gpc]
 PLS-16_components rmse  [test: 13.9611], [val: 13.1523], (fold: 2, id: 3) - [tuebi9]
 PLS-16_components rmse  [test: 13.0280], [val: 6.4516], (avg, id: 4) - [zxqdku]
 PLS-16_components rmse  [test: 13.0242], [val: 6.4665], (w_avg, id: 5) - [9knhgw]
 PLS-19_components rmse  [test: 13.2079], [val: 11.3685], (fold: 0, id: 1) - [sgf87u]
 PLS-19_components rmse  [test: 12.8683], [val: 13.0941], (fold: 1, id: 2) - [3gsdm1]
 PLS-19_components rmse  [test: 13.9629], [val: 13.1555], (fold: 2, id: 3) - [9k1xc5]
 PLS-19_components rmse  [test: 13.0254], [val: 6.4511], (avg, id: 4) - [ic032q]
 PLS-19_components rmse  [test: 13.0217], [val: 6.4663], (w_avg, id: 5) - [ooqova]
 PLS-22_components rmse  [test: 13.2086], [val: 11.3692], (fold: 0, id: 1) - [h435fu]
 PLS-22_components rmse  [test: 12.8687], [val: 13.0944], (fold: 1, id: 2) - [gh4rha]
 PLS-22_components rmse  [test: 13.9635], [val: 13.1569], (fold: 2, id: 3) - [onstw6]
 PLS-22_components rmse  [test: 13.0257], [val: 6.4506], (avg, id: 4) - [etn1ss]
 PLS-22_components rmse  [test: 13.0220], [val: 6.4659], (w_avg, id: 5) - [5f54n2]
 PLS-25_components rmse  [test: 13.2087], [val: 11.3691], (fold: 0, id: 1) - [bw8fm5]
 PLS-25_components rmse  [test: 12.8688], [val: 13.0945], (fold: 1, id: 2) - [i443jj]
 PLS-25_components rmse  [test: 13.9635], [val: 13.1570], (fold: 2, id: 3) - [hqt36z]
 PLS-25_components rmse  [test: 13.0257], [val: 6.4506], (avg, id: 4) - [wzkpq8]
 PLS-25_components rmse  [test: 13.0220], [val: 6.4659], (w_avg, id: 5) - [v8tczj]
 PLS-28_components rmse  [test: 13.2088], [val: 11.3691], (fold: 0, id: 1) - [v6i7jw]
 PLS-28_components rmse  [test: 12.8687], [val: 13.0945], (fold: 1, id: 2) - [e5buec]
 PLS-28_components rmse  [test: 13.9635], [val: 13.1570], (fold: 2, id: 3) - [104jmo]
 PLS-28_components rmse  [test: 13.0257], [val: 6.4506], (avg, id: 4) - [c53b66]
 PLS-28_components rmse  [test: 13.0220], [val: 6.4659], (w_avg, id: 5) - [86nyyz]
 Pipeline Best: PLS-13_components - rmse [test: 13.0337], [val: 6.4349], , (fold: avg, id: 4, step: 12) - [td5soe]
========================================================================================================================
 Starting pipeline Q1_finetune_f8673f on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0322
 Best parameters: {'n_components': 5}
 Best parameters: {'n_components': 5}
Training model PLS-Finetuned with: {'n_components': 5}...
Training model PLS-Finetuned with: {'n_components': 5}...
Training model PLS-Finetuned with: {'n_components': 5}...
 PLS-Finetuned rmse  [test: 22.5943], [val: 21.2621], (fold: 0, id: 1) - [7qlhq2]
 PLS-Finetuned rmse  [test: 22.4109], [val: 22.0393], (fold: 1, id: 2) - [vhnr36]
 PLS-Finetuned rmse  [test: 21.5480], [val: 27.5287], (fold: 2, id: 3) - [hp5j4n]
 PLS-Finetuned rmse  [test: 21.6847], [val: 17.2857], (avg, id: 4) - [8lx9we]
 PLS-Finetuned rmse  [test: 21.7321], [val: 17.1545], (w_avg, id: 5) - [ezhz9o]
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0345
 Best parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}
 Best parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}
[I 2025-10-17 10:19:01,445] A new study created in memory with name: no-name-99132d0a-6782-48fe-9297-9d7e47222979
[I 2025-10-17 10:19:01,473] Trial 0 finished with value: 0.03139910973556525 and parameters: {'n_components': 2}. Best is trial 0 with value: 0.03139910973556525.
[I 2025-10-17 10:19:01,521] Trial 1 finished with value: 0.015375121353358943 and parameters: {'n_components': 6}. Best is trial 1 with value: 0.015375121353358943.
[I 2025-10-17 10:19:01,587] Trial 2 finished with value: 0.01491212226291909 and parameters: {'n_components': 10}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,633] Trial 3 finished with value: 0.015375121353358943 and parameters: {'n_components': 6}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,660] Trial 4 finished with value: 0.03139910973556525 and parameters: {'n_components': 2}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,697] Trial 5 finished with value: 0.020373852865146886 and parameters: {'n_components': 4}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,743] Trial 6 finished with value: 0.015375121353358943 and parameters: {'n_components': 6}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,773] Trial 7 finished with value: 0.024562675623764726 and parameters: {'n_components': 3}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,815] Trial 8 finished with value: 0.019304059936537415 and parameters: {'n_components': 5}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,841] Trial 9 finished with value: 0.03139910973556525 and parameters: {'n_components': 2}. Best is trial 2 with value: 0.01491212226291909.
[I 2025-10-17 10:19:01,971] A new study created in memory with name: no-name-9b32cb9b-8828-461c-bcbc-2487db02d45e
[I 2025-10-17 10:19:03,470] Trial 0 finished with value: 0.03483457863330841 and parameters: {'filters_1': 32, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.03483457863330841.
[I 2025-10-17 10:19:04,984] Trial 1 finished with value: 0.03606099635362625 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.03483457863330841.
[I 2025-10-17 10:19:06,526] Trial 2 finished with value: 0.06390434503555298 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 64}. Best is trial 0 with value: 0.03483457863330841.
[I 2025-10-17 10:19:08,032] Trial 3 finished with value: 0.1370212435722351 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.03483457863330841.
[I 2025-10-17 10:19:09,542] Trial 4 finished with value: 0.04207439720630646 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}. Best is trial 0 with value: 0.03483457863330841.
[I 2025-10-17 10:19:11,610] Trial 5 finished with value: 0.03481818735599518 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}. Best is trial 5 with value: 0.03481818735599518.
[I 2025-10-17 10:19:13,136] Trial 6 finished with value: 0.06515973061323166 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 64}. Best is trial 5 with value: 0.03481818735599518.
[I 2025-10-17 10:19:14,670] Trial 7 finished with value: 0.09672553092241287 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 8}. Best is trial 5 with value: 0.03481818735599518.
[I 2025-10-17 10:19:16,156] Trial 8 finished with value: 0.04724446311593056 and parameters: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}. Best is trial 5 with value: 0.03481818735599518.
[I 2025-10-17 10:19:17,657] Trial 9 finished with value: 0.03585530072450638 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}. Best is trial 5 with value: 0.03481818735599518.
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}...
 PLS-Default rmse  [test: 23.7391], [val: 27.1658], (fold: 0, id: 1) - [yh8bkg]
 PLS-Default rmse  [test: 20.7819], [val: 23.6217], (fold: 1, id: 2) - [qjlmy5]
 PLS-Default rmse  [test: 20.8121], [val: 27.3341], (fold: 2, id: 3) - [fv2tlb]
 PLS-Default rmse  [test: 20.9138], [val: 25.1223], (avg, id: 4) - [x4zytw]
 PLS-Default rmse  [test: 20.8539], [val: 25.0483], (w_avg, id: 5) - [jaipwf]
 PLS-1_components rmse  [test: 20.6604], [val: 21.1310], (fold: 0, id: 1) - [26l8kz]
 PLS-1_components rmse  [test: 20.4279], [val: 22.7613], (fold: 1, id: 2) - [pilbg7]
 PLS-1_components rmse  [test: 20.6237], [val: 28.3503], (fold: 2, id: 3) - [m5exkj]
 PLS-1_components rmse  [test: 20.4301], [val: 23.2538], (avg, id: 4) - [3zwxdw]
 PLS-1_components rmse  [test: 20.4403], [val: 23.2503], (w_avg, id: 5) - [vdmzjl]
 PLS-4_components rmse  [test: 22.5848], [val: 21.7829], (fold: 0, id: 1) - [avg2wk]
 PLS-4_components rmse  [test: 22.6729], [val: 22.0039], (fold: 1, id: 2) - [fbajku]
 PLS-4_components rmse  [test: 21.1290], [val: 28.4800], (fold: 2, id: 3) - [psrbed]
 PLS-4_components rmse  [test: 21.6963], [val: 18.5467], (avg, id: 4) - [e1m6hd]
 PLS-4_components rmse  [test: 21.7894], [val: 18.4309], (w_avg, id: 5) - [1blwq5]
 PLS-7_components rmse  [test: 21.7771], [val: 19.9161], (fold: 0, id: 1) - [9bqaz9]
 PLS-7_components rmse  [test: 21.4510], [val: 20.5690], (fold: 1, id: 2) - [u25sx6]
 PLS-7_components rmse  [test: 20.4901], [val: 27.3306], (fold: 2, id: 3) - [8kqzbi]
 PLS-7_components rmse  [test: 20.6988], [val: 14.5113], (avg, id: 4) - [lw5miz]
 PLS-7_components rmse  [test: 20.7896], [val: 14.2515], (w_avg, id: 5) - [fnxsjf]
 PLS-10_components rmse  [test: 22.9235], [val: 20.3824], (fold: 0, id: 1) - [seb2z8]
 PLS-10_components rmse  [test: 22.4756], [val: 21.7248], (fold: 1, id: 2) - [7rhviq]
 PLS-10_components rmse  [test: 20.6860], [val: 27.3804], (fold: 2, id: 3) - [9gspwj]
 PLS-10_components rmse  [test: 21.3056], [val: 13.4484], (avg, id: 4) - [k12ngs]
 PLS-10_components rmse  [test: 21.4389], [val: 13.1647], (w_avg, id: 5) - [0zvzrh]
 PLS-13_components rmse  [test: 22.9428], [val: 20.5484], (fold: 0, id: 1) - [tk0o98]
 PLS-13_components rmse  [test: 22.3585], [val: 21.9062], (fold: 1, id: 2) - [il5lhz]
 PLS-13_components rmse  [test: 20.6225], [val: 27.4400], (fold: 2, id: 3) - [feb0ce]
 PLS-13_components rmse  [test: 21.2266], [val: 13.4658], (avg, id: 4) - [bclfp8]
 PLS-13_components rmse  [test: 21.3629], [val: 13.1837], (w_avg, id: 5) - [kmpg64]
 PLS-16_components rmse  [test: 22.9421], [val: 20.5774], (fold: 0, id: 1) - [cxllpb]
 PLS-16_components rmse  [test: 22.3873], [val: 21.8948], (fold: 1, id: 2) - [kpxike]
 PLS-16_components rmse  [test: 20.6448], [val: 27.4296], (fold: 2, id: 3) - [x4pmin]
 PLS-16_components rmse  [test: 21.2425], [val: 13.4620], (avg, id: 4) - [58892v]
 PLS-16_components rmse  [test: 21.3775], [val: 13.1813], (w_avg, id: 5) - [2v6hta]
 PLS-19_components rmse  [test: 22.9358], [val: 20.5725], (fold: 0, id: 1) - [2yzk8k]
 PLS-19_components rmse  [test: 22.3887], [val: 21.8943], (fold: 1, id: 2) - [216fsl]
 PLS-19_components rmse  [test: 20.6443], [val: 27.4239], (fold: 2, id: 3) - [enos9x]
 PLS-19_components rmse  [test: 21.2399], [val: 13.4596], (avg, id: 4) - [a3znz8]
 PLS-19_components rmse  [test: 21.3746], [val: 13.1791], (w_avg, id: 5) - [vs5mhm]
 PLS-22_components rmse  [test: 22.9356], [val: 20.5727], (fold: 0, id: 1) - [6pi2bm]
 PLS-22_components rmse  [test: 22.3892], [val: 21.8938], (fold: 1, id: 2) - [yrdj7y]
 PLS-22_components rmse  [test: 20.6446], [val: 27.4231], (fold: 2, id: 3) - [glws75]
 PLS-22_components rmse  [test: 21.2400], [val: 13.4591], (avg, id: 4) - [fht94z]
 PLS-22_components rmse  [test: 21.3747], [val: 13.1786], (w_avg, id: 5) - [qb3xu7]
 PLS-25_components rmse  [test: 22.9356], [val: 20.5727], (fold: 0, id: 1) - [86b63u]
 PLS-25_components rmse  [test: 22.3891], [val: 21.8937], (fold: 1, id: 2) - [kkwxan]
 PLS-25_components rmse  [test: 20.6446], [val: 27.4233], (fold: 2, id: 3) - [hq8jiy]
 PLS-25_components rmse  [test: 21.2400], [val: 13.4591], (avg, id: 4) - [94k0nj]
 PLS-25_components rmse  [test: 21.3747], [val: 13.1786], (w_avg, id: 5) - [wdv7sn]
 PLS-28_components rmse  [test: 22.9356], [val: 20.5727], (fold: 0, id: 1) - [vbxw5v]
 PLS-28_components rmse  [test: 22.3891], [val: 21.8937], (fold: 1, id: 2) - [aiwczx]
 PLS-28_components rmse  [test: 20.6445], [val: 27.4233], (fold: 2, id: 3) - [ube00n]
 PLS-28_components rmse  [test: 21.2400], [val: 13.4591], (avg, id: 4) - [kl63eh]
 PLS-28_components rmse  [test: 21.3747], [val: 13.1786], (w_avg, id: 5) - [nbqtpl]
 Pipeline Best: PLS-10_components - rmse [test: 21.4389], [val: 13.1647], , (fold: w_avg, id: 5, step: 11) - [0zvzrh]
========================================================================================================================
 Starting pipeline Q1_finetune_bf09d6 on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0143
 Best parameters: {'n_components': 8}
 Best parameters: {'n_components': 8}
Training model PLS-Finetuned with: {'n_components': 8}...
Training model PLS-Finetuned with: {'n_components': 8}...
Training model PLS-Finetuned with: {'n_components': 8}...
 PLS-Finetuned rmse  [test: 13.4141], [val: 17.5028], (fold: 0, id: 1) - [82u1r0]
 PLS-Finetuned rmse  [test: 10.9976], [val: 17.0729], (fold: 1, id: 2) - [fbqpqh]
 PLS-Finetuned rmse  [test: 13.4853], [val: 16.7996], (fold: 2, id: 3) - [235z2g]
 PLS-Finetuned rmse  [test: 12.0550], [val: 12.8459], (avg, id: 4) - [gj34gl]
 PLS-Finetuned rmse  [test: 12.0524], [val: 12.8390], (w_avg, id: 5) - [7gxal6]
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0348
 Best parameters: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}
 Best parameters: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}
Training model PLS-Default with: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}...
 PLS-Default rmse  [test: 21.2012], [val: 21.0530], (fold: 0, id: 1) - [zsy06k]
 PLS-Default rmse  [test: 21.1070], [val: 26.4465], (fold: 1, id: 2) - [79fucc]
 PLS-Default rmse  [test: 20.9154], [val: 19.4823], (fold: 2, id: 3) - [q3ihkr]
 PLS-Default rmse  [test: 21.0339], [val: 22.2517], (avg, id: 4) - [a7zbpp]
 PLS-Default rmse  [test: 21.0258], [val: 22.2298], (w_avg, id: 5) - [bd8v6c]
 PLS-1_components rmse  [test: 20.4890], [val: 18.4036], (fold: 0, id: 1) - [nsv9aw]
 PLS-1_components rmse  [test: 20.8767], [val: 24.1194], (fold: 1, id: 2) - [dheyx2]
 PLS-1_components rmse  [test: 20.2132], [val: 17.8198], (fold: 2, id: 3) - [sdzyjg]
 PLS-1_components rmse  [test: 20.5097], [val: 20.0028], (avg, id: 4) - [bb1nd3]
 PLS-1_components rmse  [test: 20.4773], [val: 19.9895], (w_avg, id: 5) - [fp2ne0]
 PLS-4_components rmse  [test: 15.6083], [val: 19.5126], (fold: 0, id: 1) - [evaqsr]
 PLS-4_components rmse  [test: 16.0106], [val: 19.7528], (fold: 1, id: 2) - [oqnpsx]
 PLS-4_components rmse  [test: 14.5582], [val: 19.5338], (fold: 2, id: 3) - [vccsea]
 PLS-4_components rmse  [test: 15.2045], [val: 18.3508], (avg, id: 4) - [mh4kf8]
 PLS-4_components rmse  [test: 15.2023], [val: 18.3523], (w_avg, id: 5) - [zwtg3k]
 PLS-7_components rmse  [test: 13.6592], [val: 16.8505], (fold: 0, id: 1) - [prr3kp]
 PLS-7_components rmse  [test: 11.4561], [val: 17.0489], (fold: 1, id: 2) - [kab56q]
[I 2025-10-17 10:19:28,979] A new study created in memory with name: no-name-06f41f7c-a096-4458-9ea3-15782bfc5914
[I 2025-10-17 10:19:29,002] Trial 0 finished with value: 0.039875577060132394 and parameters: {'n_components': 1}. Best is trial 0 with value: 0.039875577060132394.
[I 2025-10-17 10:19:29,058] Trial 1 finished with value: 0.02394655512286355 and parameters: {'n_components': 8}. Best is trial 1 with value: 0.02394655512286355.
[I 2025-10-17 10:19:29,119] Trial 2 finished with value: 0.02394655512286355 and parameters: {'n_components': 8}. Best is trial 1 with value: 0.02394655512286355.
[I 2025-10-17 10:19:29,158] Trial 3 finished with value: 0.023637399240666258 and parameters: {'n_components': 4}. Best is trial 3 with value: 0.023637399240666258.
[I 2025-10-17 10:19:29,195] Trial 4 finished with value: 0.023637399240666258 and parameters: {'n_components': 4}. Best is trial 3 with value: 0.023637399240666258.
[I 2025-10-17 10:19:29,253] Trial 5 finished with value: 0.02394655512286355 and parameters: {'n_components': 8}. Best is trial 3 with value: 0.023637399240666258.
[I 2025-10-17 10:19:29,275] Trial 6 finished with value: 0.039875577060132394 and parameters: {'n_components': 1}. Best is trial 3 with value: 0.023637399240666258.
[I 2025-10-17 10:19:29,312] Trial 7 finished with value: 0.023637399240666258 and parameters: {'n_components': 4}. Best is trial 3 with value: 0.023637399240666258.
[I 2025-10-17 10:19:29,355] Trial 8 finished with value: 0.024816873690914815 and parameters: {'n_components': 5}. Best is trial 3 with value: 0.023637399240666258.
[I 2025-10-17 10:19:29,393] Trial 9 finished with value: 0.023637399240666258 and parameters: {'n_components': 4}. Best is trial 3 with value: 0.023637399240666258.
[I 2025-10-17 10:19:29,502] A new study created in memory with name: no-name-87d1bb21-53bb-405c-abac-733336a6a6e1
[I 2025-10-17 10:19:31,024] Trial 0 finished with value: 0.03936970606446266 and parameters: {'filters_1': 32, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:32,533] Trial 1 finished with value: 0.040262620896101 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:34,696] Trial 2 finished with value: 0.16750593483448029 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 64}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:36,226] Trial 3 finished with value: 0.13759596645832062 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 16}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:37,753] Trial 4 finished with value: 0.08892735838890076 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 32}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:39,259] Trial 5 finished with value: 0.1414806991815567 and parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:40,738] Trial 6 finished with value: 0.07982464134693146 and parameters: {'filters_1': 16, 'filters_2': 64, 'filters_3': 64}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:42,271] Trial 7 finished with value: 0.09960812330245972 and parameters: {'filters_1': 32, 'filters_2': 32, 'filters_3': 8}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:43,753] Trial 8 finished with value: 0.05790494754910469 and parameters: {'filters_1': 16, 'filters_2': 32, 'filters_3': 32}. Best is trial 0 with value: 0.03936970606446266.
[I 2025-10-17 10:19:45,251] Trial 9 finished with value: 0.03428523615002632 and parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}. Best is trial 9 with value: 0.03428523615002632.
 PLS-7_components rmse  [test: 13.5741], [val: 16.0725], (fold: 2, id: 3) - [qbvbxs]
 PLS-7_components rmse  [test: 12.4558], [val: 13.1097], (avg, id: 4) - [99gf3h]
 PLS-7_components rmse  [test: 12.4733], [val: 13.1205], (w_avg, id: 5) - [zvqc04]
 PLS-10_components rmse  [test: 13.8414], [val: 17.3294], (fold: 0, id: 1) - [wwx316]
 PLS-10_components rmse  [test: 11.7255], [val: 16.6355], (fold: 1, id: 2) - [cdxysd]
 PLS-10_components rmse  [test: 14.8685], [val: 16.9899], (fold: 2, id: 3) - [2hea5n]
 PLS-10_components rmse  [test: 12.6288], [val: 10.7964], (avg, id: 4) - [gndz07]
 PLS-10_components rmse  [test: 12.6097], [val: 10.7751], (w_avg, id: 5) - [iiwuwy]
 PLS-13_components rmse  [test: 14.5660], [val: 18.3012], (fold: 0, id: 1) - [mgf89u]
 PLS-13_components rmse  [test: 11.6755], [val: 18.0284], (fold: 1, id: 2) - [ns7zqr]
 PLS-13_components rmse  [test: 15.3954], [val: 17.2635], (fold: 2, id: 3) - [w3014h]
 PLS-13_components rmse  [test: 12.7572], [val: 10.0038], (avg, id: 4) - [pv5muc]
 PLS-13_components rmse  [test: 12.7780], [val: 9.9867], (w_avg, id: 5) - [m3jd34]
 PLS-16_components rmse  [test: 15.0399], [val: 18.7690], (fold: 0, id: 1) - [x0an8y]
 PLS-16_components rmse  [test: 12.3587], [val: 18.2016], (fold: 1, id: 2) - [i4mrbv]
 PLS-16_components rmse  [test: 16.0550], [val: 17.5727], (fold: 2, id: 3) - [vchpc4]
 PLS-16_components rmse  [test: 13.2659], [val: 9.7574], (avg, id: 4) - [if7s6b]
 PLS-16_components rmse  [test: 13.2793], [val: 9.7300], (w_avg, id: 5) - [lb9v04]
 PLS-19_components rmse  [test: 15.0973], [val: 18.5696], (fold: 0, id: 1) - [hzioql]
 PLS-19_components rmse  [test: 12.5670], [val: 18.2030], (fold: 1, id: 2) - [e632mp]
 PLS-19_components rmse  [test: 15.9602], [val: 17.6704], (fold: 2, id: 3) - [937rc6]
 PLS-19_components rmse  [test: 13.3325], [val: 9.5453], (avg, id: 4) - [cl379g]
 PLS-19_components rmse  [test: 13.3435], [val: 9.5279], (w_avg, id: 5) - [dgikuh]
 PLS-22_components rmse  [test: 15.1323], [val: 18.7029], (fold: 0, id: 1) - [fbruc2]
 PLS-22_components rmse  [test: 12.6262], [val: 18.2970], (fold: 1, id: 2) - [ooz9pc]
 PLS-22_components rmse  [test: 16.0113], [val: 17.7076], (fold: 2, id: 3) - [xa6gjd]
 PLS-22_components rmse  [test: 13.3896], [val: 9.5551], (avg, id: 4) - [r96cpj]
 PLS-22_components rmse  [test: 13.4024], [val: 9.5362], (w_avg, id: 5) - [c872bo]
 PLS-25_components rmse  [test: 15.1377], [val: 18.7440], (fold: 0, id: 1) - [dib5ok]
 PLS-25_components rmse  [test: 12.6449], [val: 18.2954], (fold: 1, id: 2) - [zy5g3q]
 PLS-25_components rmse  [test: 15.9873], [val: 17.7149], (fold: 2, id: 3) - [93gwhd]
 PLS-25_components rmse  [test: 13.3894], [val: 9.5494], (avg, id: 4) - [95tw6s]
 PLS-25_components rmse  [test: 13.4011], [val: 9.5286], (w_avg, id: 5) - [fqa9fs]
 PLS-28_components rmse  [test: 15.1372], [val: 18.7262], (fold: 0, id: 1) - [9802ok]
 PLS-28_components rmse  [test: 12.6439], [val: 18.2980], (fold: 1, id: 2) - [vfkgbe]
 PLS-28_components rmse  [test: 15.9828], [val: 17.7205], (fold: 2, id: 3) - [q0qiop]
 PLS-28_components rmse  [test: 13.3888], [val: 9.5500], (avg, id: 4) - [dcv8t2]
 PLS-28_components rmse  [test: 13.4005], [val: 9.5301], (w_avg, id: 5) - [ft2wpo]
 Pipeline Best: PLS-19_components - rmse [test: 13.3435], [val: 9.5279], , (fold: w_avg, id: 5, step: 14) - [dgikuh]
========================================================================================================================
 Starting pipeline Q1_finetune_9beccd on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0149
 Best parameters: {'n_components': 10}
 Best parameters: {'n_components': 10}
Training model PLS-Finetuned with: {'n_components': 10}...
Training model PLS-Finetuned with: {'n_components': 10}...
Training model PLS-Finetuned with: {'n_components': 10}...
 PLS-Finetuned rmse  [test: 13.0032], [val: 11.9982], (fold: 0, id: 1) - [vnn6ro]
 PLS-Finetuned rmse  [test: 13.1872], [val: 16.6294], (fold: 1, id: 2) - [q99zw0]
 PLS-Finetuned rmse  [test: 12.9258], [val: 14.8989], (fold: 2, id: 3) - [9htpai]
 PLS-Finetuned rmse  [test: 12.6088], [val: 8.5033], (avg, id: 4) - [ypsav9]
 PLS-Finetuned rmse  [test: 12.6095], [val: 8.3494], (w_avg, id: 5) - [05gamq]
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0348
 Best parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}
 Best parameters: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}...
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}...
Training model PLS-Default with: {'filters_1': 32, 'filters_2': 8, 'filters_3': 16}...
 PLS-Default rmse  [test: 20.7685], [val: 22.4454], (fold: 0, id: 1) - [jqxxsq]
 PLS-Default rmse  [test: 20.5219], [val: 29.2169], (fold: 1, id: 2) - [9e63sg]
 PLS-Default rmse  [test: 20.1263], [val: 22.5744], (fold: 2, id: 3) - [o72dsg]
 PLS-Default rmse  [test: 20.3061], [val: 23.3170], (avg, id: 4) - [mrsfhm]
 PLS-Default rmse  [test: 20.3093], [val: 23.1929], (w_avg, id: 5) - [w6id6f]
 PLS-1_components rmse  [test: 20.3453], [val: 22.4712], (fold: 0, id: 1) - [mnux4h]
 PLS-1_components rmse  [test: 19.7771], [val: 22.8517], (fold: 1, id: 2) - [big4pv]
 PLS-1_components rmse  [test: 20.3475], [val: 21.4959], (fold: 2, id: 3) - [rkamk2]
 PLS-1_components rmse  [test: 20.1245], [val: 21.4643], (avg, id: 4) - [7y6u1m]
 PLS-1_components rmse  [test: 20.1298], [val: 21.4676], (w_avg, id: 5) - [lgexv6]
 PLS-4_components rmse  [test: 14.3124], [val: 14.5220], (fold: 0, id: 1) - [50e5nl]
 PLS-4_components rmse  [test: 13.5938], [val: 19.2636], (fold: 1, id: 2) - [vssvey]
 PLS-4_components rmse  [test: 14.9033], [val: 16.4030], (fold: 2, id: 3) - [86w7oc]
 PLS-4_components rmse  [test: 14.1663], [val: 15.5894], (avg, id: 4) - [0jn7y8]
 PLS-4_components rmse  [test: 14.2090], [val: 15.5588], (w_avg, id: 5) - [egkx6c]
 PLS-7_components rmse  [test: 12.8357], [val: 12.6781], (fold: 0, id: 1) - [1sc7yn]
 PLS-7_components rmse  [test: 12.3694], [val: 16.4545], (fold: 1, id: 2) - [lk299f]
 PLS-7_components rmse  [test: 12.0209], [val: 14.3664], (fold: 2, id: 3) - [rvol4i]
 PLS-7_components rmse  [test: 12.1373], [val: 11.1541], (avg, id: 4) - [njtvwp]
 PLS-7_components rmse  [test: 12.1625], [val: 11.0989], (w_avg, id: 5) - [dtz9yp]
 PLS-10_components rmse  [test: 13.0032], [val: 11.9982], (fold: 0, id: 1) - [n5pvzp]
 PLS-10_components rmse  [test: 13.1872], [val: 16.6294], (fold: 1, id: 2) - [2wkplo]
 PLS-10_components rmse  [test: 12.9258], [val: 14.8989], (fold: 2, id: 3) - [laxwqg]
 PLS-10_components rmse  [test: 12.6088], [val: 8.5033], (avg, id: 4) - [3ggsp7]
 PLS-10_components rmse  [test: 12.6095], [val: 8.3494], (w_avg, id: 5) - [58mi74]
 PLS-13_components rmse  [test: 13.3009], [val: 12.5660], (fold: 0, id: 1) - [srl2jq]
 PLS-13_components rmse  [test: 13.4927], [val: 17.2008], (fold: 1, id: 2) - [lu9au0]
 PLS-13_components rmse  [test: 13.1400], [val: 15.6967], (fold: 2, id: 3) - [xz0r8m]
 PLS-13_components rmse  [test: 12.7071], [val: 8.0390], (avg, id: 4) - [fcukua]
 PLS-13_components rmse  [test: 12.7153], [val: 7.8539], (w_avg, id: 5) - [f8y2jm]
 PLS-16_components rmse  [test: 13.3403], [val: 12.6136], (fold: 0, id: 1) - [js9ao5]
 PLS-16_components rmse  [test: 13.5754], [val: 17.1651], (fold: 1, id: 2) - [u8ogvw]
 PLS-16_components rmse  [test: 13.2016], [val: 15.9837], (fold: 2, id: 3) - [pasu5p]
 PLS-16_components rmse  [test: 12.7464], [val: 7.9366], (avg, id: 4) - [ro6ko9]
 PLS-16_components rmse  [test: 12.7544], [val: 7.7450], (w_avg, id: 5) - [3nuu7r]
 PLS-19_components rmse  [test: 13.3037], [val: 12.6314], (fold: 0, id: 1) - [bckihq]
 PLS-19_components rmse  [test: 13.6289], [val: 17.1685], (fold: 1, id: 2) - [3ocotz]
 PLS-19_components rmse  [test: 13.2014], [val: 15.9995], (fold: 2, id: 3) - [xf247x]
 PLS-19_components rmse  [test: 12.7552], [val: 7.9271], (avg, id: 4) - [nqlb2y]
 PLS-19_components rmse  [test: 12.7595], [val: 7.7381], (w_avg, id: 5) - [abbaxf]
 PLS-22_components rmse  [test: 13.2920], [val: 12.6414], (fold: 0, id: 1) - [hh952b]
 PLS-22_components rmse  [test: 13.6355], [val: 17.1643], (fold: 1, id: 2) - [wwytqd]
 PLS-22_components rmse  [test: 13.1911], [val: 15.9920], (fold: 2, id: 3) - [8f3sd5]
 PLS-22_components rmse  [test: 12.7517], [val: 7.9315], (avg, id: 4) - [oubtvf]
 PLS-22_components rmse  [test: 12.7547], [val: 7.7443], (w_avg, id: 5) - [tctno2]
 PLS-25_components rmse  [test: 13.2913], [val: 12.6372], (fold: 0, id: 1) - [in4qmd]
 PLS-25_components rmse  [test: 13.6334], [val: 17.1629], (fold: 1, id: 2) - [3r268u]
 PLS-25_components rmse  [test: 13.1875], [val: 15.9935], (fold: 2, id: 3) - [o70f4r]
 PLS-25_components rmse  [test: 12.7501], [val: 7.9300], (avg, id: 4) - [kjtwe8]
 PLS-25_components rmse  [test: 12.7533], [val: 7.7423], (w_avg, id: 5) - [gzx0u6]
 PLS-28_components rmse  [test: 13.2913], [val: 12.6368], (fold: 0, id: 1) - [u6wrcq]
 PLS-28_components rmse  [test: 13.6330], [val: 17.1628], (fold: 1, id: 2) - [i6aphm]
 PLS-28_components rmse  [test: 13.1869], [val: 15.9938], (fold: 2, id: 3) - [40htd1]
 PLS-28_components rmse  [test: 12.7501], [val: 7.9302], (avg, id: 4) - [njfobf]
 PLS-28_components rmse  [test: 12.7533], [val: 7.7425], (w_avg, id: 5) - [e8e1qz]
 Pipeline Best: PLS-19_components - rmse [test: 12.7595], [val: 7.7381], , (fold: w_avg, id: 5, step: 14) - [abbaxf]
========================================================================================================================
 Starting pipeline Q1_finetune_14aa2c on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0236
 Best parameters: {'n_components': 4}
 Best parameters: {'n_components': 4}
Training model PLS-Finetuned with: {'n_components': 4}...
Training model PLS-Finetuned with: {'n_components': 4}...
Training model PLS-Finetuned with: {'n_components': 4}...
 PLS-Finetuned rmse  [test: 14.8941], [val: 22.6920], (fold: 0, id: 1) - [q9b4wb]
 PLS-Finetuned rmse  [test: 16.1105], [val: 22.2048], (fold: 1, id: 2) - [1f6h9b]
 PLS-Finetuned rmse  [test: 16.8057], [val: 24.4976], (fold: 2, id: 3) - [2kok1l]
 PLS-Finetuned rmse  [test: 15.6118], [val: 16.2913], (avg, id: 4) - [et07h5]
 PLS-Finetuned rmse  [test: 15.5909], [val: 16.2257], (w_avg, id: 5) - [90k8bt]
 Starting hyperparameter optimization:
   Strategy: single
   Eval mode: best
   Trials: 10
   Folds: 3
 Running optimization (10 trials)...
 Best score: 0.0343
 Best parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}
 Best parameters: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}
Training model PLS-Default with: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}...
Training model PLS-Default with: {'filters_1': 64, 'filters_2': 64, 'filters_3': 32}...
 PLS-Default rmse  [test: 20.7028], [val: 29.5695], (fold: 0, id: 1) - [l0apfm]
 PLS-Default rmse  [test: 21.8073], [val: 28.4370], (fold: 1, id: 2) - [10kask]
 PLS-Default rmse  [test: 24.0237], [val: 33.7512], (fold: 2, id: 3) - [51btv2]
 PLS-Default rmse  [test: 21.9350], [val: 30.4871], (avg, id: 4) - [gtblvw]
 PLS-Default rmse  [test: 21.8620], [val: 30.4816], (w_avg, id: 5) - [sqp2x9]
 PLS-1_components rmse  [test: 19.9620], [val: 28.9445], (fold: 0, id: 1) - [hwva1l]
 PLS-1_components rmse  [test: 20.4792], [val: 29.2260], (fold: 1, id: 2) - [3yggtj]
 PLS-1_components rmse  [test: 20.6050], [val: 33.5155], (fold: 2, id: 3) - [7d2aj2]
 PLS-1_components rmse  [test: 20.3107], [val: 29.9792], (avg, id: 4) - [cpwblk]
 PLS-1_components rmse  [test: 20.2984], [val: 29.9447], (w_avg, id: 5) - [s3tvsu]
 PLS-4_components rmse  [test: 14.8941], [val: 22.6920], (fold: 0, id: 1) - [ck8qbr]
 PLS-4_components rmse  [test: 16.1105], [val: 22.2048], (fold: 1, id: 2) - [710a93]
 PLS-4_components rmse  [test: 16.8057], [val: 24.4976], (fold: 2, id: 3) - [vr53kp]
 PLS-4_components rmse  [test: 15.6118], [val: 16.2913], (avg, id: 4) - [uohipc]
 PLS-4_components rmse  [test: 15.5909], [val: 16.2257], (w_avg, id: 5) - [tqsakq]
 PLS-7_components rmse  [test: 15.2207], [val: 22.2632], (fold: 0, id: 1) - [npm4tk]
 PLS-7_components rmse  [test: 16.6013], [val: 22.6022], (fold: 1, id: 2) - [1iwvu9]
 PLS-7_components rmse  [test: 17.4439], [val: 25.2182], (fold: 2, id: 3) - [non607]
 PLS-7_components rmse  [test: 16.0116], [val: 14.5329], (avg, id: 4) - [82ftms]
 PLS-7_components rmse  [test: 15.9641], [val: 14.3880], (w_avg, id: 5) - [ckh4d0]
 PLS-10_components rmse  [test: 15.0235], [val: 22.5043], (fold: 0, id: 1) - [o4vry0]
 PLS-10_components rmse  [test: 16.3586], [val: 22.3137], (fold: 1, id: 2) - [qr4m61]
 PLS-10_components rmse  [test: 17.3781], [val: 24.9575], (fold: 2, id: 3) - [755x9l]
 PLS-10_components rmse  [test: 15.7817], [val: 14.1324], (avg, id: 4) - [6z7qsm]
 PLS-10_components rmse  [test: 15.7397], [val: 14.0081], (w_avg, id: 5) - [kwgmkm]
 PLS-13_components rmse  [test: 14.9982], [val: 22.4949], (fold: 0, id: 1) - [qg2e5u]
 PLS-13_components rmse  [test: 16.3330], [val: 22.3040], (fold: 1, id: 2) - [5182p6]
 PLS-13_components rmse  [test: 17.3931], [val: 24.9630], (fold: 2, id: 3) - [e64qxc]
 PLS-13_components rmse  [test: 15.7703], [val: 14.1486], (avg, id: 4) - [8dxaky]
 PLS-13_components rmse  [test: 15.7271], [val: 14.0239], (w_avg, id: 5) - [voegbv]
 PLS-16_components rmse  [test: 15.0010], [val: 22.4928], (fold: 0, id: 1) - [yl2jez]
 PLS-16_components rmse  [test: 16.3360], [val: 22.3025], (fold: 1, id: 2) - [rj6wna]
 PLS-16_components rmse  [test: 17.3882], [val: 24.9531], (fold: 2, id: 3) - [8gwuqa]
 PLS-16_components rmse  [test: 15.7713], [val: 14.1460], (avg, id: 4) - [wz5386]
 PLS-16_components rmse  [test: 15.7284], [val: 14.0218], (w_avg, id: 5) - [qux57q]
 PLS-19_components rmse  [test: 15.0005], [val: 22.4926], (fold: 0, id: 1) - [fd67u0]
 PLS-19_components rmse  [test: 16.3359], [val: 22.3016], (fold: 1, id: 2) - [dnx6ps]
 PLS-19_components rmse  [test: 17.3878], [val: 24.9520], (fold: 2, id: 3) - [fwd9rz]
 PLS-19_components rmse  [test: 15.7710], [val: 14.1454], (avg, id: 4) - [tboil5]
 PLS-19_components rmse  [test: 15.7281], [val: 14.0212], (w_avg, id: 5) - [arhuyf]
 PLS-22_components rmse  [test: 15.0005], [val: 22.4926], (fold: 0, id: 1) - [70ebg6]
 PLS-22_components rmse  [test: 16.3360], [val: 22.3016], (fold: 1, id: 2) - [1uba9h]
 PLS-22_components rmse  [test: 17.3879], [val: 24.9518], (fold: 2, id: 3) - [l7jqdl]
 PLS-22_components rmse  [test: 15.7711], [val: 14.1454], (avg, id: 4) - [wts2q4]
 PLS-22_components rmse  [test: 15.7282], [val: 14.0212], (w_avg, id: 5) - [kbub04]
 PLS-25_components rmse  [test: 15.0005], [val: 22.4926], (fold: 0, id: 1) - [fz0lo4]
 PLS-25_components rmse  [test: 16.3360], [val: 22.3016], (fold: 1, id: 2) - [74qygg]
 PLS-25_components rmse  [test: 17.3879], [val: 24.9518], (fold: 2, id: 3) - [x22h5c]
 PLS-25_components rmse  [test: 15.7711], [val: 14.1454], (avg, id: 4) - [wdqlx6]
 PLS-25_components rmse  [test: 15.7282], [val: 14.0212], (w_avg, id: 5) - [urxtzp]
 PLS-28_components rmse  [test: 15.0005], [val: 22.4926], (fold: 0, id: 1) - [otkaan]
 PLS-28_components rmse  [test: 16.3360], [val: 22.3016], (fold: 1, id: 2) - [34znzw]
 PLS-28_components rmse  [test: 17.3879], [val: 24.9518], (fold: 2, id: 3) - [uy5pz6]
 PLS-28_components rmse  [test: 15.7711], [val: 14.1454], (avg, id: 4) - [1xkzjm]
 PLS-28_components rmse  [test: 15.7282], [val: 14.0212], (w_avg, id: 5) - [wycdqo]
 Pipeline Best: PLS-10_components - rmse [test: 15.7397], [val: 14.0081], , (fold: w_avg, id: 5, step: 11) - [kwgmkm]
========================================================================================================================
 Best prediction in run for dataset 'regression': PLS-13_components - rmse [test: 13.0337], [val: 6.4349], , (fold: avg, id: 4, step: 12) - [td5soe] | [Q1_finetune_07fc8f]
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max     | SD     | CV     | R     | RMSE   | MSE     | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
| Cros Val | 99      | 4302     | 30.105 | 25.660 | 2.050  | 111.200 | 21.771 | 0.723  | 0.913  | 6.435  | 41.408  | 6.428  | 4.727  | 3.39   | -0.289 | 100.0       |
| Train    | 130     | 4302     | 30.333 | 23.140 | 2.050  | 128.310 | 23.575 | 0.777  | 0.967  | 4.313  | 18.605  | 4.313  | 2.536  | 5.47   | 0.069  | 100.0       |
| Test     | 59      | 4302     | 31.762 | 27.110 | 1.330  | 84.570  | 19.805 | 0.624  | 0.567  | 13.034 | 169.876 | 13.019 | 10.275 | 1.52   | -0.621 | 84.7        |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
========================================================================================================================
Top 5 models by rmse:
1. PLS-13_components - rmse [test: 13.0337], [val: 6.4349], , (fold: avg, id: 4, step: 12) - [td5soe] - MinMax|MinMax>2ndDer>SG
2. PLS-13_components - rmse [test: 13.0273], [val: 6.4501], , (fold: w_avg, id: 5, step: 12) - [oep39e] - MinMax|MinMax>2ndDer>SG
3. PLS-25_components - rmse [test: 13.0257], [val: 6.4506], , (fold: avg, id: 4, step: 16) - [wzkpq8] - MinMax|MinMax>2ndDer>SG
4. PLS-28_components - rmse [test: 13.0257], [val: 6.4506], , (fold: avg, id: 4, step: 17) - [c53b66] - MinMax|MinMax>2ndDer>SG
5. PLS-22_components - rmse [test: 13.0257], [val: 6.4506], , (fold: avg, id: 4, step: 15) - [etn1ss] - MinMax|MinMax>2ndDer>SG
PredictionResult(id=td5soe, model=PLS-13_components, dataset=regression, fold=avg, step=12, op=4)
########################################
Finished running: Q3_finetune.py
########################################
Launch: Q4_multidatasets.py
########################################
2025-10-17 10:20:19.369832: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:20:20.032275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 Running pipeline with spinner enabled - watch for loading animations during model training!
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 3 dataset (3 total runs).
========================================================================================================================
 Starting pipeline Q3_0f8b1a on dataset regression
------------------------------------------------------------------------------------------------------------------------
 PLSRegression rmse  [test: 13.3695], [val: 16.5236], (fold: 0, id: 1) - [ev2mxd]
 PLSRegression rmse  [test: 14.0708], [val: 13.7301], (fold: 1, id: 2) - [jpdx0n]
 PLSRegression rmse  [test: 12.8133], [val: 16.3085], (fold: 2, id: 3) - [iy3jj7]
 PLSRegression rmse  [test: 13.2178], [val: 5.7550], (avg, id: 4) - [9hm16x]
 PLSRegression rmse  [test: 13.2598], [val: 5.6937], (w_avg, id: 5) - [pnd33x]
 ElasticNet rmse  [test: 19.8394], [val: 19.4769], (fold: 0, id: 1) - [u8z9m8]
 ElasticNet rmse  [test: 19.8074], [val: 19.2026], (fold: 1, id: 2) - [ogudkj]
 ElasticNet rmse  [test: 19.8502], [val: 19.4475], (fold: 2, id: 3) - [oljqmv]
 ElasticNet rmse  [test: 19.8181], [val: 19.0976], (avg, id: 4) - [6973mi]
 ElasticNet rmse  [test: 19.8179], [val: 19.0993], (w_avg, id: 5) - [xt8plv]
 Pipeline Best: PLSRegression - rmse [test: 13.2598], [val: 5.6937], , (fold: w_avg, id: 5, step: 7) - [pnd33x]
========================================================================================================================
 Best prediction in run for dataset 'regression': PLSRegression - rmse [test: 13.2598], [val: 5.6937], , (fold: w_avg, id: 5, step: 7) - [pnd33x] | [Q3_0f8b1a]
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max     | SD     | CV     | R     | RMSE   | MSE     | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
| Cros Val | 39      | 21510    | 24.040 | 19.150 | 2.050  | 71.240  | 17.771 | 0.739  | 0.897  | 5.694  | 32.418  | 5.323  | 4.693  | 3.34   | 2.020  | 100.0       |
| Train    | 130     | 21510    | 30.333 | 23.140 | 2.050  | 128.310 | 23.575 | 0.777  | 0.985  | 2.927  | 8.569   | 2.897  | 1.454  | 8.14   | 0.418  | 100.0       |
| Test     | 59      | 21510    | 31.762 | 27.110 | 1.330  | 84.570  | 19.805 | 0.624  | 0.552  | 13.260 | 175.823 | 13.237 | 10.230 | 1.50   | 0.781  | 84.7        |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
 Saved predictions to results\regression\10-17_10h2027s_Best_prediction_run_Q3_0f8b1a_PLSRegression_[pnd33x].csv
========================================================================================================================
 Starting pipeline Q3_0f8b1a on dataset regression_2
------------------------------------------------------------------------------------------------------------------------
 ShuffleSplit has 'groups' parameter but no 'group' specified. Using default: 'Circuit'
 PLSRegression rmse  [test: 0.3123], [val: 0.2052], (fold: 0, id: 1) - [mrzns5]
 PLSRegression rmse  [test: 0.3161], [val: 0.2192], (fold: 1, id: 2) - [knjwhn]
 PLSRegression rmse  [test: 0.3152], [val: 0.1742], (fold: 2, id: 3) - [rvvjyr]
 PLSRegression rmse  [test: 0.3139], [val: 0.1787], (avg, id: 4) - [ssw3x9]
 PLSRegression rmse  [test: 0.3139], [val: 0.1783], (w_avg, id: 5) - [kykk96]
 ElasticNet rmse  [test: 2.3016], [val: 1.1883], (fold: 0, id: 1) - [p3k1co]
 ElasticNet rmse  [test: 2.2990], [val: 1.2203], (fold: 1, id: 2) - [4ukz6l]
 ElasticNet rmse  [test: 2.2958], [val: 1.2430], (fold: 2, id: 3) - [54u7ow]
 ElasticNet rmse  [test: 2.2988], [val: 1.2171], (avg, id: 4) - [pcb9he]
 ElasticNet rmse  [test: 2.2988], [val: 1.2171], (w_avg, id: 5) - [34hliq]
 Pipeline Best: PLSRegression - rmse [test: 0.3152], [val: 0.1742], , (fold: 2, id: 3, step: 7) - [rvvjyr]
========================================================================================================================
 Best prediction in run for dataset 'regression_2': PLSRegression - rmse [test: 0.3152], [val: 0.1742], , (fold: 2, id: 3, step: 7) - [rvvjyr] | [Q3_0f8b1a]
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max    | SD     | CV     | R     | RMSE   | MSE    | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
| Cros Val | 89      | 1250     | 12.616 | 12.800 | 10.200 | 16.700 | 1.240  | 0.098  | 0.980  | 0.174  | 0.030  | 0.174  | 0.138  | 7.13   | -0.010 | 100.0       |
| Train    | 796     | 1250     | 12.535 | 12.800 | 9.500  | 16.700 | 1.289  | 0.103  | 0.984  | 0.163  | 0.027  | 0.163  | 0.129  | 7.88   | -0.000 | 100.0       |
| Test     | 150     | 1250     | 11.677 | 11.500 | 8.400  | 17.600 | 2.129  | 0.182  | 0.978  | 0.315  | 0.099  | 0.226  | 0.266  | 9.43   | 0.220  | 100.0       |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
 Saved predictions to results\regression_2\10-17_10h2037s_Best_prediction_run_Q3_0f8b1a_PLSRegression_[rvvjyr].csv
========================================================================================================================
 Starting pipeline Q3_0f8b1a on dataset regression_3
------------------------------------------------------------------------------------------------------------------------
 ShuffleSplit has 'groups' parameter but no 'group' specified. Using default: 'Circuit'
 PLSRegression rmse  [test: 0.6173], [val: 0.6907], (fold: 0, id: 1) - [q43c3k]
 PLSRegression rmse  [test: 0.6434], [val: 0.5449], (fold: 1, id: 2) - [r2x5zl]
 PLSRegression rmse  [test: 0.6260], [val: 0.4827], (fold: 2, id: 3) - [am69k4]
 PLSRegression rmse  [test: 0.6257], [val: 0.4934], (avg, id: 4) - [axayzq]
 PLSRegression rmse  [test: 0.6265], [val: 0.4905], (w_avg, id: 5) - [cco19o]
 ElasticNet rmse  [test: 2.2966], [val: 1.3743], (fold: 0, id: 1) - [qplx6w]
 ElasticNet rmse  [test: 2.3088], [val: 1.4010], (fold: 1, id: 2) - [s3rbg1]
 ElasticNet rmse  [test: 2.2866], [val: 1.2584], (fold: 2, id: 3) - [s9fdzh]
 ElasticNet rmse  [test: 2.2973], [val: 1.3419], (avg, id: 4) - [xgu6an]
 ElasticNet rmse  [test: 2.2969], [val: 1.3419], (w_avg, id: 5) - [993t69]
 Pipeline Best: PLSRegression - rmse [test: 0.6260], [val: 0.4827], , (fold: 2, id: 3, step: 7) - [am69k4]
========================================================================================================================
 Best prediction in run for dataset 'regression_3': PLSRegression - rmse [test: 0.6260], [val: 0.4827], , (fold: 2, id: 3, step: 7) - [am69k4] | [Q3_0f8b1a]
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max    | SD     | CV     | R     | RMSE   | MSE    | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
| Cros Val | 89      | 1280     | 12.787 | 13.000 | 9.600  | 15.300 | 1.228  | 0.096  | 0.845  | 0.483  | 0.233  | 0.482  | 0.389  | 2.55   | -0.030 | 100.0       |
| Train    | 801     | 1280     | 12.511 | 12.800 | 9.500  | 16.700 | 1.285  | 0.103  | 0.884  | 0.437  | 0.191  | 0.437  | 0.344  | 2.94   | -0.000 | 99.8        |
| Test     | 150     | 1280     | 11.677 | 11.500 | 8.400  | 17.600 | 2.129  | 0.182  | 0.914  | 0.626  | 0.392  | 0.599  | 0.496  | 3.55   | 0.180  | 100.0       |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
 Saved predictions to results\regression_3\10-17_10h2048s_Best_prediction_run_Q3_0f8b1a_PLSRegression_[am69k4].csv
========================================================================================================================
Dataset: name=regression, number of predictions=30
Top 4 models by RMSE:
1. PLSRegression - rmse [test: 13.2598], [val: 5.6937], [r2:0.8974], [mae:4.6928], (fold: w_avg, id: 5, step: 7) - [pnd33x] | [Q3_0f8b1a]
2. PLSRegression - rmse [test: 13.2178], [val: 5.7550], [r2:0.8951], [mae:4.7326], (fold: avg, id: 4, step: 7) - [9hm16x] | [Q3_0f8b1a]
3. PLSRegression - rmse [test: 14.0708], [val: 13.7301], [r2:-2.1346], [mae:10.5863], (fold: 1, id: 2, step: 7) - [jpdx0n] | [Q3_0f8b1a]
4. PLSRegression - rmse [test: 12.8133], [val: 16.3085], [r2:0.2954], [mae:11.5367], (fold: 2, id: 3, step: 7) - [iy3jj7] | [Q3_0f8b1a]
PredictionResult(id=pnd33x, model=PLSRegression, dataset=regression, fold=w_avg, step=7, op=5)
Dataset: name=regression_2, number of predictions=30
Top 4 models by RMSE:
1. PLSRegression - rmse [test: 0.3152], [val: 0.1742], [r2:0.9803], [mae:0.1376], (fold: 2, id: 3, step: 7) - [rvvjyr] | [Q3_0f8b1a]
2. PLSRegression - rmse [test: 0.3139], [val: 0.1783], [r2:0.9785], [mae:0.1382], (fold: w_avg, id: 5, step: 7) - [kykk96] | [Q3_0f8b1a]
3. PLSRegression - rmse [test: 0.3139], [val: 0.1787], [r2:0.9784], [mae:0.1383], (fold: avg, id: 4, step: 7) - [ssw3x9] | [Q3_0f8b1a]
4. PLSRegression - rmse [test: 0.3123], [val: 0.2052], [r2:0.9701], [mae:0.1594], (fold: 0, id: 1, step: 7) - [mrzns5] | [Q3_0f8b1a]
PredictionResult(id=rvvjyr, model=PLSRegression, dataset=regression_2, fold=2, step=7, op=3)
Dataset: name=regression_3, number of predictions=30
Top 4 models by RMSE:
1. PLSRegression - rmse [test: 0.6260], [val: 0.4827], [r2:0.8454], [mae:0.3895], (fold: 2, id: 3, step: 7) - [am69k4] | [Q3_0f8b1a]
2. PLSRegression - rmse [test: 0.6265], [val: 0.4905], [r2:0.8664], [mae:0.3839], (fold: w_avg, id: 5, step: 7) - [cco19o] | [Q3_0f8b1a]
3. PLSRegression - rmse [test: 0.6257], [val: 0.4934], [r2:0.8648], [mae:0.3845], (fold: avg, id: 4, step: 7) - [axayzq] | [Q3_0f8b1a]
4. PLSRegression - rmse [test: 0.6434], [val: 0.5449], [r2:0.8407], [mae:0.4381], (fold: 1, id: 2, step: 7) - [r2x5zl] | [Q3_0f8b1a]
PredictionResult(id=am69k4, model=PLSRegression, dataset=regression_3, fold=2, step=7, op=3)
########################################
Finished running: Q4_multidatasets.py
########################################
Launch: Q5_predict.py
########################################
2025-10-17 10:20:51.812715: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:20:52.509214: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline config_991798 on dataset regression_2
------------------------------------------------------------------------------------------------------------------------
 RepeatedKFold has 'groups' parameter but no 'group' specified. Using default: 'Circuit'
 Q4_PLS_10 rmse  [test: 0.3550], [val: 0.1940], (fold: 0, id: 1) - [v4bueu]
 Q4_PLS_10 rmse  [test: 0.3356], [val: 0.2103], (fold: 1, id: 2) - [g7r3wa]
 Q4_PLS_10 rmse  [test: 0.3361], [val: 0.2237], (fold: 2, id: 3) - [qkj3t8]
 Q4_PLS_10 rmse  [test: 0.3642], [val: 0.1965], (fold: 3, id: 4) - [ub1lsb]
 Q4_PLS_10 rmse  [test: 0.3407], [val: 0.1944], (fold: 4, id: 5) - [8vfudb]
 Q4_PLS_10 rmse  [test: 0.3338], [val: 0.1932], (fold: 5, id: 6) - [pd7aid]
 Q4_PLS_10 rmse  [test: 0.3410], [val: 0.1789], (avg, id: 7) - [dvwv71]
 Q4_PLS_10 rmse  [test: 0.3413], [val: 0.1789], (w_avg, id: 8) - [f9cawp]
 Q4_PLS_20 rmse  [test: 0.2896], [val: 0.1844], (fold: 0, id: 1) - [dggtg1]
 Q4_PLS_20 rmse  [test: 0.2905], [val: 0.1983], (fold: 1, id: 2) - [1og1tb]
 Q4_PLS_20 rmse  [test: 0.3066], [val: 0.1955], (fold: 2, id: 3) - [2kv0hr]
 Q4_PLS_20 rmse  [test: 0.3170], [val: 0.2790], (fold: 3, id: 4) - [coeyji]
 Q4_PLS_20 rmse  [test: 0.2787], [val: 0.1835], (fold: 4, id: 5) - [3jfk1r]
 Q4_PLS_20 rmse  [test: 0.3004], [val: 0.1867], (fold: 5, id: 6) - [8zfw58]
 Q4_PLS_20 rmse  [test: 0.2910], [val: 0.1563], (avg, id: 7) - [4d1nfq]
 Q4_PLS_20 rmse  [test: 0.2901], [val: 0.1558], (w_avg, id: 8) - [gtbssr]
 GradientBoostingRegressor rmse  [test: 0.8586], [val: 0.3548], (fold: 0, id: 1) - [mhykth]
 GradientBoostingRegressor rmse  [test: 0.9032], [val: 0.3880], (fold: 1, id: 2) - [9kssya]
 GradientBoostingRegressor rmse  [test: 0.8936], [val: 0.4077], (fold: 2, id: 3) - [12cdb0]
 GradientBoostingRegressor rmse  [test: 0.8883], [val: 0.3771], (fold: 3, id: 4) - [2uh7qp]
 GradientBoostingRegressor rmse  [test: 0.9301], [val: 0.3926], (fold: 4, id: 5) - [ysrhlk]
 GradientBoostingRegressor rmse  [test: 0.8791], [val: 0.3545], (fold: 5, id: 6) - [mqwriq]
 GradientBoostingRegressor rmse  [test: 0.8855], [val: 0.3274], (avg, id: 7) - [uft8qd]
 GradientBoostingRegressor rmse  [test: 0.8848], [val: 0.3271], (w_avg, id: 8) - [b99825]
 Pipeline Best: Q4_PLS_20 - rmse [test: 0.2901], [val: 0.1558], , (fold: w_avg, id: 8, step: 6) - [gtbssr]
========================================================================================================================
 Best prediction in run for dataset 'regression_2': Q4_PLS_20 - rmse [test: 0.2901], [val: 0.1558], , (fold: w_avg, id: 8, step: 6) - [gtbssr] | [config_991798]
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max    | SD     | CV     | R     | RMSE   | MSE    | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
| Cros Val | 1770    | 625      | 12.544 | 12.800 | 9.500  | 16.700 | 1.284  | 0.102  | 0.985  | 0.156  | 0.024  | 0.156  | 0.122  | 8.24   | -0.001 | 100.0       |
| Train    | 885     | 625      | 12.544 | 12.800 | 9.500  | 16.700 | 1.284  | 0.102  | 0.985  | 0.156  | 0.024  | 0.156  | 0.122  | 8.24   | -0.001 | 100.0       |
| Test     | 150     | 625      | 11.677 | 11.500 | 8.400  | 17.600 | 2.129  | 0.182  | 0.981  | 0.290  | 0.084  | 0.228  | 0.238  | 9.34   | 0.179  | 100.0       |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
 Saved predictions to results\regression_2\10-17_10h2104s_Best_prediction_run_config_991798_Q4_PLS_20_[gtbssr].csv
========================================================================================================================
=== Q4 - Model Persistence and Prediction Example ===
--- Source Model ---
Best model: Q4_PLS_20 (id: gtbssr)
Reference predictions: [10.93159942 10.60388579 10.94778361 10.58828592 10.59373705]
--------------------------------------------------------------------------------
--- Method 1: Predict with a prediction entry ---
========================================================================================================================
 Starting Nirs4all prediction(s)
========================================================================================================================
 Starting pipeline prediction on dataset regression_2_Xtest
------------------------------------------------------------------------------------------------------------------------
 Predicted with: Q4_PLS_20 [hbva6a]
 Saved predictions to results\regression_2_Xtest\Predict_[hbva6a].csv
Method 1 predictions: [10.93159942 10.60388579 10.94778361 10.58828592 10.59373705]
Method 1 identical to training:  YES
================================================================================
--- Method 2: Predict with a model ID ---
Using model ID: [gtbssr]
========================================================================================================================
 Starting Nirs4all prediction(s)
========================================================================================================================
 Starting pipeline prediction on dataset regression_2_Xtest
------------------------------------------------------------------------------------------------------------------------
 Predicted with: Q4_PLS_20 [hbva6a]
 Saved predictions to results\regression_2_Xtest\Predict_[hbva6a].csv
Method 2 predictions: [10.93159942 10.60388579 10.94778361 10.58828592 10.59373705]
Method 2 identical to training:  YES
--- Method 3: Predict with a model ID and return all predictions ---
========================================================================================================================
 Starting Nirs4all prediction(s)
========================================================================================================================
 Starting pipeline prediction on dataset regression_2_Xtest
------------------------------------------------------------------------------------------------------------------------
########################################
Finished running: Q5_predict.py
########################################
Launch: Q5_predict_NN.py
########################################
2025-10-17 10:21:10.593816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:21:11.260503: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:21:13.626592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 1, 2151), processings=['raw'], min=-0.265, max=1.436, mean=0.466, var=0.149)
Targets: (samples=189, targets=1, processings=['numeric'])
- numeric: min=1.33, max=128.31, mean=30.779
Indexes:
- partition - "train", processings - ['raw']: 130 samples
- partition - "test", processings - ['raw']: 59 samples
 Starting pipeline config_633c70 on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Step 1: {'class': 'sklearn.preprocessing._data.MinMaxScaler', '_runtime_instance': MinMaxScaler()}
 Executing controller TransformerMixinController with operator MinMaxScaler
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.0, max=1.128, mean=0.801, var=0.033)
Targets: (samples=189, targets=1, processings=['numeric'])
- numeric: min=1.33, max=128.31, mean=30.779
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1']: 59 samples
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'y_processing': {'class': 'sklearn.preprocessing._data.MinMaxScaler'}}
 Executing controller YTransformerMixinController with operator MinMaxScaler
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.0, max=1.128, mean=0.801, var=0.033)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=-0.006, max=1.0, mean=0.228
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1']: 59 samples
------------------------------------------------------------------------------------------------------------------------
 Step 3: {'feature_augmentation': [{'class': 'nirs4all.operators.transformations.scalers.StandardNormalVariate', '_runtime_instance': StandardNormalVariate()}, {'class': 'nirs4all.operators.transformations.nirs.SavitzkyGolay', '_runtime_instance': SavitzkyGolay()}, {'class': 'nirs4all.operators.transformations.signal.Gaussian', '_runtime_instance': Gaussian()}, {'class': 'nirs4all.operators.transformations.nirs.Haar', '_runtime_instance': Haar()}]}
 Executing controller FeatureAugmentationController with operator list
    Sub-step 3.1: {'class': 'nirs4all.operators.transformations.scalers.StandardNormalVariate', '_runtime_instance': StandardNormalVariate()}
 Executing controller TransformerMixinController with operator StandardNormalVariate
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 2, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1'], min=-5.548, max=7.24, mean=0.401, var=0.677)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=-0.006, max=1.0, mean=0.228
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1']: 59 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 3.2: {'class': 'nirs4all.operators.transformations.nirs.SavitzkyGolay', '_runtime_instance': SavitzkyGolay()}
 Executing controller TransformerMixinController with operator SavitzkyGolay
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2'], min=-5.548, max=7.24, mean=0.534, var=0.498)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=-0.006, max=1.0, mean=0.228
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2']: 59 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 3.3: {'class': 'nirs4all.operators.transformations.signal.Gaussian', '_runtime_instance': Gaussian()}
 Executing controller TransformerMixinController with operator Gaussian
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 4, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3'], min=-5.548, max=7.24, mean=0.401, var=0.427)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=-0.006, max=1.0, mean=0.228
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3']: 59 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 3.4: {'class': 'nirs4all.operators.transformations.nirs.Haar', '_runtime_instance': Haar()}
 Executing controller TransformerMixinController with operator Haar
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 5, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4'], min=-5.548, max=7.24, mean=0.321, var=0.367)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=-0.006, max=1.0, mean=0.228
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 59 samples
------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 5, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4'], min=-5.548, max=7.24, mean=0.321, var=0.367)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=-0.006, max=1.0, mean=0.228
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 59 samples
------------------------------------------------------------------------------------------------------------------------
 Step 4: {'class': 'sklearn.model_selection._split.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1, 'random_state': 42}, '_runtime_instance': RepeatedKFold(n_repeats=1, n_splits=3, random_state=42)}
 Executing controller CrossValidatorController with operator RepeatedKFold
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 5, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4'], min=-5.548, max=7.24, mean=0.321, var=0.367)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=-0.006, max=1.0, mean=0.228
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 59 samples
Folds: [(86, 44), (87, 43), (87, 43)]
------------------------------------------------------------------------------------------------------------------------
 Step 5: {'function': 'nirs4all.operators.models.cirad_tf.nicon', '_runtime_instance': <function nicon at 0x0000025C65EEE290>}
 Executing controller TensorFlowModelController with operator function
 Model config: {'model_instance': <function nicon at 0x0000025C65EEE290>}
Epoch 1/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m1s[0m 922ms/step - loss: 0.0842 - mae: 0.2465 - mse: 0.0842
[1m3/3[0m [32m====================[0m[37m[0m [1m1s[0m 86ms/step - loss: 0.0701 - mae: 0.2244 - mse: 0.0701 - val_loss: 0.0561 - val_mae: 0.2161 - val_mse: 0.0561
Epoch 2/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 26ms/step - loss: 0.0568 - mae: 0.1718 - mse: 0.0568
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0435 - mae: 0.1466 - mse: 0.0435 - val_loss: 0.0523 - val_mae: 0.2058 - val_mse: 0.0523
Epoch 3/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0398 - mae: 0.1318 - mse: 0.0398
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0403 - mae: 0.1372 - mse: 0.0403 - val_loss: 0.0523 - val_mae: 0.2053 - val_mse: 0.0523
Epoch 4/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0252 - mae: 0.1289 - mse: 0.0252
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0363 - mae: 0.1397 - mse: 0.0363 - val_loss: 0.0598 - val_mae: 0.2229 - val_mse: 0.0598
Epoch 5/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0475 - mae: 0.1452 - mse: 0.0475
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0400 - mae: 0.1420 - mse: 0.0400 - val_loss: 0.0537 - val_mae: 0.2125 - val_mse: 0.0537
Epoch 6/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0433 - mae: 0.1631 - mse: 0.0433
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0385 - mae: 0.1547 - mse: 0.0385 - val_loss: 0.0495 - val_mae: 0.2046 - val_mse: 0.0495
Epoch 7/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0235 - mae: 0.1223 - mse: 0.0235
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0302 - mae: 0.1408 - mse: 0.0302 - val_loss: 0.0356 - val_mae: 0.1680 - val_mse: 0.0356
Epoch 8/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 122ms/step - loss: 0.0305 - mae: 0.1383 - mse: 0.0305
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 30ms/step - loss: 0.0311 - mae: 0.1373 - mse: 0.0311 - val_loss: 0.0302 - val_mae: 0.1466 - val_mse: 0.0302
Epoch 9/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0308 - mae: 0.1351 - mse: 0.0308
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0334 - mae: 0.1376 - mse: 0.0334 - val_loss: 0.0297 - val_mae: 0.1453 - val_mse: 0.0297
Epoch 10/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0351 - mae: 0.1545 - mse: 0.0351
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0313 - mae: 0.1381 - mse: 0.0313 - val_loss: 0.0315 - val_mae: 0.1549 - val_mse: 0.0315
Epoch 11/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0379 - mae: 0.1534 - mse: 0.0379
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0322 - mae: 0.1380 - mse: 0.0322 - val_loss: 0.0336 - val_mae: 0.1638 - val_mse: 0.0336
Epoch 12/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0171 - mae: 0.1071 - mse: 0.0171
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0267 - mae: 0.1275 - mse: 0.0267 - val_loss: 0.0369 - val_mae: 0.1741 - val_mse: 0.0369
Epoch 13/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0384 - mae: 0.1472 - mse: 0.0384
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0323 - mae: 0.1384 - mse: 0.0323 - val_loss: 0.0349 - val_mae: 0.1677 - val_mse: 0.0349
Epoch 14/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0458 - mae: 0.1670 - mse: 0.0458
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0335 - mae: 0.1401 - mse: 0.0335 - val_loss: 0.0308 - val_mae: 0.1536 - val_mse: 0.0308
Epoch 15/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0277 - mae: 0.1178 - mse: 0.0277
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0299 - mae: 0.1218 - mse: 0.0299 - val_loss: 0.0280 - val_mae: 0.1424 - val_mse: 0.0280
Epoch 16/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0222 - mae: 0.1147 - mse: 0.0222
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0269 - mae: 0.1237 - mse: 0.0269 - val_loss: 0.0273 - val_mae: 0.1421 - val_mse: 0.0273
Epoch 17/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0342 - mae: 0.1347 - mse: 0.0342
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0285 - mae: 0.1225 - mse: 0.0285 - val_loss: 0.0276 - val_mae: 0.1458 - val_mse: 0.0276
Epoch 18/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0346 - mae: 0.1384 - mse: 0.0346
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0266 - mae: 0.1236 - mse: 0.0266 - val_loss: 0.0248 - val_mae: 0.1325 - val_mse: 0.0248
Epoch 19/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0245 - mae: 0.1061 - mse: 0.0245
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0280 - mae: 0.1156 - mse: 0.0280 - val_loss: 0.0236 - val_mae: 0.1215 - val_mse: 0.0236
Epoch 20/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0270 - mae: 0.1122 - mse: 0.0270
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0263 - mae: 0.1153 - mse: 0.0263 - val_loss: 0.0230 - val_mae: 0.1222 - val_mse: 0.0230
Epoch 21/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0131 - mae: 0.0931 - mse: 0.0131
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0211 - mae: 0.1079 - mse: 0.0211 - val_loss: 0.0229 - val_mae: 0.1259 - val_mse: 0.0229
Epoch 22/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0126 - mae: 0.0952 - mse: 0.0126
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0203 - mae: 0.1114 - mse: 0.0203 - val_loss: 0.0231 - val_mae: 0.1287 - val_mse: 0.0231
Epoch 23/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0379 - mae: 0.1207 - mse: 0.0379
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0292 - mae: 0.1173 - mse: 0.0292 - val_loss: 0.0223 - val_mae: 0.1206 - val_mse: 0.0223
Epoch 24/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0169 - mae: 0.0875 - mse: 0.0169
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0209 - mae: 0.1010 - mse: 0.0209 - val_loss: 0.0222 - val_mae: 0.1143 - val_mse: 0.0222
Epoch 25/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0278 - mae: 0.1031 - mse: 0.0278
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0313 - mae: 0.1143 - mse: 0.0313 - val_loss: 0.0217 - val_mae: 0.1173 - val_mse: 0.0217
Epoch 26/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0339 - mae: 0.1391 - mse: 0.0339
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0280 - mae: 0.1251 - mse: 0.0280 - val_loss: 0.0217 - val_mae: 0.1171 - val_mse: 0.0217
Epoch 27/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0223 - mae: 0.1046 - mse: 0.0223
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0223 - mae: 0.1087 - mse: 0.0223 - val_loss: 0.0215 - val_mae: 0.1150 - val_mse: 0.0215
Epoch 28/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 26ms/step - loss: 0.0277 - mae: 0.1027 - mse: 0.0277
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0254 - mae: 0.1064 - mse: 0.0254 - val_loss: 0.0213 - val_mae: 0.1161 - val_mse: 0.0213
Epoch 29/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0215 - mae: 0.1006 - mse: 0.0215
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0212 - mae: 0.1049 - mse: 0.0212 - val_loss: 0.0219 - val_mae: 0.1227 - val_mse: 0.0219
Epoch 30/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0205 - mae: 0.1027 - mse: 0.0205
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0207 - mae: 0.1074 - mse: 0.0207 - val_loss: 0.0217 - val_mae: 0.1223 - val_mse: 0.0217
Epoch 31/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0164 - mae: 0.1002 - mse: 0.0164
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0220 - mae: 0.1113 - mse: 0.0220 - val_loss: 0.0208 - val_mae: 0.1152 - val_mse: 0.0208
Epoch 32/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0204 - mae: 0.0993 - mse: 0.0204
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0217 - mae: 0.1045 - mse: 0.0217 - val_loss: 0.0202 - val_mae: 0.1139 - val_mse: 0.0202
Epoch 33/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0164 - mae: 0.1065 - mse: 0.0164
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0214 - mae: 0.1116 - mse: 0.0214 - val_loss: 0.0202 - val_mae: 0.1149 - val_mse: 0.0202
Epoch 34/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0206 - mae: 0.1065 - mse: 0.0206
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0209 - mae: 0.1056 - mse: 0.0209 - val_loss: 0.0210 - val_mae: 0.1185 - val_mse: 0.0210
Epoch 35/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0354 - mae: 0.1185 - mse: 0.0354
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0276 - mae: 0.1079 - mse: 0.0276 - val_loss: 0.0216 - val_mae: 0.1194 - val_mse: 0.0216
Epoch 36/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0188 - mae: 0.1021 - mse: 0.0188
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0212 - mae: 0.1049 - mse: 0.0212 - val_loss: 0.0230 - val_mae: 0.1277 - val_mse: 0.0230
Epoch 37/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0181 - mae: 0.1070 - mse: 0.0181
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0195 - mae: 0.1061 - mse: 0.0195 - val_loss: 0.0241 - val_mae: 0.1325 - val_mse: 0.0241
Epoch 38/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0192 - mae: 0.1089 - mse: 0.0192
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0225 - mae: 0.1106 - mse: 0.0225 - val_loss: 0.0208 - val_mae: 0.1159 - val_mse: 0.0208
Epoch 39/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0206 - mae: 0.1134 - mse: 0.0206
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0183 - mae: 0.0998 - mse: 0.0183 - val_loss: 0.0207 - val_mae: 0.1110 - val_mse: 0.0207
Epoch 40/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0209 - mae: 0.1009 - mse: 0.0209
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0244 - mae: 0.1038 - mse: 0.0244 - val_loss: 0.0207 - val_mae: 0.1161 - val_mse: 0.0207
Epoch 41/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0237 - mae: 0.1009 - mse: 0.0237
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0213 - mae: 0.1000 - mse: 0.0213 - val_loss: 0.0205 - val_mae: 0.1166 - val_mse: 0.0205
Epoch 42/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 24ms/step - loss: 0.0138 - mae: 0.0899 - mse: 0.0138
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0184 - mae: 0.0988 - mse: 0.0184 - val_loss: 0.0197 - val_mae: 0.1126 - val_mse: 0.0197
Epoch 43/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0264 - mae: 0.1287 - mse: 0.0264
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0217 - mae: 0.1109 - mse: 0.0217 - val_loss: 0.0193 - val_mae: 0.1097 - val_mse: 0.0193
Epoch 44/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0258 - mae: 0.1141 - mse: 0.0258
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0215 - mae: 0.1060 - mse: 0.0215 - val_loss: 0.0190 - val_mae: 0.1083 - val_mse: 0.0190
Epoch 45/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 24ms/step - loss: 0.0134 - mae: 0.0925 - mse: 0.0134
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0161 - mae: 0.0927 - mse: 0.0161 - val_loss: 0.0189 - val_mae: 0.1080 - val_mse: 0.0189
Epoch 46/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0140 - mae: 0.0852 - mse: 0.0140
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0196 - mae: 0.1016 - mse: 0.0196 - val_loss: 0.0190 - val_mae: 0.1110 - val_mse: 0.0190
Epoch 47/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0179 - mae: 0.0913 - mse: 0.0179
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0177 - mae: 0.0974 - mse: 0.0177 - val_loss: 0.0194 - val_mae: 0.1138 - val_mse: 0.0194
Epoch 48/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 34ms/step - loss: 0.0178 - mae: 0.0975 - mse: 0.0178
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 31ms/step - loss: 0.0199 - mae: 0.1018 - mse: 0.0199 - val_loss: 0.0185 - val_mae: 0.1090 - val_mse: 0.0185
Epoch 49/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0175 - mae: 0.0908 - mse: 0.0175
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0172 - mae: 0.0942 - mse: 0.0172 - val_loss: 0.0184 - val_mae: 0.1086 - val_mse: 0.0184
Epoch 50/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0192 - mae: 0.1057 - mse: 0.0192
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0171 - mae: 0.0968 - mse: 0.0171 - val_loss: 0.0181 - val_mae: 0.1062 - val_mse: 0.0181
Epoch 51/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0144 - mae: 0.0930 - mse: 0.0144
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0174 - mae: 0.0931 - mse: 0.0174 - val_loss: 0.0180 - val_mae: 0.1084 - val_mse: 0.0180
Epoch 52/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0103 - mae: 0.0825 - mse: 0.0103
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0187 - mae: 0.0955 - mse: 0.0187 - val_loss: 0.0179 - val_mae: 0.1082 - val_mse: 0.0179
Epoch 53/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 19ms/step - loss: 0.0104 - mae: 0.0828 - mse: 0.0104
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0149 - mae: 0.0895 - mse: 0.0149 - val_loss: 0.0177 - val_mae: 0.1058 - val_mse: 0.0177
Epoch 54/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0199 - mae: 0.0963 - mse: 0.0199
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0202 - mae: 0.0997 - mse: 0.0202 - val_loss: 0.0179 - val_mae: 0.1047 - val_mse: 0.0179
Epoch 55/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0227 - mae: 0.1014 - mse: 0.0227
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0221 - mae: 0.0978 - mse: 0.0221 - val_loss: 0.0178 - val_mae: 0.1032 - val_mse: 0.0178
Epoch 56/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0122 - mae: 0.0866 - mse: 0.0122
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0164 - mae: 0.0940 - mse: 0.0164 - val_loss: 0.0174 - val_mae: 0.1017 - val_mse: 0.0174
Epoch 57/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0178 - mae: 0.0999 - mse: 0.0178
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0187 - mae: 0.0960 - mse: 0.0187 - val_loss: 0.0168 - val_mae: 0.1033 - val_mse: 0.0168
Epoch 58/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0075 - mae: 0.0698 - mse: 0.0075
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0138 - mae: 0.0843 - mse: 0.0138 - val_loss: 0.0176 - val_mae: 0.1080 - val_mse: 0.0176
Epoch 59/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0285 - mae: 0.1217 - mse: 0.0285
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0213 - mae: 0.1079 - mse: 0.0213 - val_loss: 0.0175 - val_mae: 0.1067 - val_mse: 0.0175
Epoch 60/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0210 - mae: 0.1010 - mse: 0.0210
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0211 - mae: 0.1018 - mse: 0.0211 - val_loss: 0.0172 - val_mae: 0.1027 - val_mse: 0.0172
Epoch 61/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0119 - mae: 0.0800 - mse: 0.0119
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0156 - mae: 0.0873 - mse: 0.0156 - val_loss: 0.0174 - val_mae: 0.1032 - val_mse: 0.0174
Epoch 62/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 24ms/step - loss: 0.0251 - mae: 0.1080 - mse: 0.0251
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 29ms/step - loss: 0.0193 - mae: 0.0966 - mse: 0.0193 - val_loss: 0.0182 - val_mae: 0.1067 - val_mse: 0.0182
Epoch 63/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0164 - mae: 0.0895 - mse: 0.0164
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0155 - mae: 0.0867 - mse: 0.0155 - val_loss: 0.0182 - val_mae: 0.1054 - val_mse: 0.0182
Epoch 64/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0148 - mae: 0.0991 - mse: 0.0148
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0163 - mae: 0.0962 - mse: 0.0163 - val_loss: 0.0182 - val_mae: 0.1042 - val_mse: 0.0182
Epoch 65/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0335 - mae: 0.1122 - mse: 0.0335
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0241 - mae: 0.0980 - mse: 0.0241 - val_loss: 0.0177 - val_mae: 0.1029 - val_mse: 0.0177
Epoch 66/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0140 - mae: 0.0833 - mse: 0.0140
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0151 - mae: 0.0887 - mse: 0.0151 - val_loss: 0.0182 - val_mae: 0.1066 - val_mse: 0.0182
Epoch 67/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0112 - mae: 0.0793 - mse: 0.0112
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0135 - mae: 0.0856 - mse: 0.0135 - val_loss: 0.0215 - val_mae: 0.1163 - val_mse: 0.0215
Epoch 1/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m1s[0m 874ms/step - loss: 0.0497 - mae: 0.1848 - mse: 0.0497
[1m3/3[0m [32m====================[0m[37m[0m [1m1s[0m 81ms/step - loss: 0.0445 - mae: 0.1693 - mse: 0.0445 - val_loss: 0.0538 - val_mae: 0.1984 - val_mse: 0.0538
Epoch 2/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0232 - mae: 0.1040 - mse: 0.0232
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0279 - mae: 0.1176 - mse: 0.0279 - val_loss: 0.0628 - val_mae: 0.2274 - val_mse: 0.0628
Epoch 3/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0408 - mae: 0.1460 - mse: 0.0408
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0338 - mae: 0.1357 - mse: 0.0338 - val_loss: 0.0690 - val_mae: 0.2416 - val_mse: 0.0690
Epoch 4/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0396 - mae: 0.1611 - mse: 0.0396
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0319 - mae: 0.1416 - mse: 0.0319 - val_loss: 0.0615 - val_mae: 0.2253 - val_mse: 0.0615
Epoch 5/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0300 - mae: 0.1273 - mse: 0.0300
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0274 - mae: 0.1265 - mse: 0.0274 - val_loss: 0.0517 - val_mae: 0.1952 - val_mse: 0.0517
Epoch 6/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0125 - mae: 0.0842 - mse: 0.0125
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0173 - mae: 0.0931 - mse: 0.0173 - val_loss: 0.0477 - val_mae: 0.1767 - val_mse: 0.0477
Epoch 7/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 115ms/step - loss: 0.0192 - mae: 0.1089 - mse: 0.0192
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0198 - mae: 0.1054 - mse: 0.0198 - val_loss: 0.0477 - val_mae: 0.1839 - val_mse: 0.0477
Epoch 8/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0247 - mae: 0.1198 - mse: 0.0247
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0210 - mae: 0.1126 - mse: 0.0210 - val_loss: 0.0484 - val_mae: 0.1899 - val_mse: 0.0484
Epoch 9/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0169 - mae: 0.0975 - mse: 0.0169
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0178 - mae: 0.1047 - mse: 0.0178 - val_loss: 0.0452 - val_mae: 0.1773 - val_mse: 0.0452
Epoch 10/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 24ms/step - loss: 0.0141 - mae: 0.0993 - mse: 0.0141
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0177 - mae: 0.1039 - mse: 0.0177 - val_loss: 0.0437 - val_mae: 0.1637 - val_mse: 0.0437
Epoch 11/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0154 - mae: 0.0961 - mse: 0.0154
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0187 - mae: 0.1030 - mse: 0.0187 - val_loss: 0.0439 - val_mae: 0.1629 - val_mse: 0.0439
Epoch 12/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0096 - mae: 0.0738 - mse: 0.0096
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0192 - mae: 0.0996 - mse: 0.0192 - val_loss: 0.0449 - val_mae: 0.1693 - val_mse: 0.0449
Epoch 13/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0128 - mae: 0.0861 - mse: 0.0128
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0182 - mae: 0.0977 - mse: 0.0182 - val_loss: 0.0448 - val_mae: 0.1717 - val_mse: 0.0448
Epoch 14/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0154 - mae: 0.0921 - mse: 0.0154
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0170 - mae: 0.0996 - mse: 0.0170 - val_loss: 0.0437 - val_mae: 0.1687 - val_mse: 0.0437
Epoch 15/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0197 - mae: 0.0994 - mse: 0.0197
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0184 - mae: 0.0995 - mse: 0.0184 - val_loss: 0.0425 - val_mae: 0.1638 - val_mse: 0.0425
Epoch 16/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0102 - mae: 0.0747 - mse: 0.0102
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0126 - mae: 0.0841 - mse: 0.0126 - val_loss: 0.0425 - val_mae: 0.1648 - val_mse: 0.0425
Epoch 17/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0124 - mae: 0.0927 - mse: 0.0124
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0145 - mae: 0.0931 - mse: 0.0145 - val_loss: 0.0429 - val_mae: 0.1663 - val_mse: 0.0429
Epoch 18/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0160 - mae: 0.1010 - mse: 0.0160
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0148 - mae: 0.0943 - mse: 0.0148 - val_loss: 0.0433 - val_mae: 0.1679 - val_mse: 0.0433
Epoch 19/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0137 - mae: 0.0982 - mse: 0.0137
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0147 - mae: 0.0966 - mse: 0.0147 - val_loss: 0.0430 - val_mae: 0.1632 - val_mse: 0.0430
Epoch 20/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0330 - mae: 0.1186 - mse: 0.0330
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0248 - mae: 0.1054 - mse: 0.0248 - val_loss: 0.0427 - val_mae: 0.1536 - val_mse: 0.0427
Epoch 21/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0167 - mae: 0.0996 - mse: 0.0167
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0155 - mae: 0.0931 - mse: 0.0155 - val_loss: 0.0422 - val_mae: 0.1542 - val_mse: 0.0422
Epoch 22/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0119 - mae: 0.0864 - mse: 0.0119
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0128 - mae: 0.0880 - mse: 0.0128 - val_loss: 0.0418 - val_mae: 0.1558 - val_mse: 0.0418
Epoch 23/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0161 - mae: 0.0923 - mse: 0.0161
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0152 - mae: 0.0940 - mse: 0.0152 - val_loss: 0.0415 - val_mae: 0.1619 - val_mse: 0.0415
Epoch 24/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0166 - mae: 0.0943 - mse: 0.0166
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0144 - mae: 0.0915 - mse: 0.0144 - val_loss: 0.0410 - val_mae: 0.1642 - val_mse: 0.0410
Epoch 25/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0162 - mae: 0.1008 - mse: 0.0162
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0160 - mae: 0.0969 - mse: 0.0160 - val_loss: 0.0400 - val_mae: 0.1561 - val_mse: 0.0400
Epoch 26/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0160 - mae: 0.0950 - mse: 0.0160
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0129 - mae: 0.0842 - mse: 0.0129 - val_loss: 0.0402 - val_mae: 0.1568 - val_mse: 0.0402
Epoch 27/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0076 - mae: 0.0673 - mse: 0.0076
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0118 - mae: 0.0804 - mse: 0.0118 - val_loss: 0.0415 - val_mae: 0.1671 - val_mse: 0.0415
Epoch 28/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0182 - mae: 0.0983 - mse: 0.0182
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0149 - mae: 0.0924 - mse: 0.0149 - val_loss: 0.0417 - val_mae: 0.1686 - val_mse: 0.0417
Epoch 29/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0085 - mae: 0.0738 - mse: 0.0085
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0111 - mae: 0.0817 - mse: 0.0111 - val_loss: 0.0397 - val_mae: 0.1568 - val_mse: 0.0397
Epoch 30/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0084 - mae: 0.0760 - mse: 0.0084
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0111 - mae: 0.0806 - mse: 0.0111 - val_loss: 0.0397 - val_mae: 0.1560 - val_mse: 0.0397
Epoch 31/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0153 - mae: 0.0904 - mse: 0.0153
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0130 - mae: 0.0857 - mse: 0.0130 - val_loss: 0.0396 - val_mae: 0.1559 - val_mse: 0.0396
Epoch 32/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0158 - mae: 0.0902 - mse: 0.0158
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0129 - mae: 0.0851 - mse: 0.0129 - val_loss: 0.0391 - val_mae: 0.1527 - val_mse: 0.0391
Epoch 33/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0063 - mae: 0.0634 - mse: 0.0063
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0098 - mae: 0.0742 - mse: 0.0098 - val_loss: 0.0390 - val_mae: 0.1514 - val_mse: 0.0390
Epoch 34/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0084 - mae: 0.0737 - mse: 0.0084
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0116 - mae: 0.0811 - mse: 0.0116 - val_loss: 0.0390 - val_mae: 0.1517 - val_mse: 0.0390
Epoch 35/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0112 - mae: 0.0786 - mse: 0.0112
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0117 - mae: 0.0796 - mse: 0.0117 - val_loss: 0.0397 - val_mae: 0.1581 - val_mse: 0.0397
Epoch 36/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0109 - mae: 0.0839 - mse: 0.0109
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0109 - mae: 0.0818 - mse: 0.0109 - val_loss: 0.0398 - val_mae: 0.1598 - val_mse: 0.0398
Epoch 37/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0140 - mae: 0.0848 - mse: 0.0140
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0115 - mae: 0.0807 - mse: 0.0115 - val_loss: 0.0386 - val_mae: 0.1570 - val_mse: 0.0386
Epoch 38/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0066 - mae: 0.0640 - mse: 0.0066
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0088 - mae: 0.0708 - mse: 0.0088 - val_loss: 0.0377 - val_mae: 0.1534 - val_mse: 0.0377
Epoch 39/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0074 - mae: 0.0688 - mse: 0.0074
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0138 - mae: 0.0781 - mse: 0.0138 - val_loss: 0.0377 - val_mae: 0.1540 - val_mse: 0.0377
Epoch 40/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0110 - mae: 0.0698 - mse: 0.0110
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0107 - mae: 0.0765 - mse: 0.0107 - val_loss: 0.0380 - val_mae: 0.1536 - val_mse: 0.0380
Epoch 41/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0234 - mae: 0.0859 - mse: 0.0234
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 24ms/step - loss: 0.0166 - mae: 0.0801 - mse: 0.0166 - val_loss: 0.0386 - val_mae: 0.1509 - val_mse: 0.0386
Epoch 42/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0127 - mae: 0.0886 - mse: 0.0127
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0129 - mae: 0.0862 - mse: 0.0129 - val_loss: 0.0393 - val_mae: 0.1513 - val_mse: 0.0393
Epoch 43/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0183 - mae: 0.0955 - mse: 0.0183
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0137 - mae: 0.0837 - mse: 0.0137 - val_loss: 0.0391 - val_mae: 0.1481 - val_mse: 0.0391
Epoch 44/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0140 - mae: 0.0805 - mse: 0.0140
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0149 - mae: 0.0798 - mse: 0.0149 - val_loss: 0.0385 - val_mae: 0.1455 - val_mse: 0.0385
Epoch 45/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0136 - mae: 0.0926 - mse: 0.0136
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 24ms/step - loss: 0.0126 - mae: 0.0867 - mse: 0.0126 - val_loss: 0.0381 - val_mae: 0.1534 - val_mse: 0.0381
Epoch 46/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0058 - mae: 0.0620 - mse: 0.0058
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0086 - mae: 0.0713 - mse: 0.0086 - val_loss: 0.0390 - val_mae: 0.1610 - val_mse: 0.0390
Epoch 47/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0092 - mae: 0.0794 - mse: 0.0092
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0108 - mae: 0.0792 - mse: 0.0108 - val_loss: 0.0379 - val_mae: 0.1547 - val_mse: 0.0379
Epoch 48/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0049 - mae: 0.0557 - mse: 0.0049
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0077 - mae: 0.0658 - mse: 0.0077 - val_loss: 0.0375 - val_mae: 0.1486 - val_mse: 0.0375
Epoch 49/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0071 - mae: 0.0689 - mse: 0.0071
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0083 - mae: 0.0694 - mse: 0.0083 - val_loss: 0.0377 - val_mae: 0.1540 - val_mse: 0.0377
Epoch 50/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0054 - mae: 0.0584 - mse: 0.0054
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0082 - mae: 0.0676 - mse: 0.0082 - val_loss: 0.0387 - val_mae: 0.1590 - val_mse: 0.0387
Epoch 51/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 31ms/step - loss: 0.0070 - mae: 0.0611 - mse: 0.0070
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 32ms/step - loss: 0.0087 - mae: 0.0687 - mse: 0.0087 - val_loss: 0.0382 - val_mae: 0.1556 - val_mse: 0.0382
Epoch 52/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 27ms/step - loss: 0.0118 - mae: 0.0754 - mse: 0.0118
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 40ms/step - loss: 0.0095 - mae: 0.0714 - mse: 0.0095 - val_loss: 0.0376 - val_mae: 0.1529 - val_mse: 0.0376
Epoch 53/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 31ms/step - loss: 0.0070 - mae: 0.0671 - mse: 0.0070
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 33ms/step - loss: 0.0081 - mae: 0.0685 - mse: 0.0081 - val_loss: 0.0395 - val_mae: 0.1640 - val_mse: 0.0395
Epoch 54/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 27ms/step - loss: 0.0130 - mae: 0.0786 - mse: 0.0130
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 34ms/step - loss: 0.0112 - mae: 0.0743 - mse: 0.0112 - val_loss: 0.0379 - val_mae: 0.1544 - val_mse: 0.0379
Epoch 55/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 27ms/step - loss: 0.0071 - mae: 0.0645 - mse: 0.0071
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 32ms/step - loss: 0.0081 - mae: 0.0680 - mse: 0.0081 - val_loss: 0.0388 - val_mae: 0.1577 - val_mse: 0.0388
Epoch 56/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 32ms/step - loss: 0.0132 - mae: 0.0826 - mse: 0.0132
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 36ms/step - loss: 0.0106 - mae: 0.0758 - mse: 0.0106 - val_loss: 0.0402 - val_mae: 0.1635 - val_mse: 0.0402
Epoch 57/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0074 - mae: 0.0696 - mse: 0.0074
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0081 - mae: 0.0688 - mse: 0.0081 - val_loss: 0.0384 - val_mae: 0.1564 - val_mse: 0.0384
Epoch 58/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0181 - mae: 0.0926 - mse: 0.0181
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0135 - mae: 0.0812 - mse: 0.0135 - val_loss: 0.0374 - val_mae: 0.1553 - val_mse: 0.0374
Epoch 59/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0112 - mae: 0.0698 - mse: 0.0112
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0097 - mae: 0.0701 - mse: 0.0097 - val_loss: 0.0369 - val_mae: 0.1534 - val_mse: 0.0369
Epoch 60/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0054 - mae: 0.0569 - mse: 0.0054
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0121 - mae: 0.0736 - mse: 0.0121 - val_loss: 0.0366 - val_mae: 0.1512 - val_mse: 0.0366
Epoch 61/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0057 - mae: 0.0614 - mse: 0.0057
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0081 - mae: 0.0670 - mse: 0.0081 - val_loss: 0.0362 - val_mae: 0.1499 - val_mse: 0.0362
Epoch 62/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0075 - mae: 0.0685 - mse: 0.0075
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0084 - mae: 0.0680 - mse: 0.0084 - val_loss: 0.0370 - val_mae: 0.1525 - val_mse: 0.0370
Epoch 63/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0041 - mae: 0.0487 - mse: 0.0041
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0064 - mae: 0.0591 - mse: 0.0064 - val_loss: 0.0373 - val_mae: 0.1538 - val_mse: 0.0373
Epoch 64/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0104 - mae: 0.0693 - mse: 0.0104
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0110 - mae: 0.0711 - mse: 0.0110 - val_loss: 0.0360 - val_mae: 0.1492 - val_mse: 0.0360
Epoch 65/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0122 - mae: 0.0725 - mse: 0.0122
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0093 - mae: 0.0661 - mse: 0.0093 - val_loss: 0.0365 - val_mae: 0.1530 - val_mse: 0.0365
Epoch 66/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0086 - mae: 0.0654 - mse: 0.0086
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0091 - mae: 0.0662 - mse: 0.0091 - val_loss: 0.0361 - val_mae: 0.1495 - val_mse: 0.0361
Epoch 67/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0121 - mae: 0.0718 - mse: 0.0121
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0109 - mae: 0.0730 - mse: 0.0109 - val_loss: 0.0360 - val_mae: 0.1449 - val_mse: 0.0360
Epoch 68/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0075 - mae: 0.0662 - mse: 0.0075
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0080 - mae: 0.0640 - mse: 0.0080 - val_loss: 0.0369 - val_mae: 0.1458 - val_mse: 0.0369
Epoch 69/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0136 - mae: 0.0752 - mse: 0.0136
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0101 - mae: 0.0685 - mse: 0.0101 - val_loss: 0.0371 - val_mae: 0.1466 - val_mse: 0.0371
Epoch 70/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0056 - mae: 0.0592 - mse: 0.0056
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0072 - mae: 0.0616 - mse: 0.0072 - val_loss: 0.0366 - val_mae: 0.1451 - val_mse: 0.0366
Epoch 71/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0119 - mae: 0.0601 - mse: 0.0119
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0109 - mae: 0.0624 - mse: 0.0109 - val_loss: 0.0354 - val_mae: 0.1414 - val_mse: 0.0354
Epoch 72/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0137 - mae: 0.0634 - mse: 0.0137
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0112 - mae: 0.0623 - mse: 0.0112 - val_loss: 0.0351 - val_mae: 0.1433 - val_mse: 0.0351
Epoch 73/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0049 - mae: 0.0555 - mse: 0.0049
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0082 - mae: 0.0626 - mse: 0.0082 - val_loss: 0.0356 - val_mae: 0.1465 - val_mse: 0.0356
Epoch 74/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0166 - mae: 0.0644 - mse: 0.0166
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0153 - mae: 0.0711 - mse: 0.0153 - val_loss: 0.0351 - val_mae: 0.1429 - val_mse: 0.0351
Epoch 75/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0062 - mae: 0.0625 - mse: 0.0062
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0103 - mae: 0.0697 - mse: 0.0103 - val_loss: 0.0349 - val_mae: 0.1400 - val_mse: 0.0349
Epoch 76/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0109 - mae: 0.0664 - mse: 0.0109
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0094 - mae: 0.0662 - mse: 0.0094 - val_loss: 0.0354 - val_mae: 0.1433 - val_mse: 0.0354
Epoch 77/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0083 - mae: 0.0675 - mse: 0.0083
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0100 - mae: 0.0707 - mse: 0.0100 - val_loss: 0.0367 - val_mae: 0.1445 - val_mse: 0.0367
Epoch 78/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0071 - mae: 0.0685 - mse: 0.0071
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0082 - mae: 0.0662 - mse: 0.0082 - val_loss: 0.0379 - val_mae: 0.1481 - val_mse: 0.0379
Epoch 79/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0089 - mae: 0.0685 - mse: 0.0089
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0091 - mae: 0.0669 - mse: 0.0091 - val_loss: 0.0377 - val_mae: 0.1485 - val_mse: 0.0377
Epoch 80/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0109 - mae: 0.0712 - mse: 0.0109
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0096 - mae: 0.0665 - mse: 0.0096 - val_loss: 0.0367 - val_mae: 0.1455 - val_mse: 0.0367
Epoch 81/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0101 - mae: 0.0661 - mse: 0.0101
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0080 - mae: 0.0622 - mse: 0.0080 - val_loss: 0.0369 - val_mae: 0.1500 - val_mse: 0.0369
Epoch 82/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0113 - mae: 0.0734 - mse: 0.0113
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0086 - mae: 0.0657 - mse: 0.0086 - val_loss: 0.0383 - val_mae: 0.1576 - val_mse: 0.0383
Epoch 83/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0078 - mae: 0.0663 - mse: 0.0078
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0094 - mae: 0.0646 - mse: 0.0094 - val_loss: 0.0366 - val_mae: 0.1493 - val_mse: 0.0366
Epoch 84/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0054 - mae: 0.0614 - mse: 0.0054
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0066 - mae: 0.0603 - mse: 0.0066 - val_loss: 0.0356 - val_mae: 0.1441 - val_mse: 0.0356
Epoch 85/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0120 - mae: 0.0761 - mse: 0.0120
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0096 - mae: 0.0690 - mse: 0.0096 - val_loss: 0.0350 - val_mae: 0.1422 - val_mse: 0.0350
Epoch 1/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m1s[0m 957ms/step - loss: 0.1673 - mae: 0.3795 - mse: 0.1673
[1m3/3[0m [32m====================[0m[37m[0m [1m1s[0m 84ms/step - loss: 0.1451 - mae: 0.3457 - mse: 0.1451 - val_loss: 0.1352 - val_mae: 0.3416 - val_mse: 0.1352
Epoch 2/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0359 - mae: 0.1617 - mse: 0.0359
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0443 - mae: 0.1687 - mse: 0.0443 - val_loss: 0.1077 - val_mae: 0.2987 - val_mse: 0.1077
Epoch 3/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0442 - mae: 0.1462 - mse: 0.0442
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0480 - mae: 0.1508 - mse: 0.0480 - val_loss: 0.1078 - val_mae: 0.2910 - val_mse: 0.1078
Epoch 4/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0440 - mae: 0.1488 - mse: 0.0440
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0454 - mae: 0.1425 - mse: 0.0454 - val_loss: 0.1190 - val_mae: 0.2985 - val_mse: 0.1190
Epoch 5/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0336 - mae: 0.1215 - mse: 0.0336
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0403 - mae: 0.1326 - mse: 0.0403 - val_loss: 0.1260 - val_mae: 0.3021 - val_mse: 0.1260
Epoch 6/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0414 - mae: 0.1486 - mse: 0.0414
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0385 - mae: 0.1422 - mse: 0.0385 - val_loss: 0.1408 - val_mae: 0.3185 - val_mse: 0.1408
Epoch 7/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0567 - mae: 0.1743 - mse: 0.0567
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0458 - mae: 0.1555 - mse: 0.0458 - val_loss: 0.1397 - val_mae: 0.3171 - val_mse: 0.1397
Epoch 8/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0393 - mae: 0.1574 - mse: 0.0393
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0430 - mae: 0.1624 - mse: 0.0430 - val_loss: 0.1087 - val_mae: 0.2774 - val_mse: 0.1087
Epoch 9/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0326 - mae: 0.1423 - mse: 0.0326
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0346 - mae: 0.1442 - mse: 0.0346 - val_loss: 0.0828 - val_mae: 0.2419 - val_mse: 0.0828
Epoch 10/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0248 - mae: 0.1254 - mse: 0.0248
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0325 - mae: 0.1349 - mse: 0.0325 - val_loss: 0.0826 - val_mae: 0.2424 - val_mse: 0.0826
Epoch 11/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0410 - mae: 0.1461 - mse: 0.0410
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 24ms/step - loss: 0.0404 - mae: 0.1480 - mse: 0.0404 - val_loss: 0.1090 - val_mae: 0.2779 - val_mse: 0.1090
Epoch 12/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0257 - mae: 0.1291 - mse: 0.0257
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0302 - mae: 0.1360 - mse: 0.0302 - val_loss: 0.1196 - val_mae: 0.2900 - val_mse: 0.1196
Epoch 13/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0403 - mae: 0.1575 - mse: 0.0403
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0355 - mae: 0.1464 - mse: 0.0355 - val_loss: 0.1135 - val_mae: 0.2814 - val_mse: 0.1135
Epoch 14/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0369 - mae: 0.1382 - mse: 0.0369
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0350 - mae: 0.1396 - mse: 0.0350 - val_loss: 0.1022 - val_mae: 0.2663 - val_mse: 0.1022
Epoch 15/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0304 - mae: 0.1336 - mse: 0.0304
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 28ms/step - loss: 0.0289 - mae: 0.1284 - mse: 0.0289 - val_loss: 0.0832 - val_mae: 0.2422 - val_mse: 0.0832
Epoch 16/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0338 - mae: 0.1256 - mse: 0.0338
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0313 - mae: 0.1294 - mse: 0.0313 - val_loss: 0.0841 - val_mae: 0.2462 - val_mse: 0.0841
Epoch 17/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0374 - mae: 0.1439 - mse: 0.0374
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0323 - mae: 0.1358 - mse: 0.0323 - val_loss: 0.1022 - val_mae: 0.2736 - val_mse: 0.1022
Epoch 18/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0245 - mae: 0.1338 - mse: 0.0245
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0286 - mae: 0.1340 - mse: 0.0286 - val_loss: 0.0981 - val_mae: 0.2664 - val_mse: 0.0981
Epoch 19/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0357 - mae: 0.1432 - mse: 0.0357
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0287 - mae: 0.1307 - mse: 0.0287 - val_loss: 0.0837 - val_mae: 0.2442 - val_mse: 0.0837
Epoch 20/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0179 - mae: 0.0976 - mse: 0.0179
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0219 - mae: 0.1129 - mse: 0.0219 - val_loss: 0.0704 - val_mae: 0.2211 - val_mse: 0.0704
Epoch 21/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0141 - mae: 0.0937 - mse: 0.0141
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0216 - mae: 0.1110 - mse: 0.0216 - val_loss: 0.0778 - val_mae: 0.2318 - val_mse: 0.0778
Epoch 22/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0268 - mae: 0.1199 - mse: 0.0268
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0249 - mae: 0.1198 - mse: 0.0249 - val_loss: 0.0657 - val_mae: 0.2129 - val_mse: 0.0657
Epoch 23/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0299 - mae: 0.1160 - mse: 0.0299
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0289 - mae: 0.1240 - mse: 0.0289 - val_loss: 0.0477 - val_mae: 0.1802 - val_mse: 0.0477
Epoch 24/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0215 - mae: 0.1120 - mse: 0.0215
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0243 - mae: 0.1127 - mse: 0.0243 - val_loss: 0.0372 - val_mae: 0.1589 - val_mse: 0.0372
Epoch 25/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0319 - mae: 0.1192 - mse: 0.0319
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0263 - mae: 0.1148 - mse: 0.0263 - val_loss: 0.0507 - val_mae: 0.1880 - val_mse: 0.0507
Epoch 26/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0199 - mae: 0.1111 - mse: 0.0199
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0210 - mae: 0.1141 - mse: 0.0210 - val_loss: 0.0662 - val_mae: 0.2182 - val_mse: 0.0662
Epoch 27/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0158 - mae: 0.1074 - mse: 0.0158
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0184 - mae: 0.1111 - mse: 0.0184 - val_loss: 0.0666 - val_mae: 0.2169 - val_mse: 0.0666
Epoch 28/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0319 - mae: 0.1404 - mse: 0.0319
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 25ms/step - loss: 0.0262 - mae: 0.1267 - mse: 0.0262 - val_loss: 0.0584 - val_mae: 0.2004 - val_mse: 0.0584
Epoch 29/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 23ms/step - loss: 0.0156 - mae: 0.0987 - mse: 0.0156
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 27ms/step - loss: 0.0244 - mae: 0.1097 - mse: 0.0244 - val_loss: 0.0540 - val_mae: 0.1904 - val_mse: 0.0540
Epoch 30/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0150 - mae: 0.0967 - mse: 0.0150
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0237 - mae: 0.1107 - mse: 0.0237 - val_loss: 0.0699 - val_mae: 0.2183 - val_mse: 0.0699
Epoch 31/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0297 - mae: 0.1255 - mse: 0.0297
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0233 - mae: 0.1146 - mse: 0.0233 - val_loss: 0.0871 - val_mae: 0.2448 - val_mse: 0.0871
Epoch 32/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 20ms/step - loss: 0.0258 - mae: 0.1192 - mse: 0.0258
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0280 - mae: 0.1189 - mse: 0.0280 - val_loss: 0.0561 - val_mae: 0.1949 - val_mse: 0.0561
Epoch 33/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 21ms/step - loss: 0.0406 - mae: 0.1505 - mse: 0.0406
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0281 - mae: 0.1222 - mse: 0.0281 - val_loss: 0.0387 - val_mae: 0.1603 - val_mse: 0.0387
Epoch 34/100

[1m1/3[0m [32m======[0m[37m==============[0m [1m0s[0m 22ms/step - loss: 0.0213 - mae: 0.1074 - mse: 0.0213
[1m3/3[0m [32m====================[0m[37m[0m [1m0s[0m 26ms/step - loss: 0.0220 - mae: 0.1082 - mse: 0.0220 - val_loss: 0.0456 - val_mae: 0.1736 - val_mse: 0.0456
WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025C6AA2CE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:5 out of the last 45 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025C8F551120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
 nicon rmse  [test: 17.6381], [val: 16.3759], (fold: 0, id: 1) - [z2kcid]
 nicon rmse  [test: 20.1370], [val: 23.5953], (fold: 1, id: 2) - [xpivhe]
 nicon rmse  [test: 33.0420], [val: 24.3666], (fold: 2, id: 3) - [xvogx9]
 nicon rmse  [test: 22.2867], [val: 17.9289], (avg, id: 4) - [4xcobk]
 nicon rmse  [test: 21.4995], [val: 17.6535], (w_avg, id: 5) - [r95c9a]
------------------------------------------------------------------------------------------------------------------------
 Pipeline Best: nicon - rmse [test: 17.6381], [val: 16.3759], , (fold: 0, id: 1, step: 5) - [z2kcid]
 Pipeline config_633c70 completed successfully on dataset regression
========================================================================================================================
 Best prediction in run for dataset 'regression': nicon - rmse [test: 17.6381], [val: 16.3759], , (fold: 0, id: 1, step: 5) - [z2kcid] | [config_633c70]
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max     | SD     | CV     | R     | RMSE   | MSE     | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
| Cros Val | 44      | 5        | 29.025 | 23.145 | 2.380  | 111.200 | 21.320 | 0.735  | 0.410  | 16.376 | 268.169 | 15.887 | 13.043 | 1.34   | 3.973  | 79.5        |
| Train    | 86      | 5        | 31.003 | 23.140 | 2.050  | 128.310 | 24.623 | 0.794  | 0.501  | 17.387 | 302.324 | 17.144 | 13.169 | 1.44   | 2.897  | 90.7        |
| Test     | 59      | 5        | 31.762 | 27.110 | 1.330  | 84.570  | 19.805 | 0.624  | 0.207  | 17.638 | 311.104 | 17.235 | 14.683 | 1.15   | 3.750  | 67.8        |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
 Saved predictions to results\regression\10-17_10h2132s_Best_prediction_run_config_633c70_nicon_[z2kcid].csv
========================================================================================================================
=== Q4 - Model Persistence and Prediction Example ===
--- Source Model ---
Best model: nicon (id: z2kcid)
Reference predictions: [28.38739777 26.43656731 30.46665001 27.68544579 38.2446785 ]
--------------------------------------------------------------------------------
--- Method 1: Predict with a prediction entry ---
========================================================================================================================
 Starting Nirs4all prediction(s)
========================================================================================================================
 Loading results\regression\config_633c70\pipeline.json, results\regression\config_633c70\metadata.json
 5 binaries found
 Starting pipeline prediction on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Step 1: {'class': 'sklearn.preprocessing._data.MinMaxScaler'}
 Executing controller TransformerMixinController with operator MinMaxScaler
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.0, max=1.128, mean=0.801, var=0.033)
Targets: (samples=189, targets=1, processings=['numeric'])
- numeric: min=1.33, max=128.31, mean=30.779
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1']: 59 samples
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'y_processing': {'class': 'sklearn.preprocessing._data.MinMaxScaler'}}
 Executing controller YTransformerMixinController with operator MinMaxScaler
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 1, 2151), processings=['raw_MinMaxScaler_1'], min=0.0, max=1.128, mean=0.801, var=0.033)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=N/A, max=N/A, mean=N/A
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1']: 59 samples
------------------------------------------------------------------------------------------------------------------------
 Step 3: {'feature_augmentation': [{'class': 'nirs4all.operators.transformations.scalers.StandardNormalVariate'}, {'class': 'nirs4all.operators.transformations.nirs.SavitzkyGolay'}, {'class': 'nirs4all.operators.transformations.signal.Gaussian'}, {'class': 'nirs4all.operators.transformations.nirs.Haar'}]}
 Executing controller FeatureAugmentationController with operator list
    Sub-step 3.1: {'class': 'nirs4all.operators.transformations.scalers.StandardNormalVariate'}
 Executing controller TransformerMixinController with operator StandardNormalVariate
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 2, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1'], min=-5.548, max=7.24, mean=0.401, var=0.677)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=N/A, max=N/A, mean=N/A
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1']: 59 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 3.2: {'class': 'nirs4all.operators.transformations.nirs.SavitzkyGolay'}
 Executing controller TransformerMixinController with operator SavitzkyGolay
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 3, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2'], min=-5.548, max=7.24, mean=0.534, var=0.498)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=N/A, max=N/A, mean=N/A
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2']: 59 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 3.3: {'class': 'nirs4all.operators.transformations.signal.Gaussian'}
 Executing controller TransformerMixinController with operator Gaussian
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 4, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3'], min=-5.548, max=7.24, mean=0.401, var=0.427)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=N/A, max=N/A, mean=N/A
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3']: 59 samples
------------------------------------------------------------------------------------------------------------------------
    Sub-step 3.4: {'class': 'nirs4all.operators.transformations.nirs.Haar'}
 Executing controller TransformerMixinController with operator Haar
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 5, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4'], min=-5.548, max=7.24, mean=0.321, var=0.367)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=N/A, max=N/A, mean=N/A
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 59 samples
------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 5, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4'], min=-5.548, max=7.24, mean=0.321, var=0.367)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=N/A, max=N/A, mean=N/A
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 59 samples
------------------------------------------------------------------------------------------------------------------------
 Step 4: {'class': 'sklearn.model_selection._split.RepeatedKFold', 'params': {'n_splits': 3, 'n_repeats': 1, 'random_state': 42}}
 Executing controller CrossValidatorController with operator RepeatedKFold
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression (regression)
Features (samples=189, sources=1):
- Source 0: (189, 5, 2151), processings=['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4'], min=-5.548, max=7.24, mean=0.321, var=0.367)
Targets: (samples=189, targets=1, processings=['numeric', 'numeric_MinMaxScaler1'])
- numeric: min=1.33, max=128.31, mean=30.779
- numeric_MinMaxScaler1: min=N/A, max=N/A, mean=N/A
Indexes:
- partition - "train", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 130 samples
- partition - "test", processings - ['raw_MinMaxScaler_1', 'raw_MinMaxScaler_1_StandardNormalVariate_1', 'raw_MinMaxScaler_1_SavitzkyGolay_2', 'raw_MinMaxScaler_1_Gaussian_3', 'raw_MinMaxScaler_1_Haar_4']: 59 samples
Folds: [(130, 0), (130, 0), (130, 0)]
------------------------------------------------------------------------------------------------------------------------
 Step 5: {'function': 'nirs4all.operators.models.cirad_tf.nicon'}
 Executing controller TensorFlowModelController with operator dict
 Model config: {'model_instance': <function nicon at 0x0000025C65EEE290>}
------------------------------------------------------------------------------------------------------------------------
 Predicted with: nicon [hi1nk3]
 Saved predictions to results\regression\Predict_[hi1nk3].csv
Method 1 predictions: [28.38739777 26.43656731 30.46665001 27.68544579 38.2446785 ]
Method 1 identical to training:  YES
================================================================================
########################################
Finished running: Q5_predict_NN.py
########################################
Launch: Q6_multisource.py
########################################
2025-10-17 10:21:37.020736: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:21:37.671499: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 Multiple train_x files found for sample_data/multi: 3 sources detected.
 Multiple test_x files found for sample_data/multi: 3 sources detected.
========================================================================================================================
 Starting Nirs4all run(s) with 5 pipeline on 1 dataset (5 total runs).
========================================================================================================================
 Starting pipeline config_a59c43 on dataset multi
------------------------------------------------------------------------------------------------------------------------
 Q6_PLS_3 rmse  [test: 17.3718], [val: 8.6635], (fold: 0, id: 1) - [dbq3e3]
 Q6_PLS_3 rmse  [test: 17.3319], [val: 19.3439], (fold: 1, id: 2) - [jy7l6y]
 Q6_PLS_3 rmse  [test: 17.2668], [val: 26.9740], (fold: 2, id: 3) - [r9d6f4]
 Q6_PLS_3 rmse  [test: 17.1043], [val: 9.1722], (avg, id: 4) - [tmj7yw]
 Q6_PLS_3 rmse  [test: 17.1672], [val: 7.2897], (w_avg, id: 5) - [4nhgbw]
 ElasticNet rmse  [test: 19.9287], [val: 23.0084], (fold: 0, id: 1) - [koo397]
 ElasticNet rmse  [test: 19.8479], [val: 17.6908], (fold: 1, id: 2) - [nuetjj]
 ElasticNet rmse  [test: 19.9201], [val: 33.1704], (fold: 2, id: 3) - [h1ur7p]
 ElasticNet rmse  [test: 19.8947], [val: 25.3834], (avg, id: 4) - [vndc0w]
 ElasticNet rmse  [test: 19.8869], [val: 25.3682], (w_avg, id: 5) - [y9ople]
 Q6_PLS_2 rmse  [test: 17.3718], [val: 8.6635], (fold: 0, id: 1) - [0kftgy]
 Q6_PLS_2 rmse  [test: 17.3319], [val: 19.3439], (fold: 1, id: 2) - [5umrzg]
 Q6_PLS_2 rmse  [test: 17.2668], [val: 26.9740], (fold: 2, id: 3) - [w6n9h5]
 Q6_PLS_2 rmse  [test: 17.1043], [val: 9.1722], (avg, id: 4) - [euc55i]
 Q6_PLS_2 rmse  [test: 17.1672], [val: 7.2897], (w_avg, id: 5) - [8d3m9h]
 Pipeline Best: Q6_PLS_3 - rmse [test: 17.1672], [val: 7.2897], , (fold: w_avg, id: 5, step: 7) - [4nhgbw]
========================================================================================================================
 Starting pipeline config_2f5e45 on dataset multi
------------------------------------------------------------------------------------------------------------------------
 Q6_PLS_3 rmse  [test: 15.9335], [val: 15.1180], (fold: 0, id: 1) - [h4m5s9]
 Q6_PLS_3 rmse  [test: 17.7626], [val: 13.4764], (fold: 1, id: 2) - [e9u5sk]
 Q6_PLS_3 rmse  [test: 16.6391], [val: 22.5797], (fold: 2, id: 3) - [qoom62]
 Q6_PLS_3 rmse  [test: 16.6095], [val: 8.1590], (avg, id: 4) - [iysda8]
 Q6_PLS_3 rmse  [test: 16.6753], [val: 7.8152], (w_avg, id: 5) - [o8lg4r]
 ElasticNet rmse  [test: 19.8059], [val: 17.0480], (fold: 0, id: 1) - [etzpfn]
 ElasticNet rmse  [test: 19.8683], [val: 18.2544], (fold: 1, id: 2) - [idjsij]
 ElasticNet rmse  [test: 19.8627], [val: 25.1666], (fold: 2, id: 3) - [ez115b]
 ElasticNet rmse  [test: 19.8356], [val: 20.2855], (avg, id: 4) - [f6ab51]
 ElasticNet rmse  [test: 19.8322], [val: 20.2956], (w_avg, id: 5) - [0m0r8v]
 Q6_PLS_2 rmse  [test: 15.9335], [val: 15.1180], (fold: 0, id: 1) - [0bfjkq]
 Q6_PLS_2 rmse  [test: 17.7626], [val: 13.4764], (fold: 1, id: 2) - [o5dnea]
 Q6_PLS_2 rmse  [test: 16.6391], [val: 22.5797], (fold: 2, id: 3) - [srxnl3]
 Q6_PLS_2 rmse  [test: 16.6095], [val: 8.1590], (avg, id: 4) - [pbsevw]
 Q6_PLS_2 rmse  [test: 16.6753], [val: 7.8152], (w_avg, id: 5) - [9xzj8x]
 Pipeline Best: Q6_PLS_3 - rmse [test: 16.6753], [val: 7.8152], , (fold: w_avg, id: 5, step: 7) - [o8lg4r]
========================================================================================================================
 Starting pipeline config_d8972f on dataset multi
------------------------------------------------------------------------------------------------------------------------
 Q6_PLS_3 rmse  [test: 17.1554], [val: 23.4124], (fold: 0, id: 1) - [q61i96]
 Q6_PLS_3 rmse  [test: 16.0705], [val: 18.3572], (fold: 1, id: 2) - [ehhvkt]
 Q6_PLS_3 rmse  [test: 15.4637], [val: 19.1010], (fold: 2, id: 3) - [pcsqei]
 Q6_PLS_3 rmse  [test: 16.0044], [val: 11.0109], (avg, id: 4) - [f4ijq8]
 Q6_PLS_3 rmse  [test: 15.9459], [val: 10.8974], (w_avg, id: 5) - [cuov90]
 ElasticNet rmse  [test: 19.8559], [val: 21.2455], (fold: 0, id: 1) - [glts7i]
 ElasticNet rmse  [test: 19.8295], [val: 17.9068], (fold: 1, id: 2) - [j1u40r]
 ElasticNet rmse  [test: 19.8356], [val: 23.0704], (fold: 2, id: 3) - [7xl3q6]
 ElasticNet rmse  [test: 19.8395], [val: 20.8356], (avg, id: 4) - [4lzen0]
 ElasticNet rmse  [test: 19.8390], [val: 20.8367], (w_avg, id: 5) - [1yh1ku]
 Q6_PLS_2 rmse  [test: 17.1554], [val: 23.4124], (fold: 0, id: 1) - [xq3l98]
 Q6_PLS_2 rmse  [test: 16.0705], [val: 18.3572], (fold: 1, id: 2) - [c37s0t]
 Q6_PLS_2 rmse  [test: 15.4637], [val: 19.1010], (fold: 2, id: 3) - [n93gn2]
 Q6_PLS_2 rmse  [test: 16.0044], [val: 11.0109], (avg, id: 4) - [clqgg1]
 Q6_PLS_2 rmse  [test: 15.9459], [val: 10.8974], (w_avg, id: 5) - [sl60w9]
 Pipeline Best: Q6_PLS_3 - rmse [test: 15.9459], [val: 10.8974], , (fold: w_avg, id: 5, step: 7) - [cuov90]
========================================================================================================================
 Starting pipeline config_ccef63 on dataset multi
------------------------------------------------------------------------------------------------------------------------
 Q6_PLS_3 rmse  [test: 14.5854], [val: 10.1667], (fold: 0, id: 1) - [4te8t2]
 Q6_PLS_3 rmse  [test: 14.9407], [val: 13.1608], (fold: 1, id: 2) - [6a1lye]
 Q6_PLS_3 rmse  [test: 14.9095], [val: 15.0198], (fold: 2, id: 3) - [dibhe3]
 Q6_PLS_3 rmse  [test: 14.6875], [val: 6.0607], (avg, id: 4) - [bu6ksm]
 Q6_PLS_3 rmse  [test: 14.6714], [val: 6.0702], (w_avg, id: 5) - [cc4ol2]
 ElasticNet rmse  [test: 19.8231], [val: 14.9904], (fold: 0, id: 1) - [26zc9a]
 ElasticNet rmse  [test: 19.8164], [val: 11.6040], (fold: 1, id: 2) - [xf5jm9]
 ElasticNet rmse  [test: 19.8328], [val: 17.9461], (fold: 2, id: 3) - [6941nf]
 ElasticNet rmse  [test: 19.8235], [val: 15.0565], (avg, id: 4) - [0sa5g4]
 ElasticNet rmse  [test: 19.8224], [val: 15.0667], (w_avg, id: 5) - [rkpmlv]
 Q6_PLS_2 rmse  [test: 14.5854], [val: 10.1667], (fold: 0, id: 1) - [xodntn]
 Q6_PLS_2 rmse  [test: 14.9407], [val: 13.1608], (fold: 1, id: 2) - [tk2ivi]
 Q6_PLS_2 rmse  [test: 14.9095], [val: 15.0198], (fold: 2, id: 3) - [z8zune]
 Q6_PLS_2 rmse  [test: 14.6875], [val: 6.0607], (avg, id: 4) - [vxlf3v]
 Q6_PLS_2 rmse  [test: 14.6714], [val: 6.0702], (w_avg, id: 5) - [ynagti]
 Pipeline Best: Q6_PLS_3 - rmse [test: 14.6875], [val: 6.0607], , (fold: avg, id: 4, step: 7) - [bu6ksm]
========================================================================================================================
 Starting pipeline config_571e2e on dataset multi
------------------------------------------------------------------------------------------------------------------------
 Q6_PLS_3 rmse  [test: 14.2601], [val: 14.4444], (fold: 0, id: 1) - [1mjl5w]
 Q6_PLS_3 rmse  [test: 14.6533], [val: 12.6895], (fold: 1, id: 2) - [ca5ywx]
 Q6_PLS_3 rmse  [test: 13.9470], [val: 7.0324], (fold: 2, id: 3) - [04guz8]
 Q6_PLS_3 rmse  [test: 14.1889], [val: 5.2045], (avg, id: 4) - [qbk6y5]
 Q6_PLS_3 rmse  [test: 14.1287], [val: 4.8772], (w_avg, id: 5) - [6fl4zq]
 ElasticNet rmse  [test: 19.8158], [val: 15.3507], (fold: 0, id: 1) - [xy7x0o]
 ElasticNet rmse  [test: 19.8375], [val: 14.7019], (fold: 1, id: 2) - [wdhjq1]
 ElasticNet rmse  [test: 19.8146], [val: 15.4632], (fold: 2, id: 3) - [k95seg]
 ElasticNet rmse  [test: 19.8213], [val: 15.1413], (avg, id: 4) - [s34cx9]
 ElasticNet rmse  [test: 19.8215], [val: 15.1392], (w_avg, id: 5) - [ria294]
 Q6_PLS_2 rmse  [test: 14.2601], [val: 14.4444], (fold: 0, id: 1) - [ly51wd]
 Q6_PLS_2 rmse  [test: 14.6533], [val: 12.6895], (fold: 1, id: 2) - [ut8kyc]
 Q6_PLS_2 rmse  [test: 13.9470], [val: 7.0324], (fold: 2, id: 3) - [17aift]
 Q6_PLS_2 rmse  [test: 14.1889], [val: 5.2045], (avg, id: 4) - [hduur9]
 Q6_PLS_2 rmse  [test: 14.1287], [val: 4.8772], (w_avg, id: 5) - [fcuf7b]
 Pipeline Best: Q6_PLS_3 - rmse [test: 14.1287], [val: 4.8772], , (fold: w_avg, id: 5, step: 7) - [6fl4zq]
========================================================================================================================
 Best prediction in run for dataset 'multi': Q6_PLS_3 - rmse [test: 14.1287], [val: 4.8772], , (fold: w_avg, id: 5, step: 7) - [6fl4zq] | [config_571e2e]
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max     | SD     | CV     | R     | RMSE   | MSE     | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
| Cros Val | 39      | 25812    | 24.770 | 22.850 | 2.050  | 57.400  | 13.822 | 0.558  | 0.875  | 4.877  | 23.787  | 4.850  | 3.766  | 2.85   | 0.517  | 100.0       |
| Train    | 130     | 25812    | 30.333 | 23.140 | 2.050  | 128.310 | 23.575 | 0.777  | 0.982  | 3.192  | 10.186  | 3.185  | 2.418  | 7.40   | 0.198  | 100.0       |
| Test     | 59      | 25812    | 31.762 | 27.110 | 1.330  | 84.570  | 19.805 | 0.624  | 0.491  | 14.129 | 199.619 | 14.040 | 11.488 | 1.41   | 1.583  | 83.1        |
|----------|---------|----------|--------|--------|--------|---------|--------|--------|--------|--------|---------|--------|--------|--------|--------|-------------|
 Saved predictions to results\multi\10-17_10h2152s_Best_prediction_run_config_571e2e_Q6_PLS_3_[6fl4zq].csv
========================================================================================================================
Top 5 models by rmse:
1. Q6_PLS_3 - rmse [test: 14.1287], [val: 4.8772], , (fold: w_avg, id: 5, step: 7) - [6fl4zq] - MinMax>MinMax|MinMax>SNV>Haar>SG>MinMax|MinMax>Gauss>SNV>SG>MinMax|MinMax>Gauss>SG>Haar>MinMax
2. Q6_PLS_2 - rmse [test: 14.1287], [val: 4.8772], , (fold: w_avg, id: 5, step: 9) - [fcuf7b] - MinMax>MinMax|MinMax>SNV>Haar>SG>MinMax|MinMax>Gauss>SNV>SG>MinMax|MinMax>Gauss>SG>Haar>MinMax
3. Q6_PLS_3 - rmse [test: 14.1889], [val: 5.2045], , (fold: avg, id: 4, step: 7) - [qbk6y5] - MinMax>MinMax|MinMax>SNV>Haar>SG>MinMax|MinMax>Gauss>SNV>SG>MinMax|MinMax>Gauss>SG>Haar>MinMax
4. Q6_PLS_2 - rmse [test: 14.1889], [val: 5.2045], , (fold: avg, id: 4, step: 9) - [hduur9] - MinMax>MinMax|MinMax>SNV>Haar>SG>MinMax|MinMax>Gauss>SNV>SG>MinMax|MinMax>Gauss>SG>Haar>MinMax
5. Q6_PLS_3 - rmse [test: 14.6875], [val: 6.0607], , (fold: avg, id: 4, step: 7) - [bu6ksm] - MinMax>MinMax|MinMax>SG>Haar>MinMax|MinMax>Gauss>SNV>Haar>MinMax|MinMax>Gauss>SG>Haar>MinMax

=== Q6 - Multisource Model + Reuse Example ===
--- Source Model ---
Best model: Q6_PLS_3 (id: 6fl4zq)
Reference predictions: [26.8166009  32.47546422 17.84073075 23.57432933 32.88882718]
--------------------------------------------------------------------------------
================================================================================
--- Predict with saved model ID ---
 Multiple train_x files found for sample_data/multi: 3 sources detected.
 Multiple test_x files found for sample_data/multi: 3 sources detected.
Using model ID: [6fl4zq] in config_571e2e
========================================================================================================================
 Starting Nirs4all prediction(s)
========================================================================================================================
 Starting pipeline prediction on dataset multi
------------------------------------------------------------------------------------------------------------------------
 Predicted with: Q6_PLS_3 [idqnri]
 Saved predictions to results\multi\Predict_[idqnri].csv
Reuse predictions: [26.8166009  32.47546422 17.84073075 23.57432933 32.88882718]
Model reuse identical to training:  YES
PredictionResult(id=6fl4zq, model=Q6_PLS_3, dataset=multi, fold=w_avg, step=7, op=5)
########################################
Finished running: Q6_multisource.py
########################################
Launch: Q7_discretization.py
########################################
2025-10-17 10:21:58.041887: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:21:58.719613: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
========================================================================================================================
 Starting Nirs4all run(s) with 5 pipeline on 1 dataset (5 total runs).
========================================================================================================================
 Starting pipeline Q7_classification_3a075d on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Task type updated from regression to classification
 RandomForest-depth-5 accuracy  [test: 0.1695], [val: 0.0909], (fold: 0, id: 1) - [voazz1]
 RandomForest-depth-5 accuracy  [test: 0.2203], [val: 0.2424], (fold: 1, id: 2) - [ane8xh]
 RandomForest-depth-5 accuracy  [test: 0.1525], [val: 0.2121], (fold: 2, id: 3) - [u9z9d9]
 RandomForest-depth-10 accuracy  [test: 0.1695], [val: 0.0909], (fold: 0, id: 1) - [9is29s]
 RandomForest-depth-10 accuracy  [test: 0.2034], [val: 0.1515], (fold: 1, id: 2) - [7maajz]
 RandomForest-depth-10 accuracy  [test: 0.2373], [val: 0.2727], (fold: 2, id: 3) - [v312mc]
 RandomForest-depth-15 accuracy  [test: 0.1864], [val: 0.0606], (fold: 0, id: 1) - [mcl1bm]
 RandomForest-depth-15 accuracy  [test: 0.2373], [val: 0.1818], (fold: 1, id: 2) - [fjwxqn]
 RandomForest-depth-15 accuracy  [test: 0.1695], [val: 0.2121], (fold: 2, id: 3) - [dhy34v]
 Pipeline Best: RandomForest-depth-10 - accuracy [test: 0.2373], [val: 0.2727], , (fold: 2, id: 3, step: 8) - [v312mc]
========================================================================================================================
 Starting pipeline Q7_classification_36a6d0 on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Task type updated from regression to classification
 RandomForest-depth-5 accuracy  [test: 0.2203], [val: 0.2424], (fold: 0, id: 1) - [jrvf0g]
 RandomForest-depth-5 accuracy  [test: 0.3051], [val: 0.2121], (fold: 1, id: 2) - [6wcm82]
 RandomForest-depth-5 accuracy  [test: 0.2373], [val: 0.1818], (fold: 2, id: 3) - [j8zv78]
 RandomForest-depth-10 accuracy  [test: 0.2203], [val: 0.1818], (fold: 0, id: 1) - [yfnyxt]
 RandomForest-depth-10 accuracy  [test: 0.3559], [val: 0.1818], (fold: 1, id: 2) - [b38v6n]
 RandomForest-depth-10 accuracy  [test: 0.2542], [val: 0.1212], (fold: 2, id: 3) - [jztwzh]
 RandomForest-depth-15 accuracy  [test: 0.2542], [val: 0.3030], (fold: 0, id: 1) - [7e5wdh]
 RandomForest-depth-15 accuracy  [test: 0.2203], [val: 0.2121], (fold: 1, id: 2) - [vjle24]
 RandomForest-depth-15 accuracy  [test: 0.2712], [val: 0.1515], (fold: 2, id: 3) - [hhyog8]
 Pipeline Best: RandomForest-depth-15 - accuracy [test: 0.2542], [val: 0.3030], , (fold: 0, id: 1, step: 9) - [7e5wdh]
========================================================================================================================
 Starting pipeline Q7_classification_ae1b76 on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Task type updated from regression to classification
 RandomForest-depth-5 accuracy  [test: 0.4407], [val: 0.2727], (fold: 0, id: 1) - [8b5pxd]
 RandomForest-depth-5 accuracy  [test: 0.3390], [val: 0.3333], (fold: 1, id: 2) - [o1r0z1]
 RandomForest-depth-5 accuracy  [test: 0.3729], [val: 0.3030], (fold: 2, id: 3) - [v6xvt9]
 RandomForest-depth-10 accuracy  [test: 0.4407], [val: 0.3939], (fold: 0, id: 1) - [fa9e3q]
 RandomForest-depth-10 accuracy  [test: 0.3390], [val: 0.3636], (fold: 1, id: 2) - [p0cwat]
 RandomForest-depth-10 accuracy  [test: 0.2712], [val: 0.3636], (fold: 2, id: 3) - [qs3q2j]
 RandomForest-depth-15 accuracy  [test: 0.3390], [val: 0.3636], (fold: 0, id: 1) - [pw4x9a]
 RandomForest-depth-15 accuracy  [test: 0.2881], [val: 0.2424], (fold: 1, id: 2) - [ywsfgg]
 RandomForest-depth-15 accuracy  [test: 0.2881], [val: 0.2727], (fold: 2, id: 3) - [qelneo]
 Pipeline Best: RandomForest-depth-10 - accuracy [test: 0.4407], [val: 0.3939], , (fold: 0, id: 1, step: 8) - [fa9e3q]
========================================================================================================================
 Starting pipeline Q7_classification_4da161 on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Task type updated from regression to classification
 RandomForest-depth-5 accuracy  [test: 0.3559], [val: 0.3030], (fold: 0, id: 1) - [r5n3lk]
 RandomForest-depth-5 accuracy  [test: 0.2373], [val: 0.3939], (fold: 1, id: 2) - [30rat5]
 RandomForest-depth-5 accuracy  [test: 0.3051], [val: 0.3333], (fold: 2, id: 3) - [ccmr3o]
 RandomForest-depth-10 accuracy  [test: 0.4237], [val: 0.3333], (fold: 0, id: 1) - [zu8rhr]
 RandomForest-depth-10 accuracy  [test: 0.2881], [val: 0.2727], (fold: 1, id: 2) - [vgkw1b]
 RandomForest-depth-10 accuracy  [test: 0.3051], [val: 0.2727], (fold: 2, id: 3) - [hpp7f3]
 RandomForest-depth-15 accuracy  [test: 0.3051], [val: 0.3030], (fold: 0, id: 1) - [17frcs]
 RandomForest-depth-15 accuracy  [test: 0.2881], [val: 0.2727], (fold: 1, id: 2) - [zub2xr]
 RandomForest-depth-15 accuracy  [test: 0.3729], [val: 0.3333], (fold: 2, id: 3) - [sobgkm]
 Pipeline Best: RandomForest-depth-5 - accuracy [test: 0.2373], [val: 0.3939], , (fold: 1, id: 2, step: 7) - [30rat5]
========================================================================================================================
 Starting pipeline Q7_classification_9413f8 on dataset regression
------------------------------------------------------------------------------------------------------------------------
 Task type updated from regression to classification
 RandomForest-depth-5 accuracy  [test: 0.3051], [val: 0.3333], (fold: 0, id: 1) - [fdqh03]
 RandomForest-depth-5 accuracy  [test: 0.2373], [val: 0.2424], (fold: 1, id: 2) - [5wnodz]
 RandomForest-depth-5 accuracy  [test: 0.2881], [val: 0.2727], (fold: 2, id: 3) - [d95xff]
 RandomForest-depth-10 accuracy  [test: 0.3220], [val: 0.3333], (fold: 0, id: 1) - [97mvm1]
 RandomForest-depth-10 accuracy  [test: 0.2373], [val: 0.2424], (fold: 1, id: 2) - [t18ru4]
 RandomForest-depth-10 accuracy  [test: 0.2373], [val: 0.3333], (fold: 2, id: 3) - [g36jgm]
 RandomForest-depth-15 accuracy  [test: 0.3390], [val: 0.3636], (fold: 0, id: 1) - [5mzd0c]
 RandomForest-depth-15 accuracy  [test: 0.2373], [val: 0.3030], (fold: 1, id: 2) - [pyh1lx]
 RandomForest-depth-15 accuracy  [test: 0.2712], [val: 0.3636], (fold: 2, id: 3) - [vxjhd4]
 Pipeline Best: RandomForest-depth-15 - accuracy [test: 0.3390], [val: 0.3636], , (fold: 0, id: 1, step: 9) - [5mzd0c]
========================================================================================================================
 Best prediction in run for dataset 'regression': RandomForest-depth-10 - accuracy [test: 0.4407], [val: 0.3939], , (fold: 0, id: 1, step: 8) - [fa9e3q] | [Q7_classification_ae1b76]
|----------|---------|-----------|----------|-----------|--------|----------|-------------|
|          | Nsample | Nfeatures | Accuracy | Precision | Recall | F1-score | Specificity |
|----------|---------|-----------|----------|-----------|--------|----------|-------------|
| Cros Val | 33      | 4302      | 0.394    | 0.445     | 0.394  | 0.386    | 0.845       |
| Train    | 97      | 4302      | 1.000    | 1.000     | 1.000  | 1.000    | 1.000       |
| Test     | 59      | 4302      | 0.441    | 0.449     | 0.441  | 0.439    | 0.854       |
|----------|---------|-----------|----------|-----------|--------|----------|-------------|
========================================================================================================================
########################################
Finished running: Q7_discretization.py
########################################
Launch: Q8_shap.py
########################################
2025-10-17 10:22:18.792343: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:22:19.468273: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Q8 - SHAP Model Explanation Example
Training models...
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline config_441eb5 on dataset regression_2
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 PLSRegression rmse  [test: 1.0842], [val: 1.0842], ( - [qtm68s]
 Pipeline Best: PLSRegression - rmse [test: 1.0842], [val: 1.0842], , (fold: None, id: 1, step: 6) - [qtm68s]
========================================================================================================================
 Best prediction in run for dataset 'regression_2': PLSRegression - rmse [test: 1.0842], [val: 1.0842], , (fold: None, id: 1, step: 6) - [qtm68s] | [config_441eb5]
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
|          | Nsample | Nfeature | Mean   | Median | Min    | Max    | SD     | CV     | R     | RMSE   | MSE    | SEP    | MAE    | RPD    | Bias   | Consistency |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
| Cros Val | 150     | 125      | 11.677 | 11.500 | 8.400  | 17.600 | 2.129  | 0.182  | 0.741  | 1.084  | 1.175  | 1.083  | 0.838  | 1.97   | 0.043  | 94.0        |
| Train    | 885     | 125      | 12.544 | 12.800 | 9.500  | 16.700 | 1.284  | 0.102  | 0.832  | 0.526  | 0.277  | 0.526  | 0.409  | 2.44   | -0.000 | 98.6        |
| Test     | 150     | 125      | 11.677 | 11.500 | 8.400  | 17.600 | 2.129  | 0.182  | 0.741  | 1.084  | 1.175  | 1.083  | 0.838  | 1.97   | 0.043  | 94.0        |
|----------|---------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|-------------|
 Saved predictions to results\regression_2\10-17_10h2221s_Best_prediction_run_config_441eb5_PLSRegression_[qtm68s].csv
========================================================================================================================
Best model: PLSRegression (RMSE: 1.0842)
Running SHAP analysis...
========================================================================================================================
 Starting SHAP Explanation Analysis
========================================================================================================================
 Starting pipeline prediction on dataset regression_2
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
================================================================================
 SHAP Analysis Starting
================================================================================
 Analyzing 150 samples with 125 features
 Creating SHAP explainer (type: auto)...
   Auto-selected explainer: linear
 Computing SHAP values...
 SHAP values computed: shape=(150, 125)
   Base value: 0.4818
   Binning config:
     spectral: size=10, stride=2, agg=mean
     waterfall: size=20, stride=10, agg=mean
     beeswarm: size=20, stride=20, agg=mean

 Generating visualizations...
    Saved: results\regression_2\regression_2\config_441eb5\explanations\qtm68s\spectral_importance.png
    Spectral importance
    Saved: results\regression_2\regression_2\config_441eb5\explanations\qtm68s\waterfall_binned.png
    Waterfall plot (binned)
    Saved: results\regression_2\regression_2\config_441eb5\explanations\qtm68s\beeswarm_binned.png
    Beeswarm plot (binned)

 SHAP analysis completed!
================================================================================
########################################
Finished running: Q8_shap.py
########################################
Launch: Q10_resampler.py
########################################
2025-10-17 10:22:25.357737: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:22:26.002966: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
======================================================================
NIRS4ALL - Resampler Example
======================================================================

Example 1: Resample using wavelengths from classification dataset
----------------------------------------------------------------------
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Dataset: regression_3 (regression)
Features (samples=1040, sources=1):
- Source 0: (1040, 1, 128), processings=['raw'], min=-0.159, max=1.069, mean=0.302, var=0.071)
Targets: (samples=1040, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.414
Indexes:
- partition - "train", processings - ['raw']: 890 samples
- partition - "test", processings - ['raw']: 150 samples
Metadata(rows=1040, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
 Starting pipeline Other_Dataset_Pipeline_b92b07 on dataset regression_3
------------------------------------------------------------------------------------------------------------------------
 Step 1: chart_2d
 Executing controller SpectraChartController without operator
   Source 0: 1 processings: ['raw']
   Data shape: (1040, 1, 128)
   Headers available: 128, features: 128
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'class': 'nirs4all.operators.transformations.resampler.Resampler', '_runtime_instance': Resampler(method='linear', n_out=125, unfitted)}
 Executing controller ResamplerController with operator Resampler
Exporting resampled features for dataset 'regression_3', source 0 to CSV...
['raw_Resampler_1']
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression_3 (regression)
Features (samples=1040, sources=1):
- Source 0: (1040, 1, 125), processings=['raw_Resampler_1'], min=-0.159, max=1.069, mean=0.282, var=0.072)
Targets: (samples=1040, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.414
Indexes:
- partition - "train", processings - ['raw_Resampler_1']: 890 samples
- partition - "test", processings - ['raw_Resampler_1']: 150 samples
Metadata(rows=1040, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
------------------------------------------------------------------------------------------------------------------------
 Step 3: chart_2d
 Executing controller SpectraChartController without operator
   Source 0: 1 processings: ['raw_Resampler_1']
   Data shape: (1040, 1, 125)
   Headers available: 125, features: 125
------------------------------------------------------------------------------------------------------------------------
========================================================================================================================

Example 2: Downsample from 125 to 10 wavelengths (descending)
----------------------------------------------------------------------
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Dataset: regression_3 (regression)
Features (samples=1040, sources=1):
- Source 0: (1040, 1, 128), processings=['raw'], min=-0.159, max=1.069, mean=0.302, var=0.071)
Targets: (samples=1040, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.414
Indexes:
- partition - "train", processings - ['raw']: 890 samples
- partition - "test", processings - ['raw']: 150 samples
Metadata(rows=1040, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
 Starting pipeline Downsample_Pipeline_b92b07 on dataset regression_3
------------------------------------------------------------------------------------------------------------------------
 Step 1: chart_2d
 Executing controller SpectraChartController without operator
   Source 0: 1 processings: ['raw']
   Data shape: (1040, 1, 128)
   Headers available: 128, features: 128
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'class': 'nirs4all.operators.transformations.resampler.Resampler', '_runtime_instance': Resampler(method='linear', n_out=10, unfitted)}
 Executing controller ResamplerController with operator Resampler
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression_3 (regression)
Features (samples=1040, sources=1):
- Source 0: (1040, 1, 10), processings=['raw_Resampler_1'], min=-0.155, max=0.986, mean=0.164, var=0.047)
Targets: (samples=1040, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.414
Indexes:
- partition - "train", processings - ['raw_Resampler_1']: 890 samples
- partition - "test", processings - ['raw_Resampler_1']: 150 samples
Metadata(rows=1040, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
------------------------------------------------------------------------------------------------------------------------
 Step 3: chart_2d
 Executing controller SpectraChartController without operator
   Source 0: 1 processings: ['raw_Resampler_1']
   Data shape: (1040, 1, 10)
   Headers available: 10, features: 10
------------------------------------------------------------------------------------------------------------------------
========================================================================================================================

Example 3: Resample to fingerprint region (9500-7000 cm)
----------------------------------------------------------------------
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline Cropped_Pipeline_b92b07 on dataset regression_3
------------------------------------------------------------------------------------------------------------------------
Exporting resampled features for dataset 'regression_3', source 0 to CSV...
['raw_Resampler_1']
========================================================================================================================
########################################
Finished running: Q10_resampler.py
########################################
Launch: Q11_flexible_inputs.py
########################################
2025-10-17 10:22:35.001366: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:22:35.654346: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
====================================================================================================
Q11: Flexible Input Formats Demo
====================================================================================================

 Generated synthetic data: 200 samples, 100 features

 Pipeline defined with StandardScaler + Ridge regression

====================================================================================================
Example 1: Traditional Approach - PipelineConfigs + DatasetConfigs
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline traditional_0e69d7 on dataset traditional_dataset
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 Ridge rmse  [test: 1.5508], [val: 1.5508], ( - [1ks0t1]
 Pipeline Best: Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [1ks0t1]
========================================================================================================================
 Best prediction in run for dataset 'traditional_dataset': Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [1ks0t1] | [traditional_0e69d7]
========================================================================================================================
 Traditional approach works perfectly!

====================================================================================================
Example 2: Direct Approach - List[steps] + Tuple[X, y, partition_info]
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline direct_0e69d7 on dataset array_data
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 Ridge rmse  [test: 1.5508], [val: 1.5508], ( - [s87q19]
 Pipeline Best: Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [s87q19]
========================================================================================================================
 Best prediction in run for dataset 'array_data': Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [s87q19] | [direct_0e69d7]
========================================================================================================================
 Direct list + tuple approach - super simple!

====================================================================================================
Example 3: Dict pipeline + Tuple (X, y, train_size)
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline with_partition_0e69d7 on dataset partitioned_data
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 Ridge rmse  [test: 1.7158], [val: 1.7158], ( - [tleiuk]
 Pipeline Best: Ridge - rmse [test: 1.7158], [val: 1.7158], , (fold: None, id: 1, step: 2) - [tleiuk]
========================================================================================================================
 Best prediction in run for dataset 'partitioned_data': Ridge - rmse [test: 1.7158], [val: 1.7158], , (fold: None, id: 1, step: 2) - [tleiuk] | [with_partition_0e69d7]
========================================================================================================================
 Dict + (X, y, partition_dict) - precise control!

====================================================================================================
Example 4: List[steps] + SpectroDataset
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline config_0e69d7 on dataset custom_spectro
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 Ridge rmse  [test: 1.5508], [val: 1.5508], ( - [ejfn0o]
 Pipeline Best: Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [ejfn0o]
========================================================================================================================
 Best prediction in run for dataset 'custom_spectro': Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [ejfn0o] | [config_0e69d7]
========================================================================================================================
 List + SpectroDataset - full control over dataset!

====================================================================================================
Example 5: PipelineConfigs + Tuple[X, y, partition_info] (mixed formats)
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline mixed_0e69d7 on dataset mixed_tuple
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 Ridge rmse  [test: 1.5508], [val: 1.5508], ( - [0pvnmf]
 Pipeline Best: Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [0pvnmf]
========================================================================================================================
 Best prediction in run for dataset 'mixed_tuple': Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [0pvnmf] | [mixed_0e69d7]
========================================================================================================================
 PipelineConfigs + tuple - flexible mixing!

====================================================================================================
Example 6: Dict pipeline + Dict dataset config
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline config_0e69d7 on dataset dict_config
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 Ridge rmse  [test: 1.5508], [val: 1.5508], ( - [7henmj]
 Pipeline Best: Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [7henmj]
========================================================================================================================
 Best prediction in run for dataset 'dict_config': Ridge - rmse [test: 1.5508], [val: 1.5508], , (fold: None, id: 1, step: 2) - [7henmj] | [config_0e69d7]
========================================================================================================================
 Dict + Dict - consistent format!

====================================================================================================
Example 7: Advanced - Custom train/test indices
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline config_0e69d7 on dataset custom_split
------------------------------------------------------------------------------------------------------------------------
  WARNING: Using test set as validation set (no folds provided) 
 Ridge rmse  [test: 1.7158], [val: 1.7158], ( - [be1w13]
 Pipeline Best: Ridge - rmse [test: 1.7158], [val: 1.7158], , (fold: None, id: 1, step: 2) - [be1w13]
========================================================================================================================
 Best prediction in run for dataset 'custom_split': Ridge - rmse [test: 1.7158], [val: 1.7158], , (fold: None, id: 1, step: 2) - [be1w13] | [config_0e69d7]
========================================================================================================================
 Custom indices - precise control!

====================================================================================================
Example 8: Pipeline with cross-validation
====================================================================================================
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Starting pipeline cv_pipeline_49e4c0 on dataset cv_data
------------------------------------------------------------------------------------------------------------------------
 Ridge rmse  [test: 1.7699], [val: 2.1387], (fold: 0, id: 1) - [8na75k]
 Ridge rmse  [test: 2.4678], [val: 2.3375], (fold: 1, id: 2) - [rnogu2]
 Ridge rmse  [test: 2.5351], [val: 2.6081], (fold: 2, id: 3) - [wk90jg]
 Ridge rmse  [test: 1.7721], [val: 0.8936], (avg, id: 4) - [w6jg2m]
 Ridge rmse  [test: 1.7433], [val: 0.8868], (w_avg, id: 5) - [r10qui]
 Pipeline Best: Ridge - rmse [test: 1.7433], [val: 0.8868], , (fold: w_avg, id: 5, step: 3) - [r10qui]
========================================================================================================================
 Best prediction in run for dataset 'cv_data': Ridge - rmse [test: 1.7433], [val: 0.8868], , (fold: w_avg, id: 5, step: 3) - [r10qui] | [cv_pipeline_49e4c0]
========================================================================================================================
 Cross-validation with flexible inputs!

====================================================================================================
 SUMMARY - All 8 Examples Completed Successfully!
====================================================================================================

 Key Takeaways:
  1. No need to wrap everything in PipelineConfigs/DatasetConfigs anymore
  2. Direct numpy array support: just pass (X, y)
  3. Control partitioning with optional dict: (X, y, {'train': 160})
  4. Mix and match any format: List + Dict, Dict + SpectroDataset, etc.
  5. Backward compatible: old code still works perfectly
  6. Cleaner, more intuitive API for quick experiments

 Use cases:
  - Quick experiments: runner.run(steps, (X, y))
  - Prediction: runner.predict(model_ref, X_new)
  - Full control: Use PipelineConfigs/DatasetConfigs/SpectroDataset as before

====================================================================================================
 Q11 Example Complete!
====================================================================================================
########################################
Finished running: Q11_flexible_inputs.py
########################################
Launch: Q12_sample_augmentation.py
########################################
2025-10-17 10:22:39.508505: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-17 10:22:40.185850: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
========================================================================================================================
 Starting Nirs4all run(s) with 1 pipeline on 1 dataset (1 total runs).
========================================================================================================================
 Dataset: regression_2 (regression)
Features (samples=1035, sources=1):
- Source 0: (1035, 1, 125), processings=['raw'], min=0.007, max=0.353, mean=0.116, var=0.008)
Targets: (samples=1035, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.418
Indexes:
- partition - "train", processings - ['raw']: 885 samples
- partition - "test", processings - ['raw']: 150 samples
Metadata(rows=1035, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
 Starting pipeline q12_2b106e on dataset regression_2
------------------------------------------------------------------------------------------------------------------------
 Step 1: fold_chart
 Executing controller FoldChartController without operator
 No CV folds found. Creating visualization from train/test partition.
  Using train (885 samples including augmented) and test (150 samples) partitions.
------------------------------------------------------------------------------------------------------------------------
 Step 2: {'sample_augmentation': {'transformers': ['nirs4all.operators.augmentation.random.Rotate_Translate'], 'balance': 'y', 'target_size': 20, 'bins': 5, 'binning_strategy': 'equal_width', 'bin_balancing': 'value', 'selection': 'random', 'random_state': 42}}
 Executing controller SampleAugmentationController with operator dict
    Sub-step 2.1: nirs4all.operators.augmentation.random.Rotate_Translate
 Executing controller TransformerMixinController with operator Rotate_Translate
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression_2 (regression)
Features (samples=1060, sources=1):
- Source 0: (1060, 1, 125), processings=['raw'], min=0.006, max=0.353, mean=0.116, var=0.008)
Targets: (samples=1035, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.418
Indexes:
- partition - "train", processings - ['raw']: 885 samples
- partition - "test", processings - ['raw']: 150 samples
- partition - "train", processings - ['raw'], augmentation - "Rotate_Translate": 25 samples
Metadata(rows=1035, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression_2 (regression)
Features (samples=1060, sources=1):
- Source 0: (1060, 1, 125), processings=['raw'], min=0.006, max=0.353, mean=0.116, var=0.008)
Targets: (samples=1035, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.418
Indexes:
- partition - "train", processings - ['raw']: 885 samples
- partition - "test", processings - ['raw']: 150 samples
- partition - "train", processings - ['raw'], augmentation - "Rotate_Translate": 25 samples
Metadata(rows=1035, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
------------------------------------------------------------------------------------------------------------------------
 Step 3: fold_chart
 Executing controller FoldChartController without operator
 No CV folds found. Creating visualization from train/test partition.
  Using train (910 samples including augmented) and test (150 samples) partitions.
------------------------------------------------------------------------------------------------------------------------
 Step 4: {'split': {'class': 'sklearn.model_selection._split.GroupKFold', 'params': {'n_splits': 2}, '_runtime_instance': GroupKFold(n_splits=2, random_state=None, shuffle=False)}, 'group': 'Sample_ID'}
 Executing controller CrossValidatorController with operator GroupKFold
------------------------------------------------------------------------------------------------------------------------
Update:  Dataset: regression_2 (regression)
Features (samples=1060, sources=1):
- Source 0: (1060, 1, 125), processings=['raw'], min=0.006, max=0.353, mean=0.116, var=0.008)
Targets: (samples=1035, targets=1, processings=['numeric'])
- numeric: min=8.4, max=17.6, mean=12.418
Indexes:
- partition - "train", processings - ['raw']: 885 samples
- partition - "test", processings - ['raw']: 150 samples
- partition - "train", processings - ['raw'], augmentation - "Rotate_Translate": 25 samples
Metadata(rows=1035, columns=['Circuit', 'Nb_Rep', 'Type_Rep', 'Resolution', 'Unite_X', 'Abs_ou_Ref', 'Labo_ou_Terrain', 'Date', 'Heure', 'Temperature', 'Hygrometrie', 'Year', 'Sample_ID', 'Team', 'Spectro', 'Rep'])
Folds: [(440, 445), (445, 440)]
------------------------------------------------------------------------------------------------------------------------
 Step 5: fold_chart
 Executing controller FoldChartController without operator
------------------------------------------------------------------------------------------------------------------------
========================================================================================================================
########################################
Finished running: Q12_sample_augmentation.py
########################################
=== Q Examples Run Completed: 2025-10-17 10:22:46 ===
