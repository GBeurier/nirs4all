{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5fd12c",
   "metadata": {},
   "source": [
    "# Comprehensive SpectraSet Workflow Demo\n",
    "\n",
    "This notebook demonstrates a detailed workflow using the `SpectraSet` class, including:\n",
    "- Custom data generation for NIRS and Raman spectroscopy.\n",
    "- Sample augmentation with multiple custom transformers.\n",
    "- Source-specific transformations.\n",
    "- Feature augmentation by creating new spectral sources.\n",
    "- Custom grouping of samples.\n",
    "- Stratified train/test splitting.\n",
    "- Repeated K-Fold cross-validation.\n",
    "- Unpacking and inspecting data by groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b228dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and Imports\n",
    "# Standard library imports\n",
    "import itertools\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RepeatedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import the SpectraSet class (ensure nirs4all.data.spectraset is in PYTHONPATH or installed)\n",
    "# Assuming spectraset.py is in d:\\Workspace\\ML\\NIRS\\nirs4all\\nirs4all\\data\\\n",
    "import sys\n",
    "sys.path.append('d:\\\\Workspace\\\\ML\\\\NIRS\\\\nirs4all') # Adjust if necessary\n",
    "from nirs4all.data.spectraset_grok import SpectraSet\n",
    "\n",
    "# IPython magic for autoreloading external modules (optional)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578f14b",
   "metadata": {},
   "source": [
    "## 2. Data Generation\n",
    "\n",
    "We will generate a dataset with 10 samples. Each sample corresponds to an \"n\" value from 1 to 5 (each n appearing twice).\n",
    "- **NIRS spectra**: 4 values, calculated as `(n-1) * i` for `i` in `0,1,2,3`.\n",
    "- **Raman spectra**: 5 values, calculated as `(n-1) * i + 5` for `i` in `0,1,2,3,4`.\n",
    "- **Target (y)**: The value of `n`.\n",
    "- **Metadata**:\n",
    "    - `n_numeric`: `n` (integer).\n",
    "    - `n_str`: `n` (string).\n",
    "    - `n_sum_str`: `f\"{n}+{n}\"` (string, e.g., \"1+1\").\n",
    "Initially, each sample has one augmentation (the original spectrum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cbbc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial SpectraSet (spec) ---\n",
      "Shape of spec: (10, 9), (10, 1)\n",
      "**************************************************\n",
      "0: [0. 0. 0. 0.] - [5. 5. 5. 5. 5.]: y = [0]\n",
      "0: [0. 0. 0. 0. 5. 5. 5. 5. 5.] - [0]\n",
      "1: [1. 1. 1. 1.] - [6. 6. 6. 6. 6.]: y = [1]\n",
      "1: [1. 1. 1. 1. 6. 6. 6. 6. 6.] - [1]\n",
      "2: [2. 2. 2. 2.] - [7. 7. 7. 7. 7.]: y = [2]\n",
      "2: [2. 2. 2. 2. 7. 7. 7. 7. 7.] - [2]\n",
      "3: [3. 3. 3. 3.] - [8. 8. 8. 8. 8.]: y = [3]\n",
      "3: [3. 3. 3. 3. 8. 8. 8. 8. 8.] - [3]\n",
      "4: [4. 4. 4. 4.] - [9. 9. 9. 9. 9.]: y = [4]\n",
      "4: [4. 4. 4. 4. 9. 9. 9. 9. 9.] - [4]\n",
      "5: [5. 5. 5. 5.] - [10. 10. 10. 10. 10.]: y = [5]\n",
      "5: [ 5.  5.  5.  5. 10. 10. 10. 10. 10.] - [5]\n",
      "6: [6. 6. 6. 6.] - [11. 11. 11. 11. 11.]: y = [6]\n",
      "6: [ 6.  6.  6.  6. 11. 11. 11. 11. 11.] - [6]\n",
      "7: [7. 7. 7. 7.] - [12. 12. 12. 12. 12.]: y = [7]\n",
      "7: [ 7.  7.  7.  7. 12. 12. 12. 12. 12.] - [7]\n",
      "8: [8. 8. 8. 8.] - [13. 13. 13. 13. 13.]: y = [8]\n",
      "8: [ 8.  8.  8.  8. 13. 13. 13. 13. 13.] - [8]\n",
      "9: [9. 9. 9. 9.] - [14. 14. 14. 14. 14.]: y = [9]\n",
      "9: [ 9.  9.  9.  9. 14. 14. 14. 14. 14.] - [9]\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for Data Generation ---\n",
    "total_samples = 10\n",
    "\n",
    "# --- Initialize data structures ---\n",
    "spectra_data_v1 = {'nirs': [], 'raman': []}\n",
    "target_y_v1 = np.zeros(total_samples, dtype=int)\n",
    "metadata_v1 = {\n",
    "    'n_numeric': np.zeros(total_samples, dtype=int),\n",
    "    'n_str': np.zeros(total_samples, dtype=object),\n",
    "    'n_sum_str': np.zeros(total_samples, dtype=object)\n",
    "}\n",
    "\n",
    "# --- Populate data ---\n",
    "for s_idx in range(total_samples):\n",
    "    nirs_spectrum = np.array([s_idx for i in range(4)], dtype=float)\n",
    "    spectra_data_v1['nirs'].append([nirs_spectrum])\n",
    "    raman_spectrum = np.array([s_idx + 5 for i in range(5)], dtype=float)\n",
    "    spectra_data_v1['raman'].append([raman_spectrum])\n",
    "    # Target\n",
    "    target_y_v1[s_idx] = s_idx\n",
    "    # Metadata\n",
    "    metadata_v1['n_numeric'][s_idx] = s_idx\n",
    "    metadata_v1['n_str'][s_idx] = str(s_idx)\n",
    "    metadata_v1['n_sum_str'][s_idx] = f\"{s_idx}+{s_idx}\"\n",
    "\n",
    "spec = SpectraSet.build(\n",
    "    spectra=spectra_data_v1,\n",
    "    target=target_y_v1,\n",
    "    metadata=metadata_v1\n",
    ")\n",
    "\n",
    "print(\"--- Initial SpectraSet (spec) ---\")\n",
    "print(f\"Shape of spec: {spec.X().shape}, {spec.y().shape}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "for i in range(10):\n",
    "    nirs = spec.X(sources=[\"nirs\"])[i]\n",
    "    raman = spec.X(sources=[\"raman\"])[i]\n",
    "    print(f\"{i}: {nirs} - {raman}: y = {spec.y()[i]}\")\n",
    "    print(f\"{i}: {spec.X()[i]} - {spec.y()[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32be53b",
   "metadata": {},
   "source": [
    "## 3. Custom Transformers for Augmentation\n",
    "\n",
    "We define `TransformerMixin` classes to add specific values to spectra.\n",
    "- `AddNTransformer`: Adds a specified value `N` to each element of a spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2096ed62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom transformers defined: AddNTransformer\n"
     ]
    }
   ],
   "source": [
    "# --- Define Custom Transformers ---\n",
    "class AddNTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, value_to_add):\n",
    "        self.value_to_add = value_to_add\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # Nothing to fit\n",
    "\n",
    "    def transform(self, X_list):\n",
    "        if isinstance(X_list, np.ndarray): # Single spectrum\n",
    "             return X_list + self.value_to_add\n",
    "        return [x + self.value_to_add for x in X_list]\n",
    "\n",
    "\n",
    "# --- Instantiate Transformers ---\n",
    "add1_transformer = AddNTransformer(value_to_add=0.1)\n",
    "add2_transformer = AddNTransformer(value_to_add=0.2)\n",
    "\n",
    "print(\"Custom transformers defined: AddNTransformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c2a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_nirs shape: (10, 4)\n",
      "[0 1 2 3 4 5 6 7 8 9] [[0. 0. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [2. 2. 2. 2.]\n",
      " [3. 3. 3. 3.]\n",
      " [4. 4. 4. 4.]\n",
      " [5. 5. 5. 5.]\n",
      " [6. 6. 6. 6.]\n",
      " [7. 7. 7. 7.]\n",
      " [8. 8. 8. 8.]\n",
      " [9. 9. 9. 9.]]\n",
      "[[0.1 0.1 0.1 0.1]\n",
      " [1.1 1.1 1.1 1.1]\n",
      " [2.1 2.1 2.1 2.1]\n",
      " [3.1 3.1 3.1 3.1]\n",
      " [4.1 4.1 4.1 4.1]\n",
      " [5.1 5.1 5.1 5.1]\n",
      " [6.1 6.1 6.1 6.1]\n",
      " [7.1 7.1 7.1 7.1]\n",
      " [8.1 8.1 8.1 8.1]\n",
      " [9.1 9.1 9.1 9.1]]\n",
      "==================================================\n",
      "X_raman shape: (10, 5)\n",
      "[0 1 2 3 4 5 6 7 8 9] [[ 5.  5.  5.  5.  5.]\n",
      " [ 6.  6.  6.  6.  6.]\n",
      " [ 7.  7.  7.  7.  7.]\n",
      " [ 8.  8.  8.  8.  8.]\n",
      " [ 9.  9.  9.  9.  9.]\n",
      " [10. 10. 10. 10. 10.]\n",
      " [11. 11. 11. 11. 11.]\n",
      " [12. 12. 12. 12. 12.]\n",
      " [13. 13. 13. 13. 13.]\n",
      " [14. 14. 14. 14. 14.]]\n",
      "[[ 5.2  5.2  5.2  5.2  5.2]\n",
      " [ 6.2  6.2  6.2  6.2  6.2]\n",
      " [ 7.2  7.2  7.2  7.2  7.2]\n",
      " [ 8.2  8.2  8.2  8.2  8.2]\n",
      " [ 9.2  9.2  9.2  9.2  9.2]\n",
      " [10.2 10.2 10.2 10.2 10.2]\n",
      " [11.2 11.2 11.2 11.2 11.2]\n",
      " [12.2 12.2 12.2 12.2 12.2]\n",
      " [13.2 13.2 13.2 13.2 13.2]\n",
      " [14.2 14.2 14.2 14.2 14.2]]\n",
      "==================================================\n",
      "Shape of spec: (15, 9), (15, 1)\n",
      "**************************************************\n",
      "0: [0. 0. 0. 0. 5. 5. 5. 5. 5.] - [0]\n",
      "1: [1. 1. 1. 1. 6. 6. 6. 6. 6.] - [1]\n",
      "2: [2. 2. 2. 2. 7. 7. 7. 7. 7.] - [2]\n",
      "3: [3. 3. 3. 3. 8. 8. 8. 8. 8.] - [3]\n",
      "4: [4. 4. 4. 4. 9. 9. 9. 9. 9.] - [4]\n",
      "5: [ 5.  5.  5.  5. 10. 10. 10. 10. 10.] - [5]\n",
      "6: [ 6.  6.  6.  6. 11. 11. 11. 11. 11.] - [6]\n",
      "7: [ 7.  7.  7.  7. 12. 12. 12. 12. 12.] - [7]\n",
      "8: [ 8.  8.  8.  8. 13. 13. 13. 13. 13.] - [8]\n",
      "9: [ 9.  9.  9.  9. 14. 14. 14. 14. 14.] - [9]\n",
      "10: [0.1 0.1 0.1 0.1 5.2 5.2 5.2 5.2 5.2] - [0]\n",
      "11: [1.1 1.1 1.1 1.1 6.2 6.2 6.2 6.2 6.2] - [1]\n",
      "12: [2.1 2.1 2.1 2.1 7.2 7.2 7.2 7.2 7.2] - [2]\n",
      "13: [3.1 3.1 3.1 3.1 8.2 8.2 8.2 8.2 8.2] - [3]\n",
      "14: [4.1 4.1 4.1 4.1 9.2 9.2 9.2 9.2 9.2] - [4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nirs4all.data.spectraset_grok.SpectraSet at 0x25ce7177610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 1) grab both data & obs‚Äêindices\n",
    "# X_nirs, obs_idx = spec.X_with_labels(sources=[\"nirs\"])\n",
    "# print(f\"X_nirs shape: {X_nirs.shape}\")\n",
    "# print(obs_idx, X_nirs)\n",
    "# new_spectra_nirs = add1_transformer.transform(X_nirs)\n",
    "# print(new_spectra_nirs)\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# X_raman, obs_idx = spec.X_with_labels(sources=[\"raman\"])\n",
    "# print(f\"X_raman shape: {X_raman.shape}\")\n",
    "# print(obs_idx, X_raman)\n",
    "# new_spectra_raman = add2_transformer.transform(X_raman)\n",
    "# print(new_spectra_raman)\n",
    "# print(\"=\"*50)\n",
    "\n",
    "\n",
    "# spec.augment_samples(\n",
    "#     new_spectra_by_source = {\n",
    "#         \"nirs\": new_spectra_nirs[:5],\n",
    "#         \"raman\": new_spectra_raman[:5]\n",
    "#     },\n",
    "#     original_obs_indices=obs_idx[:5] # Use the actual obs label\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Shape of spec: {spec.X().shape}, {spec.y().shape}\")\n",
    "# print(\"*\"*50)\n",
    "\n",
    "# for i in range(len(spec)):\n",
    "#     nirs = spec.X(sources=[\"nirs\"])[i]\n",
    "#     raman = spec.X(sources=[\"raman\"])[i]\n",
    "#     print(f\"{i}: {spec.X()[i]} - {spec.y()[i]}\")\n",
    "\n",
    "\n",
    "# spec.augment_samples(\n",
    "#     new_spectra_by_source = {\n",
    "#         \"nirs\": new_spectra_nirs[:5],\n",
    "#         \"raman\": new_spectra_raman[:5]\n",
    "#     },\n",
    "#     original_obs_indices=obs_idx[:5] # Use the actual obs label\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c00df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test: Adding features to 'nirs' source ---\n",
      "\n",
      "--- Step 1: Get current NIRS data and feature names ---\n",
      "Original NIRS data shape: (20, 4)\n",
      "Current NIRS feature names: ['nirs_f0', 'nirs_f1', 'nirs_f2', 'nirs_f3']\n",
      "\n",
      "--- Step 2: Transform NIRS data (add 0.01) ---\n",
      "Transformed NIRS data shape: (20, 4)\n",
      "\n",
      "--- Step 3: Define new feature names ---\n",
      "Names for new features to be added: ['nirs_f0_plus_0.01', 'nirs_f1_plus_0.01', 'nirs_f2_plus_0.01', 'nirs_f3_plus_0.01']\n",
      "\n",
      "--- Step 4: Call spec.add_features ---\n",
      "spec.add_features called.\n",
      "\n",
      "--- Step 5: Verification ---\n",
      "Updated spec.ds['spectra_nirs'] structure:\n",
      "<xarray.DataArray 'spectra_nirs' (obs: 20, feature_nirs: 4)> Size: 640B\n",
      "array([[0. , 0. , 0. , 0. ],\n",
      "       [1. , 1. , 1. , 1. ],\n",
      "       [2. , 2. , 2. , 2. ],\n",
      "       [3. , 3. , 3. , 3. ],\n",
      "       [4. , 4. , 4. , 4. ],\n",
      "       [5. , 5. , 5. , 5. ],\n",
      "       [6. , 6. , 6. , 6. ],\n",
      "       [7. , 7. , 7. , 7. ],\n",
      "       [8. , 8. , 8. , 8. ],\n",
      "       [9. , 9. , 9. , 9. ],\n",
      "       [0.1, 0.1, 0.1, 0.1],\n",
      "       [1.1, 1.1, 1.1, 1.1],\n",
      "       [2.1, 2.1, 2.1, 2.1],\n",
      "       [3.1, 3.1, 3.1, 3.1],\n",
      "       [4.1, 4.1, 4.1, 4.1],\n",
      "       [0.1, 0.1, 0.1, 0.1],\n",
      "       [1.1, 1.1, 1.1, 1.1],\n",
      "       [2.1, 2.1, 2.1, 2.1],\n",
      "       [3.1, 3.1, 3.1, 3.1],\n",
      "       [4.1, 4.1, 4.1, 4.1]])\n",
      "Coordinates:\n",
      "  * obs           (obs) int64 160B 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 0 1 2 3 4\n",
      "  * feature_nirs  (feature_nirs) <U7 112B 'nirs_f0' 'nirs_f1' ... 'nirs_f3'\n",
      "    sample        (obs) int64 160B 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 0 1 2 3 4\n",
      "    augmentation  (obs) int64 160B 0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 7 8 9 10\n",
      "\n",
      "Shape of NIRS data after add_features: (20, 1, 4)\n",
      "Expected total NIRS features: 8\n",
      "ERROR: Total NIRS feature count mismatch! Got 1, expected 8\n",
      "\n",
      "Final NIRS feature names: ['nirs_f0', 'nirs_f1', 'nirs_f2', 'nirs_f3']\n",
      "ERROR: Some new feature names are missing: ['nirs_f0_plus_0.01', 'nirs_f1_plus_0.01', 'nirs_f2_plus_0.01', 'nirs_f3_plus_0.01']\n",
      "\n",
      "--- NIRS data for first observation after add_features ---\n",
      "Observation label: 0\n",
      "  Original NIRS data part (from spec.X, first 4 features): [[0. 0. 0. 0.]]\n",
      "  Added NIRS data part (from spec.X, next 4 features): []\n",
      "  Expected added data (from transformed input for row 0): [0.01 0.01 0.01 0.01]\n",
      "  Verification: Added NIRS features for obs 0 match the transformed input values.\n",
      "\n",
      "--- Concatenated X and y for first observation (if any) ---\n",
      "Data for first observation in current spec (label: 0):\n",
      "  spec.X()[0] (shape (9,)): [0. 0. 0. 0. 5. 5. 5. 5. 5.]\n",
      "  spec.y()[0]: [0]\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Test: Adding features to 'nirs' source ---\")\n",
    "\n",
    "# 0. Ensure AddNTransformer is defined (it's in cell c7b5dea8)\n",
    "# And numpy is imported (as np, in cell caffebc2)\n",
    "\n",
    "# 1. Get current NIRS data and its feature names\n",
    "print(\"\\n--- Step 1: Get current NIRS data and feature names ---\")\n",
    "\n",
    "available_sources = spec.available_sources\n",
    "if 'nirs' not in available_sources:\n",
    "    print(\"Error: 'nirs' source not found in spec. Skipping feature addition.\")\n",
    "else:\n",
    "    nirs_data_original = spec.X(sources=\"nirs\") # Shape: (n_obs, n_current_nirs_features)\n",
    "    print(f\"Original NIRS data shape: {nirs_data_original.shape}\")\n",
    "\n",
    "    # Get current feature names for the 'nirs' source\n",
    "    nirs_feature_dim_name = spec.ds['spectra_nirs'].dims[-1] # e.g., 'feature_nirs'\n",
    "    current_nirs_feature_names = spec.ds['spectra_nirs'].coords[nirs_feature_dim_name].values.tolist()\n",
    "    print(f\"Current NIRS feature names: {current_nirs_feature_names}\")\n",
    "\n",
    "    # 2. Transform the NIRS data\n",
    "    print(\"\\n--- Step 2: Transform NIRS data (add 0.01) ---\")\n",
    "    add_0_01_transformer = AddNTransformer(value_to_add=0.01)\n",
    "    # The transform method in AddNTransformer should handle 2D arrays (applying to each row)\n",
    "    nirs_data_transformed = add_0_01_transformer.transform(nirs_data_original.copy())\n",
    "    print(f\"Transformed NIRS data shape: {nirs_data_transformed.shape}\") # Should be same as nirs_data_original\n",
    "\n",
    "    # 3. Define feature names for the new (transformed) features\n",
    "    print(\"\\n--- Step 3: Define new feature names ---\")\n",
    "    new_feature_names = [f\"{name}_plus_0.01\" for name in current_nirs_feature_names]\n",
    "    print(f\"Names for new features to be added: {new_feature_names}\")\n",
    "    \n",
    "    if len(new_feature_names) != nirs_data_transformed.shape[1]:\n",
    "        print(f\"ERROR: Mismatch! New names count {len(new_feature_names)}, transformed data features {nirs_data_transformed.shape[1]}\")\n",
    "        # Fallback if error (e.g. if current_nirs_feature_names was empty or incorrect)\n",
    "        num_new_feats = nirs_data_transformed.shape[1]\n",
    "        new_feature_names = [f\"new_nirs_f{j}_plus_0.01\" for j in range(num_new_feats)]\n",
    "        print(f\"Using fallback new feature names: {new_feature_names}\")\n",
    "\n",
    "    # 4. Add these transformed features back to the 'nirs' source\n",
    "    print(\"\\n--- Step 4: Call spec.add_features ---\")\n",
    "    spec.add_features(\n",
    "        source_name=\"nirs\",\n",
    "        new_features=nirs_data_transformed,\n",
    "        feature_names=new_feature_names\n",
    "    )\n",
    "    print(\"spec.add_features called.\")\n",
    "\n",
    "    # 5. Verification\n",
    "    print(\"\\n--- Step 5: Verification ---\")\n",
    "    print(\"Updated spec.ds['spectra_nirs'] structure:\")\n",
    "    print(spec.ds['spectra_nirs'])\n",
    "\n",
    "    X_nirs_after, obs_labels_nirs_after = spec.X_with_labels(sources=\"nirs\", feature_shape=\"2d\")\n",
    "    print(f\"\\nShape of NIRS data after add_features: {X_nirs_after.shape}\")\n",
    "    \n",
    "    expected_total_nirs_features = len(current_nirs_feature_names) + len(new_feature_names)\n",
    "    print(f\"Expected total NIRS features: {expected_total_nirs_features}\")\n",
    "    \n",
    "    if X_nirs_after.shape[1] == expected_total_nirs_features:\n",
    "        print(\"Total NIRS feature count is as expected.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Total NIRS feature count mismatch! Got {X_nirs_after.shape[1]}, expected {expected_total_nirs_features}\")\n",
    "\n",
    "    final_nirs_feature_names = spec.ds['spectra_nirs'].coords[nirs_feature_dim_name].values.tolist()\n",
    "    print(f\"\\nFinal NIRS feature names: {final_nirs_feature_names}\")\n",
    "    \n",
    "    missing_names = [name for name in new_feature_names if name not in final_nirs_feature_names]\n",
    "    if not missing_names:\n",
    "        print(\"All new feature names are present in the final NIRS feature list.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Some new feature names are missing: {missing_names}\")\n",
    "\n",
    "    print(\"\\n--- NIRS data for first observation after add_features ---\")\n",
    "    if X_nirs_after.shape[0] > 0:\n",
    "        obs_idx_to_check = 0 # Check the first observation in the returned array    \n",
    "        obs_label = obs_labels_nirs_after[obs_idx_to_check]\n",
    "        print(f\"Observation label: {obs_label}\")\n",
    "        \n",
    "        # The original features should be first, followed by the newly added features.\n",
    "        original_part_from_X = X_nirs_after[obs_idx_to_check, :len(current_nirs_feature_names)]\n",
    "        added_part_from_X = X_nirs_after[obs_idx_to_check, len(current_nirs_feature_names):len(current_nirs_feature_names) + len(new_feature_names)]\n",
    "        \n",
    "        print(f\"  Original NIRS data part (from spec.X, first {len(current_nirs_feature_names)} features): {original_part_from_X}\")\n",
    "        print(f\"  Added NIRS data part (from spec.X, next {len(new_feature_names)} features): {added_part_from_X}\")\n",
    "        \n",
    "        # Find the corresponding row in nirs_data_transformed.\n",
    "        expected_added_data_for_this_obs = nirs_data_transformed[obs_idx_to_check] \n",
    "        print(f\"  Expected added data (from transformed input for row {obs_idx_to_check}): {expected_added_data_for_this_obs}\")\n",
    "\n",
    "        if np.allclose(added_part_from_X, expected_added_data_for_this_obs):\n",
    "            print(f\"  Verification: Added NIRS features for obs {obs_label} match the transformed input values.\")\n",
    "        else:\n",
    "            print(f\"  VERIFICATION FAILED: Added NIRS features for obs {obs_label} DO NOT match transformed input values.\")\n",
    "    else:\n",
    "        print(\"No NIRS data to display for value verification.\")\n",
    "\n",
    "    print(\"\\n--- Concatenated X and y for first observation (if any) ---\")\n",
    "    if len(spec) > 0:\n",
    "        first_obs_label_in_spec = spec.ds.obs.values[0]\n",
    "        print(f\"Data for first observation in current spec (label: {first_obs_label_in_spec}):\")\n",
    "        print(f\"  spec.X()[0] (shape {spec.X()[0].shape}): {spec.X()[0]}\")\n",
    "        print(f\"  spec.y()[0]: {spec.y()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa95d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (concatenate) shape: (20, 9)\n",
      "X (interlace) shape: (20, 10)\n",
      "X (2d) shape: (20, 2, 5)\n",
      "X (transpose2d) shape: (20, 5, 2)\n",
      "\n",
      "X (interlace) shape: (20, 10)\n",
      "X (2d) shape: (20, 2, 5)\n",
      "X (transpose2d) shape: (20, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve X with all sources, concatenated (default)\n",
    "X_concat = spec.X()\n",
    "print(f\"X (concatenate) shape: {X_concat.shape}\")\n",
    "\n",
    "# Retrieve X with features interlaced\n",
    "X_interlace = spec.X(feature_shape=\"interlace\")\n",
    "print(f\"X (interlace) shape: {X_interlace.shape}\")\n",
    "\n",
    "# Retrieve X as a 3D array (n_obs, n_sources, max_features_per_source)\n",
    "X_2d = spec.X(feature_shape=\"2d\")\n",
    "print(f\"X (2d) shape: {X_2d.shape}\")\n",
    "\n",
    "# Retrieve X as a 3D array (n_obs, max_features_per_source, n_sources)\n",
    "X_transpose2d = spec.X(feature_shape=\"transpose2d\")\n",
    "print(f\"X (transpose2d) shape: {X_transpose2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2f281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded y: [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "LabelEncoder not fitted. Call .y(encode_labels=True) first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Inverse transform to get original labels\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m y_original \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_original\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\data\\spectraset_grok.py:544\u001b[0m, in \u001b[0;36mSpectraSet.inverse_transform_y\u001b[1;34m(self, encoded_labels)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minverse_transform_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoded_labels: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workspace\\ML\\nirs4all\\nirs4all\\data\\spectraset_grok.py:402\u001b[0m, in \u001b[0;36mDataRetriever.inverse_transform_y\u001b[1;34m(self, encoded_labels)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minverse_transform_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoded_labels: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 402\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabelEncoder not fitted. Call .y(encode_labels=True) first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(encoded_labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: LabelEncoder not fitted. Call .y(encode_labels=True) first."
     ]
    }
   ],
   "source": [
    "# Get encoded target labels\n",
    "y_encoded = spec.y(encode_labels=True)\n",
    "print(f\"Encoded y: {y_encoded}\")\n",
    "\n",
    "# Inverse transform to get original labels\n",
    "y_original = spec.inverse_transform_y(y_encoded)\n",
    "print(f\"Original y: {y_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766cbcc",
   "metadata": {},
   "source": [
    "## 4. Sample Augmentation\n",
    "\n",
    "Now, we augment the samples based on their `n` value:\n",
    "- Samples with `n < 4`: Augment with `add1_transformer` (add 1). This adds one new augmentation.\n",
    "- Samples with `n >= 4`: Augment with `add1_transformer` and `add2_transformer` (add 1, then add 2 to original). This adds two new augmentations.\n",
    "\n",
    "The augmentations are applied to the *original* spectra of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b8f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spectra_data_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m original_raman_aug \u001b[38;5;241m=\u001b[39m spectra_data_v1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraman\u001b[39m\u001b[38;5;124m'\u001b[39m][s_idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Augment with add1\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mspectra_data_v2\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnirs\u001b[39m\u001b[38;5;124m'\u001b[39m][s_idx]\u001b[38;5;241m.\u001b[39mappend(add1_transformer\u001b[38;5;241m.\u001b[39mtransform(original_nirs_aug))\n\u001b[0;32m     12\u001b[0m     spectra_data_v2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraman\u001b[39m\u001b[38;5;124m'\u001b[39m][s_idx]\u001b[38;5;241m.\u001b[39mappend(add1_transformer\u001b[38;5;241m.\u001b[39mtransform(original_raman_aug))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# current_n >= 4\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Augment with add1\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spectra_data_v2' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Deep copy initial spectra data to modify for augmentation ---\n",
    "\n",
    "# --- Apply Sample Augmentation ---\n",
    "for s_idx in range(total_samples):\n",
    "    # Original spectra for this sample (first augmentation)\n",
    "    original_nirs_aug = spectra_data_v1['nirs'][s_idx][0] \n",
    "    original_raman_aug = spectra_data_v1['raman'][s_idx][0]\n",
    "\n",
    "    if s_idx < 4:\n",
    "        # Augment with add1\n",
    "        spectra_data_v2['nirs'][s_idx].append(add1_transformer.transform(original_nirs_aug))\n",
    "        spectra_data_v2['raman'][s_idx].append(add1_transformer.transform(original_raman_aug))\n",
    "    else: # current_n >= 4\n",
    "        # Augment with add1\n",
    "        spectra_data_v2['nirs'][s_idx].append(add1_transformer.transform(original_nirs_aug))\n",
    "        spectra_data_v2['raman'][s_idx].append(add1_transformer.transform(original_raman_aug))\n",
    "        # Augment with add2 (applied to original)\n",
    "        spectra_data_v2['nirs'][s_idx].append(add2_transformer.transform(original_nirs_aug))\n",
    "        spectra_data_v2['raman'][s_idx].append(add2_transformer.transform(original_raman_aug))\n",
    "\n",
    "\n",
    "print(\"--- SpectraSet after Sample Augmentation (ss_v2) ---\")\n",
    "print(f\"Shape of X from ss_v2: {ss_v2.X().shape}\") \n",
    "\n",
    "# for i in range(len(ss_v2)):\n",
    "#     nirs = ss_v2.X(include_sources=[\"nirs\"])[i]\n",
    "#     raman = ss_v2.X(include_sources=[\"raman\"])[i]\n",
    "#     print(f\"{i}: {nirs} - {raman}: y = {ss_v2.y()[i]}\")\n",
    "    \n",
    "# print(ss_v2.X(include_sources=[\"nirs\"], augment=\"augmented\"))\n",
    "print(ss_v2.subset_by_samples([0, 4]).X(sources=[\"raman\"], augment=\"augmented\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59639f98",
   "metadata": {},
   "source": [
    "## 5. Source-Specific Transformation\n",
    "\n",
    "Next, we transform all spectra (original and their augmentations) based on their source:\n",
    "- **NIRS spectra**: Apply a \"minus 1\" transformation.\n",
    "- **Raman spectra**: Apply a \"plus 100\" transformation.\n",
    "\n",
    "These transformations are applied to *all existing augmentations* for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Source-Specific Transformers ---\n",
    "class Minus1Transformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X_spectrum): return X_spectrum - 1\n",
    "\n",
    "class Plus100Transformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X_spectrum): return X_spectrum + 100\n",
    "\n",
    "minus1_transformer = Minus1Transformer()\n",
    "plus100_transformer = Plus100Transformer()\n",
    "\n",
    "# --- Deep copy augmented spectra data for source transformation ---\n",
    "spectra_data_v3 = {\n",
    "    src: [[np.copy(aug) for aug in augs_list] for augs_list in sample_augs_list]\n",
    "    for src, sample_augs_list in spectra_data_v2.items()\n",
    "}\n",
    "\n",
    "# --- Apply Source-Specific Transformations ---\n",
    "# For NIRS source\n",
    "for s_idx in range(total_samples):\n",
    "    for aug_idx in range(len(spectra_data_v3['nirs'][s_idx])):\n",
    "        spectra_data_v3['nirs'][s_idx][aug_idx] = minus1_transformer.transform(\n",
    "            spectra_data_v3['nirs'][s_idx][aug_idx]\n",
    "        )\n",
    "\n",
    "# For Raman source\n",
    "for s_idx in range(total_samples):\n",
    "    for aug_idx in range(len(spectra_data_v3['raman'][s_idx])):\n",
    "        spectra_data_v3['raman'][s_idx][aug_idx] = plus100_transformer.transform(\n",
    "            spectra_data_v3['raman'][s_idx][aug_idx]\n",
    "        )\n",
    "\n",
    "# --- Build SpectraSet with Source-Transformed Data ---\n",
    "ss_v3 = SpectraSet.build(\n",
    "    spectra=spectra_data_v3,\n",
    "    target=target_y_v1,\n",
    "    metadata=metadata_v1\n",
    ")\n",
    "\n",
    "# print(\"--- SpectraSet after Source-Specific Transformation (ss_v3) ---\")\n",
    "# print(ss_v3.ds)\n",
    "# print(f\"\\nShape of X from ss_v3: {ss_v3.X().shape}\") # Shape should be same as ss_v2\n",
    "\n",
    "# # Verify transformations for one sample\n",
    "# print(f\"\\nNIRS for sample 0, aug 0 (original was {spectra_data_v2['nirs'][0][0]}):\")\n",
    "# print(f\"  Transformed: {spectra_data_v3['nirs'][0][0]}\")\n",
    "# print(f\"Raman for sample 0, aug 0 (original was {spectra_data_v2['raman'][0][0]}):\")\n",
    "# print(f\"  Transformed: {spectra_data_v3['raman'][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb44d8",
   "metadata": {},
   "source": [
    "## 6. Feature Augmentation (Adding New Sources)\n",
    "\n",
    "We create new spectral sources by applying a transformation to existing ones.\n",
    "- For every sample and its existing augmentations (which are now source-transformed):\n",
    "    - Create `nirs_plus_01` by adding 0.1 to each NIRS spectrum.\n",
    "    - Create `raman_plus_01` by adding 0.1 to each Raman spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa51ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SpectraSet after Feature Augmentation (ss_v4_pre_group_split) ---\n",
      "<xarray.Dataset> Size: 6kB\n",
      "Dimensions:                (obs: 26, feature_nirs: 4, feature_raman: 5,\n",
      "                            feature_nirs_plus_01: 4, feature_raman_plus_01: 5,\n",
      "                            variable: 1)\n",
      "Coordinates:\n",
      "  * obs                    (obs) int64 208B 0 1 2 3 4 5 6 ... 20 21 22 23 24 25\n",
      "  * feature_nirs           (feature_nirs) <U7 112B 'nirs_f0' ... 'nirs_f3'\n",
      "  * feature_raman          (feature_raman) <U8 160B 'raman_f0' ... 'raman_f4'\n",
      "  * feature_nirs_plus_01   (feature_nirs_plus_01) <U15 240B 'nirs_plus_01_f0'...\n",
      "  * feature_raman_plus_01  (feature_raman_plus_01) <U16 320B 'raman_plus_01_f...\n",
      "  * variable               (variable) int64 8B 0\n",
      "    sample                 (obs) int64 208B 0 0 1 1 2 2 3 3 ... 7 7 8 8 8 9 9 9\n",
      "    augmentation           (obs) int64 208B 0 1 0 1 0 1 0 1 ... 1 2 0 1 2 0 1 2\n",
      "Data variables:\n",
      "    spectra_nirs           (obs, feature_nirs) float64 832B -1.0 -1.0 ... 8.2\n",
      "    spectra_raman          (obs, feature_raman) float64 1kB 105.0 ... 114.2\n",
      "    spectra_nirs_plus_01   (obs, feature_nirs_plus_01) float64 832B -0.9 ... 8.3\n",
      "    spectra_raman_plus_01  (obs, feature_raman_plus_01) float64 1kB 105.1 ......\n",
      "    target                 (obs, variable) int64 208B 0 0 1 1 2 2 ... 8 8 9 9 9\n",
      "    metadata_n_numeric     (obs) int64 208B 0 0 1 1 2 2 3 3 ... 7 7 8 8 8 9 9 9\n",
      "    metadata_n_str         (obs) <U1 104B '0' '0' '1' '1' ... '8' '9' '9' '9'\n",
      "    metadata_n_sum_str     (obs) <U3 312B '0+0' '0+0' '1+1' ... '9+9' '9+9'\n",
      "\n",
      "Shape of X from ss_v4_pre_group_split: (26, 18)\n",
      "Sources in ss_v4_pre_group_split: ['spectra_nirs', 'spectra_raman', 'spectra_nirs_plus_01', 'spectra_raman_plus_01']\n",
      "\n",
      "NIRS for sample 0, aug 0: [-1. -1. -1. -1.]\n",
      "NIRS_plus_01 for sample 0, aug 0: [-0.9 -0.9 -0.9 -0.9]\n"
     ]
    }
   ],
   "source": [
    "# --- Define Feature Augmentation Transformer ---\n",
    "class AddConstantTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, constant_to_add):\n",
    "        self.constant_to_add = constant_to_add\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X_spectrum): return X_spectrum + self.constant_to_add\n",
    "\n",
    "add_point_one_transformer = AddConstantTransformer(0.1)\n",
    "\n",
    "# --- Deep copy source-transformed spectra data for feature augmentation ---\n",
    "spectra_data_v4 = {\n",
    "    src: [[np.copy(aug) for aug in augs_list] for augs_list in sample_augs_list]\n",
    "    for src, sample_augs_list in spectra_data_v3.items()\n",
    "}\n",
    "\n",
    "# --- Initialize new sources ---\n",
    "spectra_data_v4['nirs_plus_01'] = [[] for _ in range(total_samples)]\n",
    "spectra_data_v4['raman_plus_01'] = [[] for _ in range(total_samples)]\n",
    "\n",
    "# --- Apply Feature Augmentation ---\n",
    "for s_idx in range(total_samples):\n",
    "    # For nirs_plus_01\n",
    "    for aug_spectrum in spectra_data_v3['nirs'][s_idx]: # Iterate existing NIRS augs\n",
    "        spectra_data_v4['nirs_plus_01'][s_idx].append(\n",
    "            add_point_one_transformer.transform(aug_spectrum)\n",
    "        )\n",
    "    # For raman_plus_01\n",
    "    for aug_spectrum in spectra_data_v3['raman'][s_idx]: # Iterate existing Raman augs\n",
    "        spectra_data_v4['raman_plus_01'][s_idx].append(\n",
    "            add_point_one_transformer.transform(aug_spectrum)\n",
    "        )\n",
    "        \n",
    "# --- Build SpectraSet with Feature-Augmented Data ---\n",
    "# This will be our main SpectraSet for subsequent steps\n",
    "ss_v4_pre_group_split = SpectraSet.build(\n",
    "    spectra=spectra_data_v4,\n",
    "    target=target_y_v1,\n",
    "    metadata=metadata_v1\n",
    ")\n",
    "\n",
    "print(\"--- SpectraSet after Feature Augmentation (ss_v4_pre_group_split) ---\")\n",
    "print(ss_v4_pre_group_split.ds)\n",
    "# X shape should be (24, 9+9) = (24, 18) if concatenated (nirs, raman, nirs_plus_01, raman_plus_01)\n",
    "# NIRS (4) + Raman (5) + NIRS_plus_01 (4) + Raman_plus_01 (5) = 18 features\n",
    "print(f\"\\nShape of X from ss_v4_pre_group_split: {ss_v4_pre_group_split.X().shape}\")\n",
    "print(f\"Sources in ss_v4_pre_group_split: {[k for k in ss_v4_pre_group_split.ds.data_vars if k.startswith('spectra_')]}\")\n",
    "\n",
    "# Verify new source for one sample\n",
    "print(f\"\\nNIRS for sample 0, aug 0: {spectra_data_v4['nirs'][0][0]}\")\n",
    "print(f\"NIRS_plus_01 for sample 0, aug 0: {spectra_data_v4['nirs_plus_01'][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa9720",
   "metadata": {},
   "source": [
    "## 7. Grouping Samples\n",
    "\n",
    "We group samples based on whether the first value of their *original* NIRS spectrum is odd or even.\n",
    "The original NIRS spectra are taken from `spectra_data_v1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508363e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SpectraSet with Custom Groups (ss_v5_with_groups) ---\n",
      "<xarray.Dataset> Size: 8kB\n",
      "Dimensions:                     (obs: 26, feature_nirs: 4, feature_raman: 5,\n",
      "                                 feature_nirs_plus_01: 4,\n",
      "                                 feature_raman_plus_01: 5, variable: 1)\n",
      "Coordinates:\n",
      "  * obs                         (obs) int64 208B 0 1 2 3 4 5 ... 21 22 23 24 25\n",
      "  * feature_nirs                (feature_nirs) <U7 112B 'nirs_f0' ... 'nirs_f3'\n",
      "  * feature_raman               (feature_raman) <U8 160B 'raman_f0' ... 'rama...\n",
      "  * feature_nirs_plus_01        (feature_nirs_plus_01) <U15 240B 'nirs_plus_0...\n",
      "  * feature_raman_plus_01       (feature_raman_plus_01) <U16 320B 'raman_plus...\n",
      "  * variable                    (variable) int64 8B 0\n",
      "    sample                      (obs) int64 208B 0 0 1 1 2 2 3 ... 7 8 8 8 9 9 9\n",
      "    augmentation                (obs) int64 208B 0 1 0 1 0 1 0 ... 2 0 1 2 0 1 2\n",
      "    group_id_nirs_parity_group  (obs) <U15 2kB 'even_first_nirs' ... 'odd_fir...\n",
      "Data variables:\n",
      "    spectra_nirs                (obs, feature_nirs) float64 832B -1.0 ... 8.2\n",
      "    spectra_raman               (obs, feature_raman) float64 1kB 105.0 ... 114.2\n",
      "    spectra_nirs_plus_01        (obs, feature_nirs_plus_01) float64 832B -0.9...\n",
      "    spectra_raman_plus_01       (obs, feature_raman_plus_01) float64 1kB 105....\n",
      "    target                      (obs, variable) int64 208B 0 0 1 1 2 ... 8 9 9 9\n",
      "    metadata_n_numeric          (obs) int64 208B 0 0 1 1 2 2 3 ... 7 8 8 8 9 9 9\n",
      "    metadata_n_str              (obs) <U1 104B '0' '0' '1' '1' ... '9' '9' '9'\n",
      "    metadata_n_sum_str          (obs) <U3 312B '0+0' '0+0' '1+1' ... '9+9' '9+9'\n",
      "\n",
      "Shape of X from ss_v5_with_groups: (26, 18)\n",
      "\n",
      "Group labels per sample (first 10): ['even_first_nirs' 'odd_first_nirs']\n",
      "\n",
      "Value counts for 'nirs_parity_group' (per observation):\n",
      "even_first_nirs    13\n",
      "odd_first_nirs     13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for 'nirs_parity_group' (per original sample):\n",
      "even_first_nirs    5\n",
      "odd_first_nirs     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Adjusting Grouping Logic for Better Distribution ---\n",
      "--- SpectraSet with Adjusted Custom Groups (ss_final_with_groups) ---\n",
      "<xarray.Dataset> Size: 7kB\n",
      "Dimensions:                         (obs: 26, feature_nirs: 4,\n",
      "                                     feature_raman: 5, feature_nirs_plus_01: 4,\n",
      "                                     feature_raman_plus_01: 5, variable: 1)\n",
      "Coordinates:\n",
      "  * obs                             (obs) int64 208B 0 1 2 3 4 ... 22 23 24 25\n",
      "  * feature_nirs                    (feature_nirs) <U7 112B 'nirs_f0' ... 'ni...\n",
      "  * feature_raman                   (feature_raman) <U8 160B 'raman_f0' ... '...\n",
      "  * feature_nirs_plus_01            (feature_nirs_plus_01) <U15 240B 'nirs_pl...\n",
      "  * feature_raman_plus_01           (feature_raman_plus_01) <U16 320B 'raman_...\n",
      "  * variable                        (variable) int64 8B 0\n",
      "    sample                          (obs) int64 208B 0 0 1 1 2 2 ... 8 8 8 9 9 9\n",
      "    augmentation                    (obs) int64 208B 0 1 0 1 0 1 ... 0 1 2 0 1 2\n",
      "    group_id_nirs_sum_parity_group  (obs) <U13 1kB 'even_sum_nirs' ... 'even_...\n",
      "Data variables:\n",
      "    spectra_nirs                    (obs, feature_nirs) float64 832B -1.0 ......\n",
      "    spectra_raman                   (obs, feature_raman) float64 1kB 105.0 .....\n",
      "    spectra_nirs_plus_01            (obs, feature_nirs_plus_01) float64 832B ...\n",
      "    spectra_raman_plus_01           (obs, feature_raman_plus_01) float64 1kB ...\n",
      "    target                          (obs, variable) int64 208B 0 0 1 1 ... 9 9 9\n",
      "    metadata_n_numeric              (obs) int64 208B 0 0 1 1 2 2 ... 8 8 8 9 9 9\n",
      "    metadata_n_str                  (obs) <U1 104B '0' '0' '1' ... '9' '9' '9'\n",
      "    metadata_n_sum_str              (obs) <U3 312B '0+0' '0+0' ... '9+9' '9+9'\n",
      "\n",
      "Value counts for 'nirs_sum_parity_group' (per original sample):\n",
      "even_sum_nirs    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Final Grouping Logic: n % 2 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_values_for_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 77\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# n=1, NIRS=[0,0,0,0], sum=0 (even)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# n=2, NIRS=[0,1,2,3], sum=6 (even)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# n=3, NIRS=[0,2,4,6], sum=12 (even)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Still all even. The data generation is very regular.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Let's group by n % 2 for simplicity to get a split.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Final Grouping Logic: n \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m 2 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m group_labels_n_parity \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_odd\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_values_for_samples[s_idx] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_even\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_samples)]\n\u001b[0;32m     78\u001b[0m group_labels_n_parity_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(group_labels_n_parity)\n\u001b[0;32m     79\u001b[0m final_custom_groups \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_parity_group\u001b[39m\u001b[38;5;124m'\u001b[39m: group_labels_n_parity_np}\n",
      "Cell \u001b[1;32mIn[7], line 77\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# n=1, NIRS=[0,0,0,0], sum=0 (even)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# n=2, NIRS=[0,1,2,3], sum=6 (even)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# n=3, NIRS=[0,2,4,6], sum=12 (even)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Still all even. The data generation is very regular.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Let's group by n % 2 for simplicity to get a split.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Final Grouping Logic: n \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m 2 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m group_labels_n_parity \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_odd\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn_values_for_samples\u001b[49m[s_idx] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_even\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_samples)]\n\u001b[0;32m     78\u001b[0m group_labels_n_parity_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(group_labels_n_parity)\n\u001b[0;32m     79\u001b[0m final_custom_groups \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_parity_group\u001b[39m\u001b[38;5;124m'\u001b[39m: group_labels_n_parity_np}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_values_for_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Define Groups ---\n",
    "group_labels_per_sample = []\n",
    "for s_idx in range(total_samples):\n",
    "    # Access the first value of the original NIRS spectrum for the sample\n",
    "    # spectra_data_v1['nirs'][s_idx] is a list containing one np.array (the original aug)\n",
    "    original_nirs_spectrum = spectra_data_v1['nirs'][s_idx][0]\n",
    "    first_nirs_value = original_nirs_spectrum[0]\n",
    "    \n",
    "    if first_nirs_value % 2 != 0: # Check if odd\n",
    "        group_labels_per_sample.append('odd_first_nirs')\n",
    "    else:\n",
    "        group_labels_per_sample.append('even_first_nirs')\n",
    "\n",
    "group_labels_np = np.array(group_labels_per_sample)\n",
    "custom_groups = {'nirs_parity_group': group_labels_np}\n",
    "\n",
    "# --- Rebuild SpectraSet with Groups ---\n",
    "# Using spectra_data_v4 (which has all spectral transformations and augmentations)\n",
    "ss_v5_with_groups = SpectraSet.build(\n",
    "    spectra=spectra_data_v4,\n",
    "    target=target_y_v1,\n",
    "    metadata=metadata_v1,\n",
    "    groups=custom_groups\n",
    ")\n",
    "\n",
    "print(\"--- SpectraSet with Custom Groups (ss_v5_with_groups) ---\")\n",
    "print(ss_v5_with_groups.ds)\n",
    "print(f\"\\nShape of X from ss_v5_with_groups: {ss_v5_with_groups.X().shape}\") # Shape should be same as ss_v4\n",
    "print(f\"\\nGroup labels per sample (first {total_samples}): {np.unique(group_labels_np)[:total_samples]}\")\n",
    "print(f\"\\nValue counts for 'nirs_parity_group' (per observation):\")\n",
    "print(pd.Series(ss_v5_with_groups.ds['group_id_nirs_parity_group'].values).value_counts())\n",
    "# To see counts per original sample:\n",
    "print(f\"\\nValue counts for 'nirs_parity_group' (per original sample):\")\n",
    "print(pd.Series(group_labels_np).value_counts())\n",
    "\n",
    "# Example: n=1 (s_idx=0,1), NIRS starts with 0 (even)\n",
    "# n=2 (s_idx=2,3), NIRS starts with 0 (even) for (2-1)*0\n",
    "# n=3 (s_idx=4,5), NIRS starts with 0 (even) for (3-1)*0\n",
    "# n=4 (s_idx=6,7), NIRS starts with 0 (even) for (4-1)*0\n",
    "# n=5 (s_idx=8,9), NIRS starts with 0 (even) for (5-1)*0\n",
    "# It seems all will be 'even_first_nirs' with current (n-1)*i logic. Let's check.\n",
    "# (n-1)*0 is always 0. So all are 'even'.\n",
    "# Let's adjust the grouping logic to be more interesting, e.g. based on `current_n` itself.\n",
    "# Or, let's use the sum of the first NIRS spectrum.\n",
    "\n",
    "print(\"\\n--- Adjusting Grouping Logic for Better Distribution ---\")\n",
    "group_labels_per_sample_adj = []\n",
    "for s_idx in range(total_samples):\n",
    "    original_nirs_spectrum = spectra_data_v1['nirs'][s_idx][0]\n",
    "    sum_of_nirs_values = np.sum(original_nirs_spectrum)\n",
    "    if sum_of_nirs_values % 2 != 0: # Check if sum is odd\n",
    "        group_labels_per_sample_adj.append('odd_sum_nirs')\n",
    "    else:\n",
    "        group_labels_per_sample_adj.append('even_sum_nirs')\n",
    "\n",
    "group_labels_adj_np = np.array(group_labels_per_sample_adj)\n",
    "custom_groups_adj = {'nirs_sum_parity_group': group_labels_adj_np}\n",
    "\n",
    "ss_final_with_groups = SpectraSet.build(\n",
    "    spectra=spectra_data_v4,\n",
    "    target=target_y_v1,\n",
    "    metadata=metadata_v1,\n",
    "    groups=custom_groups_adj # Use adjusted groups\n",
    ")\n",
    "print(\"--- SpectraSet with Adjusted Custom Groups (ss_final_with_groups) ---\")\n",
    "print(ss_final_with_groups.ds)\n",
    "print(f\"\\nValue counts for 'nirs_sum_parity_group' (per original sample):\")\n",
    "print(pd.Series(group_labels_adj_np).value_counts())\n",
    "# n=1, NIRS=[0,0,0,0], sum=0 (even)\n",
    "# n=2, NIRS=[0,1,2,3], sum=6 (even)\n",
    "# n=3, NIRS=[0,2,4,6], sum=12 (even)\n",
    "# n=4, NIRS=[0,3,6,9], sum=18 (even)\n",
    "# n=5, NIRS=[0,4,8,12], sum=24 (even)\n",
    "# Still all even. The data generation is very regular.\n",
    "# Let's group by n % 2 for simplicity to get a split.\n",
    "print(\"\\n--- Final Grouping Logic: n % 2 ---\")\n",
    "group_labels_n_parity = ['n_odd' if n_values_for_samples[s_idx] % 2 != 0 else 'n_even' for s_idx in range(total_samples)]\n",
    "group_labels_n_parity_np = np.array(group_labels_n_parity)\n",
    "final_custom_groups = {'n_parity_group': group_labels_n_parity_np}\n",
    "\n",
    "ss_final_grouped = SpectraSet.build(\n",
    "    spectra=spectra_data_v4,\n",
    "    target=target_y_v1,\n",
    "    metadata=metadata_v1,\n",
    "    groups=final_custom_groups\n",
    ")\n",
    "print(\"--- SpectraSet with Final Custom Groups (ss_final_grouped) ---\")\n",
    "print(ss_final_grouped.ds)\n",
    "print(f\"\\nValue counts for 'n_parity_group' (per original sample):\")\n",
    "print(pd.Series(group_labels_n_parity_np).value_counts())\n",
    "# n=1 (odd), n=2 (even), n=3 (odd), n=4 (even), n=5 (odd)\n",
    "# Each n appears twice. So 6 'n_odd' samples, 4 'n_even' samples. This is good for stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6aafb4",
   "metadata": {},
   "source": [
    "## 8. Train/Test Split\n",
    "\n",
    "We split the data into training (80%) and testing (20%) sets.\n",
    "The split is stratified based on the `n_parity_group` to ensure proportional representation of each group in both sets.\n",
    "The split is performed on the original sample indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SpectraSet with Train/Test Splits (ss_final_split) ---\n",
      "<xarray.Dataset> Size: 6kB\n",
      "Dimensions:                  (obs: 24, feature: 5, variable: 1)\n",
      "Coordinates:\n",
      "  * obs                      (obs) object 192B MultiIndex\n",
      "  * sample                   (obs) int64 192B 0 0 1 1 2 2 3 3 ... 7 8 8 8 9 9 9\n",
      "  * augmentation             (obs) int64 192B 0 1 0 1 0 1 0 1 ... 2 0 1 2 0 1 2\n",
      "  * feature                  (feature) int64 40B 0 1 2 3 4\n",
      "  * variable                 (variable) int64 8B 0\n",
      "    split                    (obs) <U5 480B 'train' 'train' ... 'train' 'train'\n",
      "    group_id_n_parity_group  (obs) <U6 576B 'n_odd' 'n_odd' ... 'n_odd' 'n_odd'\n",
      "Data variables:\n",
      "    spectra_nirs             (obs, feature) float64 960B -1.0 -1.0 ... 13.0 nan\n",
      "    spectra_raman            (obs, feature) float64 960B 105.0 105.0 ... 123.0\n",
      "    spectra_nirs_plus_01     (obs, feature) float64 960B -0.9 -0.9 ... 13.1 nan\n",
      "    spectra_raman_plus_01    (obs, feature) float64 960B 105.1 105.1 ... 123.1\n",
      "    target                   (obs, variable) int64 192B 1 1 1 1 2 ... 5 5 5 5 5\n",
      "    metadata_n_numeric       (obs) int64 192B 1 1 1 1 2 2 2 2 ... 4 5 5 5 5 5 5\n",
      "    metadata_n_str           (obs) <U1 96B '1' '1' '1' '1' ... '5' '5' '5' '5'\n",
      "    metadata_n_sum_str       (obs) <U3 288B '1+1' '1+1' '1+1' ... '5+5' '5+5'\n",
      "\n",
      "Shape of X_train: (19, 20), y_train: (19, 1)\n",
      "Shape of X_test: (5, 20), y_test: (5, 1)\n",
      "\n",
      "Original sample group counts: n_odd     6\n",
      "n_even    4\n",
      "Name: count, dtype: int64\n",
      "Train sample group counts: n_odd     5\n",
      "n_even    3\n",
      "Name: count, dtype: int64\n",
      "Test sample group counts: n_even    1\n",
      "n_odd     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Perform Train/Test Split on Sample Indices ---\n",
    "# We need unique sample indices and their corresponding group labels for stratification\n",
    "sample_indices = np.arange(total_samples) # 0 to 9 for our 10 original samples\n",
    "\n",
    "# Stratify based on group_labels_n_parity_np (one label per original sample)\n",
    "train_sample_idx, test_sample_idx = train_test_split(\n",
    "    sample_indices,\n",
    "    test_size=0.2, # 20% for test\n",
    "    stratify=group_labels_n_parity_np, # Stratify by the group labels of original samples\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Create splits array for SpectraSet ---\n",
    "# This array indicates 'train' or 'test' for each original sample\n",
    "splits_array = np.full(total_samples, 'train', dtype=object)\n",
    "splits_array[test_sample_idx] = 'test'\n",
    "\n",
    "# --- Rebuild SpectraSet with Splits Information ---\n",
    "ss_final_split = SpectraSet.build(\n",
    "    spectra=spectra_data_v4, # Use the fully processed spectra\n",
    "    target=target_y_v1,\n",
    "    metadata=metadata_v1,\n",
    "    groups=final_custom_groups, # Keep the groups\n",
    "    splits=splits_array        # Add the split information\n",
    ")\n",
    "\n",
    "print(\"--- SpectraSet with Train/Test Splits (ss_final_split) ---\")\n",
    "print(ss_final_split.ds)\n",
    "\n",
    "# --- Extract Train and Test Data using SpectraSet methods ---\n",
    "X_train = ss_final_split.X(split='train')\n",
    "y_train = ss_final_split.y(split='train')\n",
    "X_test = ss_final_split.X(split='test')\n",
    "y_test = ss_final_split.y(split='test')\n",
    "\n",
    "print(f\"\\nShape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Verify stratification (counts of groups in train/test based on original samples)\n",
    "print(f\"\\nOriginal sample group counts: {pd.Series(group_labels_n_parity_np).value_counts()}\")\n",
    "print(f\"Train sample group counts: {pd.Series(group_labels_n_parity_np[train_sample_idx]).value_counts()}\")\n",
    "print(f\"Test sample group counts: {pd.Series(group_labels_n_parity_np[test_sample_idx]).value_counts()}\")\n",
    "# Test set should have 2 samples (20% of 10).\n",
    "# If n_odd=6, n_even=4:\n",
    "# Test (2 samples): 20% of 6 n_odd is ~1.2 (so 1 or 2), 20% of 4 n_even is ~0.8 (so 0 or 1).\n",
    "# train_test_split will try its best. For small N, it might not be perfect but aims for proportion.\n",
    "# With 2 test samples: if 1 n_odd, 1 n_even, that's good.\n",
    "# Test sample indices: {test_sample_idx}, their groups: {group_labels_n_parity_np[test_sample_idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d27423",
   "metadata": {},
   "source": [
    "## 9. Repeated K-Fold Cross-Validation\n",
    "\n",
    "We apply `RepeatedKFold` to the 80% training data.\n",
    "- `n_splits=5`, `n_repeats=2`.\n",
    "- The folding is done on the *sample indices* of the training set.\n",
    "- The prompt mentioned \"excluding only a fold from one random group\". Standard `RepeatedKFold` operates on the provided samples. If group-specific exclusion per fold is needed, a more custom loop would be required. Here, we perform standard `RepeatedKFold` on the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique samples in training set: 8\n",
      "Unique sample IDs in training set: [0 1 2 3 5 7 8 9]\n",
      "\n",
      "--- Iterating through RepeatedKFold Splits ---\n",
      "Fold 1:\n",
      "  Train samples in fold (IDs): [0 2 3 5 8 9]\n",
      "  Validation samples in fold (IDs): [1 7]\n",
      "  X_train_fold shape: (14, 20), y_train_fold shape: (14, 1)\n",
      "  X_val_fold shape: (5, 20), y_val_fold shape: (5, 1)\n",
      "  Example y_val_fold (first 5): [1 1 4 4 4]\n",
      "Fold 2:\n",
      "  Train samples in fold (IDs): [1 2 3 5 7 8]\n",
      "  Validation samples in fold (IDs): [0 9]\n",
      "  X_train_fold shape: (14, 20), y_train_fold shape: (14, 1)\n",
      "  X_val_fold shape: (5, 20), y_val_fold shape: (5, 1)\n",
      "Fold 3:\n",
      "  Train samples in fold (IDs): [0 1 3 7 8 9]\n",
      "  Validation samples in fold (IDs): [2 5]\n",
      "  X_train_fold shape: (15, 20), y_train_fold shape: (15, 1)\n",
      "  X_val_fold shape: (4, 20), y_val_fold shape: (4, 1)\n",
      "Fold 4:\n",
      "  Train samples in fold (IDs): [0 1 2 5 7 8 9]\n",
      "  Validation samples in fold (IDs): [3]\n",
      "  X_train_fold shape: (17, 20), y_train_fold shape: (17, 1)\n",
      "  X_val_fold shape: (2, 20), y_val_fold shape: (2, 1)\n",
      "Fold 5:\n",
      "  Train samples in fold (IDs): [0 1 2 3 5 7 9]\n",
      "  Validation samples in fold (IDs): [8]\n",
      "  X_train_fold shape: (16, 20), y_train_fold shape: (16, 1)\n",
      "  X_val_fold shape: (3, 20), y_val_fold shape: (3, 1)\n",
      "Fold 6:\n",
      "  Train samples in fold (IDs): [0 1 2 5 7 8]\n",
      "  Validation samples in fold (IDs): [3 9]\n",
      "  X_train_fold shape: (14, 20), y_train_fold shape: (14, 1)\n",
      "  X_val_fold shape: (5, 20), y_val_fold shape: (5, 1)\n",
      "Fold 7:\n",
      "  Train samples in fold (IDs): [1 2 3 7 8 9]\n",
      "  Validation samples in fold (IDs): [0 5]\n",
      "  X_train_fold shape: (15, 20), y_train_fold shape: (15, 1)\n",
      "  X_val_fold shape: (4, 20), y_val_fold shape: (4, 1)\n",
      "Fold 8:\n",
      "  Train samples in fold (IDs): [0 1 3 5 8 9]\n",
      "  Validation samples in fold (IDs): [2 7]\n",
      "  X_train_fold shape: (14, 20), y_train_fold shape: (14, 1)\n",
      "  X_val_fold shape: (5, 20), y_val_fold shape: (5, 1)\n",
      "Fold 9:\n",
      "  Train samples in fold (IDs): [0 2 3 5 7 8 9]\n",
      "  Validation samples in fold (IDs): [1]\n",
      "  X_train_fold shape: (17, 20), y_train_fold shape: (17, 1)\n",
      "  X_val_fold shape: (2, 20), y_val_fold shape: (2, 1)\n",
      "Fold 10:\n",
      "  Train samples in fold (IDs): [0 1 2 3 5 7 9]\n",
      "  Validation samples in fold (IDs): [8]\n",
      "  X_train_fold shape: (16, 20), y_train_fold shape: (16, 1)\n",
      "  X_val_fold shape: (3, 20), y_val_fold shape: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Setup RepeatedKFold ---\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "\n",
    "# We need to apply KFold to the *samples* within the training set.\n",
    "# `train_sample_idx` contains the original indices (0-9) of the samples in the training set.\n",
    "# Let's get a SpectraSet view of only the training data samples.\n",
    "ss_train_samples_only = ss_final_split.subset_by_samples(train_sample_idx)\n",
    "\n",
    "# The KFold split will be on the *unique sample IDs present in ss_train_samples_only*.\n",
    "# These are not necessarily contiguous from 0 to len(train_sample_idx)-1 if we directly use train_sample_idx.\n",
    "# Instead, we split based on the number of unique samples in the training set.\n",
    "unique_train_sample_ids_in_ss = ss_train_samples_only.ds.sample.to_series().unique()\n",
    "# unique_train_sample_ids_in_ss contains the actual sample IDs (from 0-9 range) that are in training.\n",
    "\n",
    "print(f\"Number of unique samples in training set: {len(unique_train_sample_ids_in_ss)}\")\n",
    "print(f\"Unique sample IDs in training set: {unique_train_sample_ids_in_ss}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Iterating through RepeatedKFold Splits ---\")\n",
    "for fold_num, (train_fold_indices, val_fold_indices) in enumerate(rkf.split(unique_train_sample_ids_in_ss)):\n",
    "    # train_fold_indices and val_fold_indices are indices *within* unique_train_sample_ids_in_ss\n",
    "    \n",
    "    # Get the actual sample IDs for this fold's train and validation sets\n",
    "    current_fold_train_sample_ids = unique_train_sample_ids_in_ss[train_fold_indices]\n",
    "    current_fold_val_sample_ids = unique_train_sample_ids_in_ss[val_fold_indices]\n",
    "    \n",
    "    # Get SpectraSet views for this specific fold's train and validation samples\n",
    "    ss_fold_train = ss_train_samples_only.subset_by_samples(current_fold_train_sample_ids)\n",
    "    ss_fold_val = ss_train_samples_only.subset_by_samples(current_fold_val_sample_ids)\n",
    "    \n",
    "    # Extract X and y for this fold\n",
    "    X_fold_tr = ss_fold_train.X()\n",
    "    y_fold_tr = ss_fold_train.y()\n",
    "    X_fold_val = ss_fold_val.X()\n",
    "    y_fold_val = ss_fold_val.y()\n",
    "    \n",
    "    print(f\"Fold {fold_num + 1}:\")\n",
    "    print(f\"  Train samples in fold (IDs): {current_fold_train_sample_ids}\")\n",
    "    print(f\"  Validation samples in fold (IDs): {current_fold_val_sample_ids}\")\n",
    "    print(f\"  X_train_fold shape: {X_fold_tr.shape}, y_train_fold shape: {y_fold_tr.shape}\")\n",
    "    print(f\"  X_val_fold shape: {X_fold_val.shape}, y_val_fold shape: {y_fold_val.shape}\")\n",
    "    if fold_num == 0: # Print details for the first fold\n",
    "        print(f\"  Example y_val_fold (first 5): {y_fold_val[:5].ravel()}\")\n",
    "\n",
    "# Note: With n_splits=5 and 8 training samples, some validation sets might be small (1-2 samples).\n",
    "# This means X_val/y_val shapes will reflect observations from those 1-2 samples.\n",
    "# E.g., if a val sample has 2 augs, its y_val will have 2 identical target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f82991",
   "metadata": {},
   "source": [
    "## 10. Unpacking Groups\n",
    "\n",
    "Finally, we iterate through the defined groups (`n_parity_group`) in the full dataset (before train/test split, but with all transformations and augmentations - `ss_final_grouped`) and display the shape of `X` and `y` for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594d911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Shapes per Group (from ss_final_grouped) ---\n",
      "\n",
      "Group: 'n_even'\n",
      "  Shape of X for this group: (10, 20)\n",
      "  Shape of y for this group: (10, 1)\n",
      "  Unique 'n' values (target y) in this group: [2 4]\n",
      "    All target 'n' values are even: True\n",
      "\n",
      "Group: 'n_odd'\n",
      "  Shape of X for this group: (14, 20)\n",
      "  Shape of y for this group: (14, 1)\n",
      "  Unique 'n' values (target y) in this group: [1 3 5]\n",
      "    All target 'n' values are odd: True\n"
     ]
    }
   ],
   "source": [
    "# --- Unpack and Inspect Data by Group ---\n",
    "# Using ss_final_grouped, which contains all samples and their group assignments.\n",
    "unique_group_values = np.unique(ss_final_grouped.ds['group_id_n_parity_group'].values)\n",
    "\n",
    "print(f\"--- Data Shapes per Group (from ss_final_grouped) ---\")\n",
    "for group_val in unique_group_values:\n",
    "    # Filter SpectraSet by the current group value\n",
    "    # The groups argument in X() and y() filters observations.\n",
    "    X_group = ss_final_grouped.X(groups={'n_parity_group': group_val})\n",
    "    y_group = ss_final_grouped.y(groups={'n_parity_group': group_val})\n",
    "    \n",
    "    print(f\"\\nGroup: '{group_val}'\")\n",
    "    print(f\"  Shape of X for this group: {X_group.shape}\")\n",
    "    print(f\"  Shape of y for this group: {y_group.shape}\")\n",
    "    # Verify target values in this group (should be consistent with n_parity)\n",
    "    unique_n_in_group_y = np.unique(y_group.ravel())\n",
    "    print(f\"  Unique 'n' values (target y) in this group: {unique_n_in_group_y}\")\n",
    "    if group_val == 'n_odd':\n",
    "        all_odd = all(n % 2 != 0 for n in unique_n_in_group_y)\n",
    "        print(f\"    All target 'n' values are odd: {all_odd}\")\n",
    "    elif group_val == 'n_even':\n",
    "        all_even = all(n % 2 == 0 for n in unique_n_in_group_y)\n",
    "        print(f\"    All target 'n' values are even: {all_even}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0dca2",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "This notebook demonstrated a comprehensive workflow using `SpectraSet`, covering data generation, various types of augmentations and transformations, grouping, splitting, and cross-validation preparation. Each step involved careful manipulation of the spectral data and rebuilding or querying the `SpectraSet` object to reflect the changes, while displaying shapes to track the data transformations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
