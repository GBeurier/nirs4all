{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (6, 10)\n",
      "┌────────┬───────────┬────────┬────────────┬───┬───────┬─────────┬─────────┬────────┐\n",
      "│ obs_id ┆ sample_id ┆ source ┆ processing ┆ … ┆ split ┆ fold_id ┆ row_idx ┆ target │\n",
      "│ ---    ┆ ---       ┆ ---    ┆ ---        ┆   ┆ ---   ┆ ---     ┆ ---     ┆ ---    │\n",
      "│ i64    ┆ i64       ┆ str    ┆ str        ┆   ┆ null  ┆ null    ┆ i64     ┆ i64    │\n",
      "╞════════╪═══════════╪════════╪════════════╪═══╪═══════╪═════════╪═════════╪════════╡\n",
      "│ 0      ┆ 0         ┆ nirs   ┆ raw        ┆ … ┆ null  ┆ null    ┆ 0       ┆ 0      │\n",
      "│ 1      ┆ 1         ┆ nirs   ┆ raw        ┆ … ┆ null  ┆ null    ┆ 1       ┆ 1      │\n",
      "│ 2      ┆ 2         ┆ nirs   ┆ raw        ┆ … ┆ null  ┆ null    ┆ 2       ┆ 2      │\n",
      "│ 3      ┆ 0         ┆ raman  ┆ raw        ┆ … ┆ null  ┆ null    ┆ 3       ┆ 0      │\n",
      "│ 4      ┆ 1         ┆ raman  ┆ raw        ┆ … ┆ null  ┆ null    ┆ 4       ┆ 1      │\n",
      "│ 5      ┆ 2         ┆ raman  ┆ raw        ┆ … ┆ null  ┆ null    ┆ 5       ┆ 2      │\n",
      "└────────┴───────────┴────────┴────────────┴───┴───────┴─────────┴─────────┴────────┘\n",
      "[[10.  11. ]\n",
      " [10.1 11.1]\n",
      " [40.  41. ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SpectraSet._filter() got an unexpected keyword argument 'sources'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 270\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(ss\u001b[38;5;241m.\u001b[39mmeta)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mprint\u001b[39m(ss\u001b[38;5;241m.\u001b[39mX(sources\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraman\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[40], line 170\u001b[0m, in \u001b[0;36mSpectraSet.y\u001b[1;34m(self, encode_labels, **kw)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21my\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, encode_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 170\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m meta\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n",
      "\u001b[1;31mTypeError\u001b[0m: SpectraSet._filter() got an unexpected keyword argument 'sources'"
     ]
    }
   ],
   "source": [
    "# spectra_set_arrow.py\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Sequence, Optional, Union, Literal, Tuple\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "AugPolicy   = Literal[\"all\", \"original\", \"augmented\"]\n",
    "FeatShape   = Literal[\"concatenate\", \"interlace\", \"2d\", \"transpose2d\"]\n",
    "\n",
    "class SpectraSet:\n",
    "    \"\"\"\n",
    "    A minimal Arrow + Polars replacement for the previous xarray SpectraSet.\n",
    "    All public methods keep (almost) the same signature.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # constructor / helpers                                              #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def __init__(self, meta: pl.DataFrame, spectra_tbl: pa.Table):\n",
    "        self.meta   = meta      # every row = one observation\n",
    "        self.tbl    = spectra_tbl    # column \"spec\" list<float32>\n",
    "\n",
    "    # >>> factory for toy examples ------------------------------------- #\n",
    "    @classmethod\n",
    "    def from_dict(cls,\n",
    "                  spectra: Dict[str, Sequence[Sequence[Sequence[float]]]],\n",
    "                  # spectra[source][sample][augmentation] = 1-D list/array\n",
    "                  target: Optional[Sequence[Any]] = None):\n",
    "        rows_meta = []\n",
    "        spec_buf  = []\n",
    "        obs_id    = 0\n",
    "        for src, samples in spectra.items():\n",
    "            for sid, aug_list in enumerate(samples):\n",
    "                for aug_id, vec in enumerate(aug_list):\n",
    "                    spec_buf.append(np.asarray(vec, dtype=np.float32))\n",
    "                    rows_meta.append({\n",
    "                        \"obs_id\":       obs_id,\n",
    "                        \"sample_id\":    sid,\n",
    "                        \"source\":       src,\n",
    "                        \"processing\":   \"raw\",\n",
    "                        \"augmentation_id\": aug_id,\n",
    "                        \"branch\":       None,\n",
    "                        \"split\":        None,\n",
    "                        \"fold_id\":      None,\n",
    "                        \"row_idx\":      obs_id\n",
    "                    })\n",
    "                    obs_id += 1\n",
    "        meta = pl.DataFrame(rows_meta)\n",
    "        tbl  = pa.Table.from_pydict({\"spec\": pa.array(spec_buf, type=pa.list_(pa.float32()))})\n",
    "        if target is not None:\n",
    "            meta = meta.with_columns(pl.Series(\"target\", list(target))[meta[\"sample_id\"]])\n",
    "        return cls(meta, tbl)\n",
    "\n",
    "    # internal: return a filtered meta dataframe ---------------------- #\n",
    "    def _filter(self,\n",
    "                *,\n",
    "                split:  Optional[Union[str, Sequence[str]]] = None,\n",
    "                fold_id:Optional[Union[int, Sequence[int]]] = None,\n",
    "                groups: Optional[Dict[str, Any]]           = None,\n",
    "                branch: Optional[Union[str, Sequence[str]]] = None,\n",
    "                augment:AugPolicy = \"all\") -> pl.DataFrame:\n",
    "\n",
    "        m = self.meta.lazy()\n",
    "\n",
    "        def _isin(col, val):\n",
    "            return pl.col(col).is_in(val if isinstance(val, (list, tuple, np.ndarray)) else [val])\n",
    "\n",
    "        if split is not None:     m = m.filter(_isin(\"split\", split))\n",
    "        if fold_id is not None:   m = m.filter(_isin(\"fold_id\", fold_id))\n",
    "        if branch is not None:    m = m.filter(_isin(\"branch\", branch))\n",
    "        if groups:\n",
    "            for gname, v in groups.items():\n",
    "                m = m.filter(_isin(f\"group_{gname}\", v))\n",
    "        if augment != \"all\":\n",
    "            if augment == \"original\":\n",
    "                m = m.filter(pl.col(\"augmentation_id\") == 0)\n",
    "            else:  # \"augmented\"\n",
    "                m = m.filter(pl.col(\"augmentation_id\") > 0)\n",
    "\n",
    "        return m.collect()\n",
    "\n",
    "    # internal: Arrow take -------------------------------------------- #\n",
    "    def _take_specs(self, row_idx: np.ndarray) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Given Arrow row indices, return a Python list of NumPy 1-D float32\n",
    "        vectors (one per observation, in the same order).\n",
    "        \"\"\"\n",
    "        arr = pc.take(self.tbl.column(\"spec\"), pa.array(row_idx))\n",
    "        # arr is a ListArray; arr.to_pylist() -> list[list[float]]\n",
    "        return [np.asarray(v, dtype=np.float32) for v in arr.to_pylist()]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # public getters                                                     #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def X(self, *, split=None, fold_id=None, groups=None, branch=None,\n",
    "           sources: Optional[Union[bool, str, Sequence[str]]] = None,\n",
    "           augment: AugPolicy = \"all\",\n",
    "           feature_shape: FeatShape = \"concatenate\") -> np.ndarray:\n",
    "\n",
    "        meta = self._filter(split=split, fold_id=fold_id, groups=groups,\n",
    "                            branch=branch, augment=augment)\n",
    "\n",
    "        if meta.is_empty():\n",
    "            return np.empty((0, 0), dtype=np.float32)\n",
    "\n",
    "        # --- choose sources ----------------------------------------- #\n",
    "        if sources is True or sources is None:\n",
    "            kept = meta\n",
    "        elif sources is False:\n",
    "            return np.empty((meta.height, 0), dtype=np.float32)\n",
    "        elif isinstance(sources, str):\n",
    "            kept = meta.filter(pl.col(\"source\") == sources)\n",
    "        else:\n",
    "            kept = meta.filter(pl.col(\"source\").is_in(list(sources)))\n",
    "\n",
    "        # --- fetch spectra ------------------------------------------ #\n",
    "        vecs = self._take_specs(kept[\"row_idx\"].to_numpy())\n",
    "\n",
    "        # concatenate/interlace/reshape per feature_shape ------------- #\n",
    "        if feature_shape == \"concatenate\":\n",
    "            # 1. quelles colonnes définissent une 'clé observation' ?\n",
    "            key_cols = [\"sample_id\", \"processing\", \"augmentation_id\",\n",
    "                        \"branch\", \"split\", \"fold_id\"]\n",
    "            keys = kept.select(key_cols)\n",
    "\n",
    "            # 2. ordre stable des sources  & taille max par source\n",
    "            srcs_order = sorted(set(kept[\"source\"]))\n",
    "            max_feat = {s: max(len(v) for v, s2 in zip(vecs, kept[\"source\"])\n",
    "                               if s2 == s) for s in srcs_order}\n",
    "\n",
    "            # 3. dictionnaire {key_tuple -> dict(source -> vector)}\n",
    "            rows: Dict[Tuple, Dict[str, np.ndarray]] = {}\n",
    "            for key_vals, src, vec in zip(keys.iter_rows(), kept[\"source\"], vecs):\n",
    "                k = tuple(key_vals)\n",
    "                rows.setdefault(k,\n",
    "                                {s: np.full(max_feat[s], np.nan, np.float32)\n",
    "                                 for s in srcs_order})\n",
    "                rows[k][src][:len(vec)] = vec\n",
    "\n",
    "            # 4. concatène chaque source dans l’ordre & empile les lignes\n",
    "            matrix = np.vstack([\n",
    "                np.hstack([rows[k][s] for s in srcs_order])\n",
    "                for k in sorted(rows)            # ordre déterministe\n",
    "            ])\n",
    "            return matrix\n",
    "        else:\n",
    "            # Build 3-D tensor [obs, source, feat] first\n",
    "            srcs   = kept[\"source\"].to_list()\n",
    "            uniq   = {s: i for i, s in enumerate(sorted(set(srcs)))}\n",
    "            max_f  = max(len(v) for v in vecs)\n",
    "            tensor = np.full((kept.height, len(uniq), max_f), np.nan,\n",
    "                             dtype=np.float32)\n",
    "            for i, (s, v) in enumerate(zip(srcs, vecs)):\n",
    "                tensor[i, uniq[s], :len(v)] = v\n",
    "            if feature_shape == \"2d\":\n",
    "                return tensor\n",
    "            if feature_shape == \"transpose2d\":\n",
    "                return np.transpose(tensor, (0, 2, 1))\n",
    "            # \"interlace\"\n",
    "            return tensor.reshape(tensor.shape[0], -1)\n",
    "\n",
    "    def X_with_labels(self, **kw) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        meta   = self._filter(**{k: kw[k] for k in kw if k in\n",
    "                                 {\"split\", \"fold_id\", \"groups\", \"branch\", \"augment\"}})\n",
    "        labels = meta[\"obs_id\"].to_numpy()\n",
    "        return self.X(**kw), labels\n",
    "\n",
    "    def y(self, *, encode_labels=False, **kw) -> np.ndarray:\n",
    "        meta = self._filter(**kw)\n",
    "        if \"target\" not in meta.columns:\n",
    "            return np.array([])\n",
    "        y = meta[\"target\"].to_numpy()\n",
    "        if encode_labels and y.dtype.kind in \"UO\":\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            enc = LabelEncoder()\n",
    "            y   = enc.fit_transform(y)\n",
    "        return y\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # mutators                                                           #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def add_split(self, labels: Sequence[Any]):\n",
    "        self.meta = self.meta.with_columns(split=pl.Series(\"split\", labels)[self.meta[\"sample_id\"]])\n",
    "\n",
    "    def add_folds(self, labels: Sequence[Any]):\n",
    "        self.meta = self.meta.with_columns(fold_id=pl.Series(labels)[self.meta[\"sample_id\"]])\n",
    "\n",
    "    def add_groups(self, name: str, labels: Sequence[Any]):\n",
    "        self.meta = self.meta.with_columns(pl.Series(f\"group_{name}\", labels)[self.meta[\"sample_id\"]])\n",
    "\n",
    "    # --- add_processing = sample-level augmentation ------------------- #\n",
    "    def augment_samples(self,\n",
    "                        new_samples: Dict[str, Sequence[np.ndarray]],\n",
    "                        parent_obs: Sequence[int],\n",
    "                        name: str = \"aug\",\n",
    "                        branch: Any = None):\n",
    "        \"\"\"\n",
    "        Duplicate observations referenced by `parent_obs`, replace their spectra\n",
    "        with `new_samples[src][i]`, push into Arrow table, extend meta.\n",
    "        \"\"\"\n",
    "        if not new_samples:\n",
    "            raise ValueError(\"new_samples cannot be empty.\")\n",
    "        n_new = len(parent_obs)\n",
    "        # 1/ build Arrow column\n",
    "        new_vecs = []\n",
    "        rows     = []\n",
    "        next_rid = int(self.tbl.num_rows)\n",
    "        for i, obs in enumerate(parent_obs):\n",
    "            parent = self.meta.row(obs)\n",
    "            for src, vec_list in new_samples.items():\n",
    "                vec = np.asarray(vec_list[i], dtype=np.float32)\n",
    "                new_vecs.append(vec)\n",
    "                rows.append({\n",
    "                    **parent,\n",
    "                    \"obs_id\":        int(self.meta[\"obs_id\"].max()) + 1 + i,\n",
    "                    \"processing\":    name,\n",
    "                    \"augmentation_id\": parent[\"augmentation_id\"] + 1,\n",
    "                    \"branch\":        branch,\n",
    "                    \"row_idx\":       next_rid\n",
    "                })\n",
    "                next_rid += 1\n",
    "        self.tbl  = pa.concat_tables([self.tbl,\n",
    "                                      pa.Table.from_pydict({\"spec\":\n",
    "                                      pa.array(new_vecs,\n",
    "                                               type=pa.list_(pa.float32()))})])\n",
    "        self.meta = pl.concat([self.meta, pl.DataFrame(rows)])\n",
    "\n",
    "    # --- add_features -------------------------------------------------- #\n",
    "    def add_features(self, source: str, new_feat: np.ndarray,\n",
    "                     feature_names: Optional[Sequence[str]] = None):\n",
    "        \"\"\"\n",
    "        Append features horizontally for *all* observations of a source.\n",
    "        new_feat shape = (n_obs_source, n_new_feat)\n",
    "        \"\"\"\n",
    "        mask = self.meta[\"source\"] == source\n",
    "        row_idx = self.meta.filter(mask)[\"row_idx\"].to_numpy()\n",
    "        old_vecs = self._take_specs(row_idx)\n",
    "        if new_feat.shape[0] != len(old_vecs):\n",
    "            raise ValueError(\"new_feat rows mismatch.\")\n",
    "        merged = [np.hstack([o, n.astype(np.float32)]) for o, n in\n",
    "                  zip(old_vecs, new_feat)]\n",
    "        col = pa.array(merged, type=pa.list_(pa.float32()))\n",
    "        self.tbl = self.tbl.set_column(0, \"spec\",\n",
    "                   pc.if_else(pc.is_in(pa.array(row_idx),\n",
    "                                       value_set=pa.array(row_idx)),\n",
    "                              col, self.tbl.column(\"spec\")))\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # predictions                                                        #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def add_prediction(self, model_id: str, y_pred: Sequence[float], **kw):\n",
    "        meta = self._filter(**kw)\n",
    "        if len(y_pred) != meta.height:\n",
    "            raise ValueError(\"y_pred length mismatch after filtering.\")\n",
    "        pred_col = np.full(self.meta.height, np.nan, dtype=float)\n",
    "        pred_col[meta[\"obs_id\"].to_numpy()] = y_pred\n",
    "        self.meta = self.meta.with_columns(\n",
    "            pl.Series(f\"pred_{model_id}\", pred_col))\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "# Fix: Each sample should be a list of augmentations (even if only one)\n",
    "toy = {\n",
    "    \"nirs\":  [ [[1,2,3]], [[1.1,2.1,3.1]], [[4,5,6]] ],\n",
    "    \"raman\": [ [[10,11]], [[10.1,11.1]], [[40,41]] ]\n",
    "}\n",
    "ss = SpectraSet.from_dict(toy, target=[0, 1, 2])\n",
    "print(ss.meta)\n",
    "print(ss.X(sources=\"raman\"))\n",
    "print(ss.y(sources=\"raman\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fab9fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from typing import Any, Sequence, Union\n",
    "\n",
    "class SpectraDataset:\n",
    "    \"\"\"\n",
    "    Dataset ultra-rapide pour ML basé sur polars/Arrow.\n",
    "    \n",
    "    Chaque spectre est stocké dans une colonne 'spectrum' de type List[Float64],\n",
    "    et un identifiant unique 'row_id' est assigné à chaque entrée.\n",
    "    Les index par défaut sont gérés dynamiquement, avec broadcast ou vecteur dédié.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_INDEX = (\n",
    "        \"origin\", \"sample\", \"type\", \"set\",\n",
    "        \"processing\", \"augmentation\", \"branch\"\n",
    "    )\n",
    "    _DEFAULT_VALUES: dict[str, Any] = {\n",
    "        \"set\":        \"train\",\n",
    "        \"processing\": \"raw\",\n",
    "        \"augmentation\": \"raw\",\n",
    "        \"branch\":       0,\n",
    "    }\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.df: pl.DataFrame | None = None\n",
    "        self._next_id: int = 0\n",
    "\n",
    "    def _mask(self, **filters: Any) -> pl.Expr:\n",
    "        exprs = []\n",
    "        for key, val in filters.items():\n",
    "            if isinstance(val, (list, tuple, set, np.ndarray)):\n",
    "                exprs.append(pl.col(key).is_in(val))\n",
    "            else:\n",
    "                exprs.append(pl.col(key) == val)\n",
    "        return exprs[0] if len(exprs) == 1 else pl.all_horizontal(exprs)\n",
    "\n",
    "    def _select(self, **filters: Any) -> pl.DataFrame:\n",
    "        if self.df is None:\n",
    "            return pl.DataFrame()\n",
    "        return self.df.filter(self._mask(**filters)) if filters else self.df\n",
    "\n",
    "    def add_spectra(\n",
    "        self,\n",
    "        spectra: Sequence[Sequence[float]],\n",
    "        target: Sequence[Any] | None = None,\n",
    "        **index_values: Union[Any, Sequence[Any]],\n",
    "    ) -> None:\n",
    "        n = len(spectra)\n",
    "        tgt = target if target is not None else [None] * n\n",
    "        if len(tgt) != n:\n",
    "            raise ValueError(\"La longueur de 'target' ne correspond pas au nombre de spectres\")\n",
    "\n",
    "        data: dict[str, list[Any]] = {}\n",
    "        for k in self.DEFAULT_INDEX:\n",
    "            if k in index_values:\n",
    "                v = index_values[k]\n",
    "                if isinstance(v, Sequence) and not isinstance(v, (str, bytes)):\n",
    "                    if len(v) != n:\n",
    "                        raise ValueError(f\"Index '{k}' longueur {len(v)} != {n}\")\n",
    "                    data[k] = list(v)\n",
    "                else:\n",
    "                    data[k] = [v] * n\n",
    "            else:\n",
    "                default = self._DEFAULT_VALUES.get(k)\n",
    "                data[k] = [default] * n\n",
    "\n",
    "        data[\"spectrum\"] = [list(vec) for vec in spectra]\n",
    "        data[\"target\"]   = list(tgt)\n",
    "        data[\"row_id\"]   = list(range(self._next_id, self._next_id + n))\n",
    "        self._next_id += n\n",
    "\n",
    "        new_df = pl.DataFrame(data)\n",
    "        if self.df is None:\n",
    "            self.df = new_df\n",
    "        else:\n",
    "            self.df = pl.concat([self.df, new_df], how=\"vertical\")\n",
    "\n",
    "    def change_spectra(\n",
    "        self,\n",
    "        new_spectra: Sequence[Sequence[float]],\n",
    "        **filters: Any,\n",
    "    ) -> None:\n",
    "        if self.df is None:\n",
    "            return\n",
    "        mask = self._mask(**filters)\n",
    "        n = self.df.filter(mask).height\n",
    "        if n != len(new_spectra):\n",
    "            raise ValueError(\"new_spectra count mismatch\")\n",
    "        self.df = self.df.with_columns(\n",
    "            pl.when(mask)\n",
    "              .then(pl.Series(new_spectra, dtype=pl.List(pl.Float64)))\n",
    "              .otherwise(pl.col(\"spectrum\"))\n",
    "              .alias(\"spectrum\")\n",
    "        )\n",
    "\n",
    "    def add_tag(\n",
    "        self,\n",
    "        tag_name: str,\n",
    "        tag_values: Union[Any, Sequence[Any]],\n",
    "        **filters: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Ajoute une nouvelle colonne `tag_name`.\n",
    "        - Si `tag_values` scalaire → broadcast sur toutes les lignes filtrées.\n",
    "        - Si `tag_values` séquence de longueur m → assigné séquentiellement aux m lignes filtrées.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Dataset vide.\")\n",
    "        if tag_name in self.df.columns:\n",
    "            raise ValueError(f\"Le tag '{tag_name}' existe déjà ; utilisez set_tag.\")\n",
    "\n",
    "        df = self.df\n",
    "        mask_series = df.select(self._mask(**filters).alias(\"_mask\"))\n",
    "        mask = mask_series.to_series()\n",
    "        total = len(mask)\n",
    "        # Construire liste complète\n",
    "        full: list[Any] = [None] * total\n",
    "        if isinstance(tag_values, Sequence) and not isinstance(tag_values, (str, bytes)):\n",
    "            values = list(tag_values)\n",
    "            if sum(mask) != len(values):\n",
    "                raise ValueError(\"Le nombre de 'tag_values' ne correspond pas aux lignes filtrées\")\n",
    "            it = iter(values)\n",
    "            for i, m in enumerate(mask):\n",
    "                if m:\n",
    "                    full[i] = next(it)\n",
    "        else:\n",
    "            for i, m in enumerate(mask):\n",
    "                if m:\n",
    "                    full[i] = tag_values\n",
    "        self.df = df.with_columns(pl.Series(tag_name, full))\n",
    "\n",
    "    def set_tag(\n",
    "        self,\n",
    "        tag_name: str,\n",
    "        new_value: Union[Any, Sequence[Any]],\n",
    "        **filters: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Change les valeurs d'un tag existant.\n",
    "        - Si `new_value` scalaire → broadcast.\n",
    "        - Si `new_value` séquence de longueur m → assigné séquentiellement aux m lignes filtrées.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Dataset vide.\")\n",
    "        if tag_name not in self.df.columns:\n",
    "            raise KeyError(f\"Tag '{tag_name}' inexistant.\")\n",
    "\n",
    "        df = self.df\n",
    "        mask_series = df.select(self._mask(**filters).alias(\"_mask\")).to_series()\n",
    "        total = len(mask_series)\n",
    "        full = df.get_column(tag_name).to_list()\n",
    "        if isinstance(new_value, Sequence) and not isinstance(new_value, (str, bytes)):\n",
    "            vals = list(new_value)\n",
    "            if sum(mask_series) != len(vals):\n",
    "                raise ValueError(\"Le nombre de 'new_value' ne correspond pas aux lignes filtrées\")\n",
    "            it = iter(vals)\n",
    "            for i, m in enumerate(mask_series):\n",
    "                if m:\n",
    "                    full[i] = next(it)\n",
    "        else:\n",
    "            for i, m in enumerate(mask_series):\n",
    "                if m:\n",
    "                    full[i] = new_value\n",
    "        self.df = df.with_columns(pl.Series(tag_name, full))\n",
    "\n",
    "    def X(\n",
    "        self,\n",
    "        pad: bool = False,\n",
    "        pad_value: float = np.nan,\n",
    "        as_arrow: bool = False,\n",
    "        return_ids: bool = True,\n",
    "        **filters: Any,\n",
    "    ) -> Union[np.ndarray, tuple[Union[np.ndarray, \"pyarrow.ListArray\"], np.ndarray]]:\n",
    "        sub = self._select(**filters)\n",
    "        spectra = sub[\"spectrum\"].to_list()\n",
    "\n",
    "        if as_arrow and not pad:\n",
    "            import pyarrow as pa\n",
    "            arr = pa.array(spectra, type=pa.list_(pa.float64()))\n",
    "            ids = sub[\"row_id\"].to_numpy() if return_ids else None\n",
    "            return (arr, ids) if return_ids else arr\n",
    "\n",
    "        if pad:\n",
    "            max_len = max((len(v) for v in spectra), default=0)\n",
    "            out = np.full((len(spectra), max_len), pad_value, dtype=np.float64)\n",
    "            for i, vec in enumerate(spectra):\n",
    "                out[i, : len(vec)] = vec\n",
    "        else:\n",
    "            out = np.array(spectra, dtype=object)\n",
    "\n",
    "        if return_ids:\n",
    "            ids = sub[\"row_id\"].to_numpy()\n",
    "            return out, ids\n",
    "        return out\n",
    "\n",
    "    def y(self, **filters: Any) -> np.ndarray:\n",
    "        sub = self._select(**filters)\n",
    "        return sub[\"target\"].to_numpy()\n",
    "\n",
    "    def to_arrow_table(self) -> \"pyarrow.Table\":\n",
    "        return self.df.to_arrow()  # type: ignore\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.height if self.df is not None else 0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        cols = self.df.columns if self.df is not None else []\n",
    "        return f\"SpectraDataset(n={len(self)}, cols={cols})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed154841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2, 0.3] 0 0.25\n",
      "[1 1 'nirs' 'train' 'raw' 'raw' 0 array([0.1, 0.2, 0.3]) 0.25 0 'A']\n",
      "[1.0, 1.1] 1 1.05\n",
      "[1 2 'nirs' 'train' 'raw' 'raw' 0 array([1. , 1.1]) 1.05 1 'B']\n"
     ]
    }
   ],
   "source": [
    "ds = SpectraDataset()\n",
    "\n",
    "# ajout de 2 spectres\n",
    "ds.add_spectra(\n",
    "    spectra=[[0.1, 0.2, 0.3], [1.0, 1.1]],\n",
    "    target=[0.25, 1.05],\n",
    "    origin=1,\n",
    "    sample=[1, 2],\n",
    "    type=\"nirs\"\n",
    ")\n",
    "\n",
    "# récupérer X/y côté train\n",
    "X, ids = ds.X(pad=True)  # pad à la longueur max\n",
    "y = ds.y()\n",
    "\n",
    "# taguer un groupe de spectres\n",
    "ds.add_tag(\"groupe\", [\"A\", \"B\"], type=\"nirs\")\n",
    "\n",
    "c = ds._select().to_numpy()\n",
    "a, b = ds.X(pad=False)\n",
    "t = ds.y()\n",
    "for i in range(len(a)):\n",
    "    print(a[i], b[i], t[i])\n",
    "    print(c[i])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
