{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "304bfbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from SpectraDataset import SpectraDataset\n",
    "\n",
    "dataset_reg_1_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression\n",
    "dataset_reg_2_1 = SpectraDataset(task_type=\"regression\")  # Single-output regression (first output)\n",
    "dataset_reg_2_2 = SpectraDataset(task_type=\"regression\")  # Single-output regression (second output)\n",
    "dataset_cla_2_1 = SpectraDataset(task_type=\"classification\")  # Single-output classification (first output)\n",
    "dataset_cla_2_2 = SpectraDataset(task_type=\"classification\")  # Single-output classification (second output)\n",
    "dataset_bin_1_1 = SpectraDataset(task_type=\"binary\")  # Binary classification\n",
    "\n",
    "# Features\n",
    "f1_source = np.random.rand(100, 1000) * 2.5 + 1.5\n",
    "f2_source = np.random.rand(100, 500) * 12 + 3.5\n",
    "\n",
    "# Targets\n",
    "reg_target_1 = np.random.rand(100,)  # 1D array for single-output regression\n",
    "reg_target_2_first = np.random.rand(100,)  # First output of multi-output regression\n",
    "reg_target_2_second = np.random.rand(100,)  # Second output of multi-output regression\n",
    "cla_target_2_first = np.random.randint(0, 5, size=(100,))  # First output of multi-output classification\n",
    "cla_target_2_second = np.random.randint(0, 5, size=(100,))  # Second output of multi-output classification\n",
    "bin_target_1 = np.random.randint(0, 2, size=(100,))  # 1D array for binary classification\n",
    "\n",
    "# Add data to datasets\n",
    "dataset_reg_1_1.add_data([f1_source], reg_target_1)\n",
    "dataset_reg_2_1.add_data([f1_source, f2_source], reg_target_2_first)\n",
    "dataset_reg_2_2.add_data([f1_source, f2_source], reg_target_2_second)\n",
    "dataset_cla_2_1.add_data([f1_source, f2_source], cla_target_2_first)\n",
    "dataset_cla_2_2.add_data([f1_source, f2_source], cla_target_2_second)\n",
    "dataset_bin_1_1.add_data([f1_source], bin_target_1)\n",
    "pass\n",
    "# print(\"Dataset for regression 1-1:\", dataset_reg_1_1)\n",
    "# print(\"Dataset for regression 2-1 (first output):\", dataset_reg_2_1)\n",
    "# print(\"Dataset for regression 2-2 (second output):\", dataset_reg_2_2)\n",
    "# print(\"Dataset for classification 2-1 (first output):\", dataset_cla_2_1)\n",
    "# print(\"Dataset for classification 2-2 (second output):\", dataset_cla_2_2)\n",
    "# print(\"Dataset for binary classification 1-1:\", dataset_bin_1_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecc4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PipelineConfig' object has no attribute 'display_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# steps = [ MinMaxScaler(feature_range=(0.2,0.8)) ]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m config \u001b[38;5;241m=\u001b[39m PipelineConfig(steps)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_steps\u001b[49m())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# config.save(\"pipeline_config.json\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m config\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelineConfig' object has no attribute 'display_steps'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nirs4all.core.PipelineConfig import PipelineConfig\n",
    "import json\n",
    "from sample import config as steps\n",
    "\n",
    "# steps = [ MinMaxScaler(feature_range=(0.2,0.8)) ]\n",
    "config = PipelineConfig(steps)\n",
    "print(config)\n",
    "# config.save(\"pipeline_config.json\")\n",
    "config.save(\"pipeline_config.yaml\")\n",
    "# print(config)\n",
    "# print(json.dumps(config.steps_wo_runtime_instances(), indent=2))\n",
    "\n",
    "# runner = PipelineRunner(max_workers=4, continue_on_error=False)\n",
    "# dataset_res_json, fitted_json, history_json, tree_json = runner.run(config, dataset_reg_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "770e2f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "Type mismatch: <class 'int'> != <class 'NoneType'>\n",
      "{'dataset': {'type': 'classification', 'folder': './sample_data'}, 'pipeline': [MinMaxScaler(feature_range=(0.2, 0.8)), {'feature_augmentation': [None, <class 'nirs4all.transformations._nirs.SavitzkyGolay'>, [<class 'sklearn.preprocessing._data.StandardScaler'>, <class 'nirs4all.transformations._standard.Gaussian'>]]}, {'sample_augmentation': [<class 'nirs4all.transformations._random_augmentation.Rotate_Translate'>, Rotate_Translate(p_range=3)]}, 'sklearn.model_selection._split.ShuffleSplit', {'cluster': KMeans(n_clusters=5, random_state=42)}, RepeatedStratifiedKFold(n_repeats=2, n_splits=5, random_state=42), 'uncluster', {'dispatch': [['sklearn.preprocessing._data.MinMaxScaler', {'feature_augmentation': [None, <class 'nirs4all.transformations._nirs.SavitzkyGolay'>, [<class 'sklearn.preprocessing._data.StandardScaler'>, <class 'nirs4all.transformations._standard.Gaussian'>]]}, {'model': RandomForestClassifier(max_depth=10, random_state=42), 'y_pipeline': <class 'sklearn.preprocessing._data.StandardScaler'>}], {'model': <function decon at 0x000001FCDAB789D0>, 'y_pipeline': 'sklearn.preprocessing._data.StandardScaler'}, {'model': SVC(kernel='linear', random_state=42), 'y_pipeline': [<class 'sklearn.preprocessing._data.MinMaxScaler'>, RobustScaler(with_centering=False)], 'finetune_params': {'C': [0.1, 1.0, 10.0]}}, {'stack': {'model': RandomForestClassifier(max_depth=10, random_state=42), 'y_pipeline': 'sklearn.preprocessing._data.StandardScaler', 'base_learners': [{'model': GradientBoostingClassifier(max_depth=5, random_state=42), 'y_pipeline': 'sklearn.preprocessing._data.MinMaxScaler'}, {'model': DecisionTreeClassifier(max_depth=5, random_state=42), 'y_pipeline': 'sklearn.preprocessing._data.MinMaxScaler', 'finetune_params': {'max_depth': [3, 5, 7]}}]}}]}, 'PlotConfusionMatrix']}\n",
      "Unserialized config (pretty): {\n",
      "    dataset: {\n",
      "    type: classification,\n",
      "    folder: ./sample_data,\n",
      "},\n",
      ",\n",
      "    pipeline: [\n",
      "    MinMaxScaler(feature_range=(0.2, 0.8))\n",
      "    {\n",
      "        feature_augmentation: [\n",
      "    None\n",
      "    <class 'nirs4all.transformations._nirs.SavitzkyGolay'>\n",
      "    [\n",
      "        <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "        <class 'nirs4all.transformations._standard.Gaussian'>\n",
      "    ],\n",
      "],,\n",
      "    },\n",
      "\n",
      "    {\n",
      "        sample_augmentation: [\n",
      "    <class 'nirs4all.transformations._random_augmentation.Rotate_Translate'>\n",
      "    Rotate_Translate(p_range=3)\n",
      "],,\n",
      "    },\n",
      "\n",
      "    sklearn.model_selection._split.ShuffleSplit\n",
      "    {\n",
      "        cluster: KMeans(n_clusters=5, random_state=42),\n",
      "    },\n",
      "\n",
      "    RepeatedStratifiedKFold(n_repeats=2, n_splits=5, random_state=42)\n",
      "    uncluster\n",
      "    {\n",
      "        dispatch: [\n",
      "    [\n",
      "        sklearn.preprocessing._data.MinMaxScaler\n",
      "        {\n",
      "            feature_augmentation: [\n",
      "    None\n",
      "    <class 'nirs4all.transformations._nirs.SavitzkyGolay'>\n",
      "    [\n",
      "        <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "        <class 'nirs4all.transformations._standard.Gaussian'>\n",
      "    ],\n",
      "],,\n",
      "        },\n",
      "\n",
      "        {\n",
      "            model: RandomForestClassifier(max_depth=10, random_state=42),\n",
      "            y_pipeline: <class 'sklearn.preprocessing._data.StandardScaler'>,\n",
      "        },\n",
      "\n",
      "    ],\n",
      "    {\n",
      "        model: <function decon at 0x000001FCDAB789D0>,\n",
      "        y_pipeline: sklearn.preprocessing._data.StandardScaler,\n",
      "    },\n",
      "\n",
      "    {\n",
      "        model: SVC(kernel='linear', random_state=42),\n",
      "        y_pipeline: [\n",
      "    <class 'sklearn.preprocessing._data.MinMaxScaler'>\n",
      "    RobustScaler(with_centering=False)\n",
      "],,\n",
      "        finetune_params: {\n",
      "    C: [\n",
      "    0.1\n",
      "    1.0\n",
      "    10.0\n",
      "],,\n",
      "},\n",
      ",\n",
      "    },\n",
      "\n",
      "    {\n",
      "        stack: {\n",
      "    model: RandomForestClassifier(max_depth=10, random_state=42),\n",
      "    y_pipeline: sklearn.preprocessing._data.StandardScaler,\n",
      "    base_learners: [\n",
      "    {\n",
      "        model: GradientBoostingClassifier(max_depth=5, random_state=42),\n",
      "        y_pipeline: sklearn.preprocessing._data.MinMaxScaler,\n",
      "    },\n",
      "\n",
      "    {\n",
      "        model: DecisionTreeClassifier(max_depth=5, random_state=42),\n",
      "        y_pipeline: sklearn.preprocessing._data.MinMaxScaler,\n",
      "        finetune_params: {\n",
      "    max_depth: [\n",
      "    3\n",
      "    5\n",
      "    7\n",
      "],,\n",
      "},\n",
      ",\n",
      "    },\n",
      "\n",
      "],,\n",
      "},\n",
      ",\n",
      "    },\n",
      "\n",
      "],,\n",
      "    },\n",
      "\n",
      "    PlotConfusionMatrix\n",
      "],,\n",
      "},\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sample import config\n",
    "from nirs4all.utils.serialization import serialize_component, deserialize_component\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# steps = [ MinMaxScaler(feature_range=(0.2,0.8)) ]\n",
    "\n",
    "serialized_config = serialize_component(config)\n",
    "# print(\"Serialized config (pretty):\", json.dumps(serialized_config, indent=4))\n",
    "\n",
    "unserialized_config = deserialize_component(serialized_config)\n",
    "\n",
    "def pretty_dict(d, txt=\"\", indent=0):\n",
    "    if isinstance(d, dict):\n",
    "        txt += \" \" * indent + \"{\\n\"\n",
    "        indent += 4\n",
    "        for key, value in d.items():\n",
    "            txt += \" \" * indent + f\"{key}: {pretty_dict(value, '', 0)},\\n\"\n",
    "        txt += \" \" * (indent - 4) + \"},\\n\"\n",
    "    elif isinstance(d, list):\n",
    "        txt += \" \" * indent + \"[\\n\"\n",
    "        indent += 4\n",
    "        for item in d:\n",
    "            txt += pretty_dict(item, '', indent) + \"\\n\"\n",
    "        txt += \" \" * (indent - 4) + \"],\"\n",
    "    else:\n",
    "        txt += \" \" * indent + str(d)\n",
    "    return txt\n",
    "\n",
    "print(unserialized_config)\n",
    "print(\"Unserialized config (pretty):\", pretty_dict(unserialized_config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Enhanced SpectraDataset Implementation\n",
    "# Improved modular architecture with origin field support, complex queries, and robust source management\n",
    "# \"\"\"\n",
    "\n",
    "# import numpy as np\n",
    "# import polars as pl\n",
    "# from typing import Dict, List, Optional, Union, Any, Tuple, Callable\n",
    "# from dataclasses import dataclass, field\n",
    "# from abc import ABC, abstractmethod\n",
    "# from datetime import datetime\n",
    "# import uuid\n",
    "# import warnings\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class SampleMetadata:\n",
    "#     \"\"\"Enhanced sample metadata with origin tracking\"\"\"\n",
    "#     sample_id: int\n",
    "#     partition: str = \"train\"\n",
    "#     group: int = 0\n",
    "#     branch: int = 0\n",
    "#     processing: str = \"raw\"\n",
    "#     origin: Optional[int] = None  # Track original sample relationship\n",
    "#     augmentation_type: str = \"original\"\n",
    "#     created_at: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         # If origin not specified, use sample_id (self-origin for original samples)\n",
    "#         if self.origin is None:\n",
    "#             self.origin = self.sample_id\n",
    "\n",
    "\n",
    "# class Query:\n",
    "#     \"\"\"Enhanced query builder with complex filtering support\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.filters = {}\n",
    "\n",
    "#     def to_polars_expr(self) -> pl.Expr:\n",
    "#         \"\"\"Convert simple filters to Polars expression\"\"\"\n",
    "#         expr = pl.lit(True)\n",
    "\n",
    "#         for field, value in self.filters.items():\n",
    "#             if isinstance(value, list):\n",
    "#                 expr = expr & pl.col(field).is_in(value)\n",
    "#             else:\n",
    "#                 expr = expr & (pl.col(field) == value)\n",
    "\n",
    "#         return expr\n",
    "\n",
    "#     def apply_to_dataframe(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Apply all filters (simple + custom) to dataframe\"\"\"\n",
    "#         # Apply simple filters first\n",
    "#         result = df.filter(self.to_polars_expr())\n",
    "\n",
    "#         return result\n",
    "\n",
    "\n",
    "# class FeatureSource:\n",
    "#     \"\"\"Enhanced feature source with validation and metadata\"\"\"\n",
    "\n",
    "#     def __init__(self, name: str, data: np.ndarray, sample_ids: List[int], metadata: Dict[str, Any] = None):\n",
    "#         self.name = name\n",
    "#         self.data = data.astype(np.float64)  # Ensure consistent dtype\n",
    "#         self.sample_ids = np.array(sample_ids)\n",
    "#         self.metadata = metadata or {}\n",
    "#         self._id_to_idx = None  # Lazy-loaded mapping\n",
    "#         self._validate()\n",
    "\n",
    "#     def _validate(self):\n",
    "#         \"\"\"Validate feature source consistency\"\"\"\n",
    "#         if len(self.data) != len(self.sample_ids):\n",
    "#             raise ValueError(f\"Source '{self.name}': Data length {len(self.data)} != sample_ids length {len(self.sample_ids)}\")\n",
    "\n",
    "#         if len(np.unique(self.sample_ids)) != len(self.sample_ids):\n",
    "#             raise ValueError(f\"Source '{self.name}': Duplicate sample IDs found\")\n",
    "\n",
    "#     def _build_id_mapping(self):\n",
    "#         \"\"\"Build sample_id -> row_index mapping\"\"\"\n",
    "#         if self._id_to_idx is None:\n",
    "#             self._id_to_idx = {sid: idx for idx, sid in enumerate(self.sample_ids)}\n",
    "\n",
    "#     def get_samples(self, sample_ids: List[int]) -> np.ndarray:\n",
    "#         \"\"\"Get features for specific sample IDs with optimized lookup\"\"\"\n",
    "#         self._build_id_mapping()\n",
    "\n",
    "#         # Find row indices for requested sample IDs\n",
    "#         row_indices = []\n",
    "#         missing_ids = []\n",
    "\n",
    "#         for sid in sample_ids:\n",
    "#             if sid in self._id_to_idx:\n",
    "#                 row_indices.append(self._id_to_idx[sid])\n",
    "#             else:\n",
    "#                 missing_ids.append(sid)\n",
    "\n",
    "#         if missing_ids:\n",
    "#             raise ValueError(f\"Source '{self.name}': Missing sample IDs {missing_ids}\")\n",
    "\n",
    "#         return self.data[row_indices]\n",
    "\n",
    "#     def has_samples(self, sample_ids: List[int]) -> bool:\n",
    "#         \"\"\"Check if all sample IDs exist in this source\"\"\"\n",
    "#         self._build_id_mapping()\n",
    "#         return all(sid in self._id_to_idx for sid in sample_ids)\n",
    "\n",
    "#     def get_compatible_samples(self, sample_ids: List[int]) -> Tuple[List[int], np.ndarray]:\n",
    "#         \"\"\"Get samples that exist in this source, return (found_ids, features)\"\"\"\n",
    "#         self._build_id_mapping()\n",
    "\n",
    "#         found_ids = [sid for sid in sample_ids if sid in self._id_to_idx]\n",
    "#         if not found_ids:\n",
    "#             return [], np.array([]).reshape(0, self.n_features)\n",
    "\n",
    "#         row_indices = [self._id_to_idx[sid] for sid in found_ids]\n",
    "#         return found_ids, self.data[row_indices]\n",
    "\n",
    "#     @property\n",
    "#     def shape(self) -> tuple:\n",
    "#         return self.data.shape\n",
    "\n",
    "#     @property\n",
    "#     def n_features(self) -> int:\n",
    "#         return self.data.shape[1] if self.data.ndim > 1 else 1\n",
    "\n",
    "#     @property\n",
    "#     def n_samples(self) -> int:\n",
    "#         return len(self.sample_ids)\n",
    "\n",
    "#     def get_info(self) -> Dict[str, Any]:\n",
    "#         \"\"\"Get comprehensive source information\"\"\"\n",
    "#         return {\n",
    "#             'name': self.name,\n",
    "#             'shape': self.shape,\n",
    "#             'n_features': self.n_features,\n",
    "#             'n_samples': self.n_samples,\n",
    "#             'dtype': str(self.data.dtype),\n",
    "#             'memory_mb': self.data.nbytes / (1024 * 1024),\n",
    "#             'sample_id_range': (int(self.sample_ids.min()), int(self.sample_ids.max())),\n",
    "#             'metadata': self.metadata\n",
    "#         }\n",
    "\n",
    "\n",
    "# class FeatureStore:\n",
    "#     \"\"\"Enhanced feature storage with robust multi-source management\"\"\"\n",
    "\n",
    "#     def __init__(self, max_workers: int = 4):\n",
    "#         self.sources: Dict[str, FeatureSource] = {}\n",
    "#         self.max_workers = max_workers\n",
    "\n",
    "#     def add_source(self, name: str, data: np.ndarray, sample_ids: List[int],\n",
    "#                    metadata: Dict[str, Any] = None, replace: bool = False) -> 'FeatureStore':\n",
    "#         \"\"\"Add a named feature source with enhanced options\"\"\"\n",
    "#         if name in self.sources and not replace:\n",
    "#             raise ValueError(f\"Source '{name}' already exists. Use replace=True to overwrite.\")\n",
    "\n",
    "#         self.sources[name] = FeatureSource(name, data, sample_ids, metadata)\n",
    "#         return self\n",
    "\n",
    "#     def remove_source(self, name: str) -> 'FeatureStore':\n",
    "#         \"\"\"Remove a feature source\"\"\"\n",
    "#         if name not in self.sources:\n",
    "#             raise ValueError(f\"Source '{name}' not found\")\n",
    "#         del self.sources[name]\n",
    "#         return self\n",
    "\n",
    "#     def get_features(self, sample_ids: List[int],\n",
    "#                     source_names: Optional[List[str]] = None,\n",
    "#                     strict: bool = True,\n",
    "#                     parallel: bool = False) -> Dict[str, np.ndarray]:\n",
    "#         \"\"\"Get features from specified sources with enhanced options\"\"\"\n",
    "#         if not sample_ids:\n",
    "#             return {}\n",
    "\n",
    "#         source_names = source_names or list(self.sources.keys())\n",
    "\n",
    "#         if not source_names:\n",
    "#             return {}\n",
    "\n",
    "#         # Validate sources exist\n",
    "#         missing_sources = [name for name in source_names if name not in self.sources]\n",
    "#         if missing_sources:\n",
    "#             raise ValueError(f\"Unknown sources: {missing_sources}\")\n",
    "\n",
    "#         if parallel and len(source_names) > 1:\n",
    "#             return self._get_features_parallel(sample_ids, source_names, strict)\n",
    "#         else:\n",
    "#             return self._get_features_sequential(sample_ids, source_names, strict)\n",
    "\n",
    "#     def _get_features_sequential(self, sample_ids: List[int],\n",
    "#                                source_names: List[str], strict: bool) -> Dict[str, np.ndarray]:\n",
    "#         \"\"\"Get features sequentially\"\"\"\n",
    "#         result = {}\n",
    "\n",
    "#         for source_name in source_names:\n",
    "#             source = self.sources[source_name]\n",
    "\n",
    "#             if strict:\n",
    "#                 result[source_name] = source.get_samples(sample_ids)\n",
    "#             else:\n",
    "#                 # Non-strict: get compatible samples only\n",
    "#                 compatible_ids, features = source.get_compatible_samples(sample_ids)\n",
    "#                 if len(compatible_ids) > 0:\n",
    "#                     result[source_name] = features\n",
    "#                 else:\n",
    "#                     warnings.warn(f\"No compatible samples found in source '{source_name}'\")\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     def _get_features_parallel(self, sample_ids: List[int],\n",
    "#                              source_names: List[str], strict: bool) -> Dict[str, np.ndarray]:\n",
    "#         \"\"\"Get features in parallel for better performance\"\"\"\n",
    "#         result = {}\n",
    "\n",
    "#         def get_source_features(source_name):\n",
    "#             source = self.sources[source_name]\n",
    "#             if strict:\n",
    "#                 return source_name, source.get_samples(sample_ids)\n",
    "#             else:\n",
    "#                 compatible_ids, features = source.get_compatible_samples(sample_ids)\n",
    "#                 return source_name, features if len(compatible_ids) > 0 else None\n",
    "\n",
    "#         with ThreadPoolExecutor(max_workers=min(self.max_workers, len(source_names))) as executor:\n",
    "#             future_to_source = {executor.submit(get_source_features, name): name\n",
    "#                               for name in source_names}\n",
    "\n",
    "#             for future in as_completed(future_to_source):\n",
    "#                 source_name = future_to_source[future]\n",
    "#                 try:\n",
    "#                     name, features = future.result()\n",
    "#                     if features is not None:\n",
    "#                         result[name] = features\n",
    "#                     elif not strict:\n",
    "#                         warnings.warn(f\"No compatible samples found in source '{source_name}'\")\n",
    "#                 except Exception as exc:\n",
    "#                     if strict:\n",
    "#                         raise exc\n",
    "#                     else:\n",
    "#                         warnings.warn(f\"Error getting features from source '{source_name}': {exc}\")\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     def concatenate_sources(self, sample_ids: List[int],\n",
    "#                           source_names: Optional[List[str]] = None,\n",
    "#                           strict: bool = True,\n",
    "#                           parallel: bool = False) -> np.ndarray:\n",
    "#         \"\"\"Concatenate features from multiple sources with enhanced options\"\"\"\n",
    "#         features_dict = self.get_features(sample_ids, source_names, strict, parallel)\n",
    "\n",
    "#         if not features_dict:\n",
    "#             return np.array([]).reshape(len(sample_ids), 0)\n",
    "\n",
    "#         # Ensure consistent ordering\n",
    "#         source_names = source_names or sorted(features_dict.keys())\n",
    "#         valid_sources = [name for name in source_names if name in features_dict]\n",
    "\n",
    "#         if not valid_sources:\n",
    "#             return np.array([]).reshape(len(sample_ids), 0)\n",
    "\n",
    "#         return np.concatenate([features_dict[name] for name in valid_sources], axis=1)\n",
    "\n",
    "#     def get_source_compatibility(self, sample_ids: List[int]) -> Dict[str, Dict[str, Any]]:\n",
    "#         \"\"\"Check which sources can provide features for given sample IDs\"\"\"\n",
    "#         result = {}\n",
    "\n",
    "#         for name, source in self.sources.items():\n",
    "#             compatible_ids, _ = source.get_compatible_samples(sample_ids)\n",
    "#             result[name] = {\n",
    "#                 'available_samples': len(compatible_ids),\n",
    "#                 'total_requested': len(sample_ids),\n",
    "#                 'coverage': len(compatible_ids) / len(sample_ids) if sample_ids else 0,\n",
    "#                 'compatible_ids': compatible_ids,\n",
    "#                 'missing_ids': [sid for sid in sample_ids if sid not in compatible_ids]\n",
    "#             }\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     @property\n",
    "#     def source_names(self) -> List[str]:\n",
    "#         return list(self.sources.keys())\n",
    "\n",
    "#     def get_source_info(self) -> Dict[str, Dict]:\n",
    "#         \"\"\"Get comprehensive information about all sources\"\"\"\n",
    "#         return {name: source.get_info() for name, source in self.sources.items()}\n",
    "\n",
    "#     def get_summary(self) -> Dict[str, Any]:\n",
    "#         \"\"\"Get summary statistics of the feature store\"\"\"\n",
    "#         if not self.sources:\n",
    "#             return {'n_sources': 0, 'total_features': 0, 'total_memory_mb': 0}\n",
    "\n",
    "#         total_features = sum(source.n_features for source in self.sources.values())\n",
    "#         total_memory = sum(source.data.nbytes for source in self.sources.values()) / (1024 * 1024)\n",
    "\n",
    "#         return {\n",
    "#             'n_sources': len(self.sources),\n",
    "#             'source_names': list(self.sources.keys()),\n",
    "#             'total_features': total_features,\n",
    "#             'total_memory_mb': total_memory,\n",
    "#             'feature_distribution': {name: source.n_features for name, source in self.sources.items()},\n",
    "#             'sample_coverage': {name: source.n_samples for name, source in self.sources.items()}\n",
    "#         }\n",
    "\n",
    "\n",
    "# class TargetManager:\n",
    "#     \"\"\"Enhanced target management with better type handling\"\"\"\n",
    "\n",
    "#     def __init__(self, task_type: str = \"auto\"):\n",
    "#         self.targets: Dict[int, Any] = {}\n",
    "#         self.task_type: str = task_type\n",
    "#         self.encoders: Dict[str, Any] = {}\n",
    "#         self._classes = None\n",
    "#         self._target_stats = None\n",
    "\n",
    "#     def add_targets(self, sample_ids: List[int], targets: List[Any]) -> 'TargetManager':\n",
    "#         \"\"\"Add targets with validation\"\"\"\n",
    "#         if len(sample_ids) != len(targets):\n",
    "#             raise ValueError(\"sample_ids and targets must have same length\")\n",
    "\n",
    "#         for sid, target in zip(sample_ids, targets):\n",
    "#             self.targets[sid] = target\n",
    "\n",
    "#         # Auto-detect task type if needed\n",
    "#         if self.task_type == \"auto\" and len(self.targets) > 0:\n",
    "#             self._detect_task_type(list(self.targets.values()))\n",
    "\n",
    "#         # Invalidate cached stats\n",
    "#         self._target_stats = None\n",
    "#         return self\n",
    "\n",
    "#     def get_targets(self, sample_ids: List[int], encoded: bool = False,\n",
    "#                    strict: bool = True) -> Union[np.ndarray, Tuple[List[int], np.ndarray]]:\n",
    "#         \"\"\"Get targets for specific sample IDs with enhanced options\"\"\"\n",
    "#         if strict:\n",
    "#             # Strict mode: all sample IDs must have targets\n",
    "#             missing = [sid for sid in sample_ids if sid not in self.targets]\n",
    "#             if missing:\n",
    "#                 raise ValueError(f\"Missing targets for sample IDs: {missing}\")\n",
    "\n",
    "#             raw_targets = [self.targets[sid] for sid in sample_ids]\n",
    "\n",
    "#             if encoded:\n",
    "#                 return self._encode_targets(raw_targets)\n",
    "#             else:\n",
    "#                 return np.array(raw_targets)\n",
    "#         else:\n",
    "#             # Non-strict mode: return only available targets\n",
    "#             available_ids = [sid for sid in sample_ids if sid in self.targets]\n",
    "#             if not available_ids:\n",
    "#                 return ([], np.array([]))\n",
    "\n",
    "#             raw_targets = [self.targets[sid] for sid in available_ids]\n",
    "\n",
    "#             if encoded:\n",
    "#                 encoded_targets = self._encode_targets(raw_targets)\n",
    "#                 return (available_ids, encoded_targets)\n",
    "#             else:\n",
    "#                 return (available_ids, np.array(raw_targets))\n",
    "\n",
    "#     def has_targets(self, sample_ids: List[int]) -> bool:\n",
    "#         \"\"\"Check if all sample IDs have targets\"\"\"\n",
    "#         return all(sid in self.targets for sid in sample_ids)\n",
    "\n",
    "#     def _detect_task_type(self, targets: List[Any]):\n",
    "#         \"\"\"Enhanced task type detection\"\"\"\n",
    "#         unique_targets = set(targets)\n",
    "#         n_unique = len(unique_targets)\n",
    "\n",
    "#         try:\n",
    "#             # Try to convert to numeric\n",
    "#             numeric_targets = [float(t) for t in targets]\n",
    "#             int_targets = [int(t) for t in numeric_targets]\n",
    "\n",
    "#             # Check if all are integers\n",
    "#             if all(nt == it for nt, it in zip(numeric_targets, int_targets)):\n",
    "#                 # Integer targets\n",
    "#                 if n_unique == 2:\n",
    "#                     self.task_type = \"binary\"\n",
    "#                 elif n_unique <= 20:  # Threshold for classification\n",
    "#                     self.task_type = \"classification\"\n",
    "#                 else:\n",
    "#                     self.task_type = \"regression\"\n",
    "#             else:\n",
    "#                 # Continuous targets\n",
    "#                 self.task_type = \"regression\"\n",
    "\n",
    "#         except (ValueError, TypeError):\n",
    "#             # Non-numeric targets\n",
    "#             if n_unique == 2:\n",
    "#                 self.task_type = \"binary\"\n",
    "#             else:\n",
    "#                 self.task_type = \"classification\"\n",
    "\n",
    "#     def _encode_targets(self, targets: List[Any]) -> np.ndarray:\n",
    "#         \"\"\"Encode targets for ML with caching\"\"\"\n",
    "#         if self.task_type == \"regression\":\n",
    "#             return np.array([float(t) for t in targets])\n",
    "#         else:\n",
    "#             # Classification: use label encoding\n",
    "#             if 'label_encoder' not in self.encoders:\n",
    "#                 from sklearn.preprocessing import LabelEncoder\n",
    "#                 self.encoders['label_encoder'] = LabelEncoder()\n",
    "#                 all_targets = list(self.targets.values())\n",
    "#                 self.encoders['label_encoder'].fit(all_targets)\n",
    "#                 self._classes = self.encoders['label_encoder'].classes_\n",
    "\n",
    "#             return self.encoders['label_encoder'].transform(targets)\n",
    "\n",
    "#     def get_target_stats(self) -> Dict[str, Any]:\n",
    "#         \"\"\"Get comprehensive target statistics\"\"\"\n",
    "#         if self._target_stats is None:\n",
    "#             self._compute_target_stats()\n",
    "#         return self._target_stats\n",
    "\n",
    "#     def _compute_target_stats(self):\n",
    "#         \"\"\"Compute and cache target statistics\"\"\"\n",
    "#         if not self.targets:\n",
    "#             self._target_stats = {}\n",
    "#             return\n",
    "\n",
    "#         all_targets = list(self.targets.values())\n",
    "\n",
    "#         self._target_stats = {\n",
    "#             'task_type': self.task_type,\n",
    "#             'n_samples': len(all_targets),\n",
    "#             'n_unique': len(set(all_targets))\n",
    "#         }\n",
    "\n",
    "#         if self.task_type == \"regression\":\n",
    "#             numeric_targets = [float(t) for t in all_targets]\n",
    "#             self._target_stats.update({\n",
    "#                 'mean': np.mean(numeric_targets),\n",
    "#                 'std': np.std(numeric_targets),\n",
    "#                 'min': np.min(numeric_targets),\n",
    "#                 'max': np.max(numeric_targets)\n",
    "#             })\n",
    "#         else:\n",
    "#             # Classification\n",
    "#             from collections import Counter\n",
    "#             target_counts = Counter(all_targets)\n",
    "#             self._target_stats.update({\n",
    "#                 'classes': list(target_counts.keys()),\n",
    "#                 'n_classes': len(target_counts),\n",
    "#                 'class_distribution': dict(target_counts),\n",
    "#                 'is_balanced': max(target_counts.values()) / min(target_counts.values()) <= 2.0\n",
    "#             })\n",
    "\n",
    "#     @property\n",
    "#     def classes_(self) -> Optional[np.ndarray]:\n",
    "#         return self._classes\n",
    "\n",
    "#     @property\n",
    "#     def n_classes(self) -> int:\n",
    "#         return len(self._classes) if self._classes is not None else 0\n",
    "\n",
    "#     def get_info(self) -> Dict[str, Any]:\n",
    "#         \"\"\"Get basic target information\"\"\"\n",
    "#         return {\n",
    "#             'task_type': self.task_type,\n",
    "#             'n_samples': len(self.targets),\n",
    "#             'n_classes': self.n_classes,\n",
    "#             'classes': self._classes.tolist() if self._classes is not None else None\n",
    "#         }\n",
    "\n",
    "\n",
    "# class ResultsManager:\n",
    "#     \"\"\"Enhanced results management with better organization\"\"\"\n",
    "\n",
    "#     RESULTS_SCHEMA = {\n",
    "#         \"experiment_id\": pl.Utf8,\n",
    "#         \"sample_id\": pl.Int64,\n",
    "#         \"prediction\": pl.Float64,\n",
    "#         \"model_name\": pl.Utf8,\n",
    "#         \"fold\": pl.Int64,\n",
    "#         \"branch\": pl.Int64,\n",
    "#         \"timestamp\": pl.Datetime,\n",
    "#         \"metadata\": pl.Utf8,\n",
    "#     }\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.predictions = pl.DataFrame(schema=self.RESULTS_SCHEMA)\n",
    "#         self.experiments: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "#     def add_experiment(self, experiment_id: str, metadata: Dict[str, Any]) -> 'ResultsManager':\n",
    "#         \"\"\"Register a new experiment with metadata\"\"\"\n",
    "#         self.experiments[experiment_id] = {\n",
    "#             **metadata,\n",
    "#             'created_at': datetime.now(),\n",
    "#             'n_predictions': 0\n",
    "#         }\n",
    "#         return self\n",
    "\n",
    "#     def add_predictions(self,\n",
    "#                        experiment_id: str,\n",
    "#                        sample_ids: List[int],\n",
    "#                        predictions: np.ndarray,\n",
    "#                        model_name: str = \"default\",\n",
    "#                        fold: int = -1,\n",
    "#                        branch: int = 0,\n",
    "#                        metadata: Optional[Dict] = None) -> 'ResultsManager':\n",
    "#         \"\"\"Add predictions with enhanced tracking\"\"\"\n",
    "\n",
    "#         if experiment_id not in self.experiments:\n",
    "#             self.add_experiment(experiment_id, {})\n",
    "\n",
    "#         import json\n",
    "#         metadata_str = json.dumps(metadata or {})\n",
    "\n",
    "#         new_predictions = pl.DataFrame({\n",
    "#             \"experiment_id\": [experiment_id] * len(sample_ids),\n",
    "#             \"sample_id\": sample_ids,\n",
    "#             \"prediction\": predictions.flatten(),\n",
    "#             \"model_name\": [model_name] * len(sample_ids),\n",
    "#             \"fold\": [fold] * len(sample_ids),\n",
    "#             \"branch\": [branch] * len(sample_ids),\n",
    "#             \"timestamp\": [datetime.now()] * len(sample_ids),\n",
    "#             \"metadata\": [metadata_str] * len(sample_ids),\n",
    "#         })\n",
    "\n",
    "#         self.predictions = pl.concat([self.predictions, new_predictions])\n",
    "\n",
    "#         # Update experiment stats\n",
    "#         self.experiments[experiment_id]['n_predictions'] += len(sample_ids)\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def get_predictions(self, experiment_id: str,\n",
    "#                        sample_ids: Optional[List[int]] = None,\n",
    "#                        model_name: Optional[str] = None,\n",
    "#                        fold: Optional[int] = None) -> Dict[str, np.ndarray]:\n",
    "#         \"\"\"Get predictions with enhanced filtering\"\"\"\n",
    "#         filtered = self.predictions.filter(pl.col(\"experiment_id\") == experiment_id)\n",
    "\n",
    "#         if sample_ids is not None:\n",
    "#             filtered = filtered.filter(pl.col(\"sample_id\").is_in(sample_ids))\n",
    "\n",
    "#         if model_name is not None:\n",
    "#             filtered = filtered.filter(pl.col(\"model_name\") == model_name)\n",
    "\n",
    "#         if fold is not None:\n",
    "#             filtered = filtered.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "#         return {\n",
    "#             \"sample_ids\": filtered[\"sample_id\"].to_numpy(),\n",
    "#             \"predictions\": filtered[\"prediction\"].to_numpy(),\n",
    "#             \"model_names\": filtered[\"model_name\"].to_numpy(),\n",
    "#             \"folds\": filtered[\"fold\"].to_numpy(),\n",
    "#             \"branches\": filtered[\"branch\"].to_numpy(),\n",
    "#         }\n",
    "\n",
    "#     def get_experiment_summary(self) -> Dict[str, Any]:\n",
    "#         \"\"\"Get summary of all experiments\"\"\"\n",
    "#         return {\n",
    "#             'n_experiments': len(self.experiments),\n",
    "#             'experiments': self.experiments,\n",
    "#             'total_predictions': len(self.predictions),\n",
    "#             'unique_models': self.predictions[\"model_name\"].unique().to_list(),\n",
    "#             'unique_folds': sorted(self.predictions[\"fold\"].unique().to_list()),\n",
    "#         }\n",
    "\n",
    "\n",
    "# class SpectraDataset:\n",
    "#     \"\"\"Enhanced modular spectral dataset with origin support and complex queries\"\"\"\n",
    "\n",
    "#     def __init__(self, max_workers: int = 4):\n",
    "#         self.features = FeatureStore(max_workers=max_workers)\n",
    "#         self.targets = TargetManager()\n",
    "#         self.results = ResultsManager()\n",
    "\n",
    "#         # Enhanced sample metadata table with origin support\n",
    "#         self.samples = pl.DataFrame({\n",
    "#             \"sample_id\": pl.Series([], dtype=pl.Int64),\n",
    "#             \"partition\": pl.Series([], dtype=pl.Utf8),\n",
    "#             \"group\": pl.Series([], dtype=pl.Int64),\n",
    "#             \"branch\": pl.Series([], dtype=pl.Int64),\n",
    "#             \"processing\": pl.Series([], dtype=pl.Utf8),\n",
    "#             \"origin\": pl.Series([], dtype=pl.Int64),  # Key addition: origin tracking\n",
    "#             \"augmentation_type\": pl.Series([], dtype=pl.Utf8),\n",
    "#             \"created_at\": pl.Series([], dtype=pl.Datetime),\n",
    "#         })\n",
    "\n",
    "#         self._next_sample_id = 0\n",
    "\n",
    "#     def add_data(self,\n",
    "#                  features: Union[np.ndarray, Dict[str, np.ndarray]],\n",
    "#                  targets: Optional[List[Any]] = None,\n",
    "#                  partition: str = \"train\",\n",
    "#                  group: int = 0,\n",
    "#                  branch: int = 0,\n",
    "#                  processing: str = \"raw\",\n",
    "#                  origin: Optional[Union[int, List[int]]] = None,\n",
    "#                  augmentation_type: str = \"original\",\n",
    "#                  source_metadata: Optional[Dict[str, Dict[str, Any]]] = None) -> List[int]:\n",
    "#         \"\"\"Add data with enhanced multi-source support and origin tracking\"\"\"\n",
    "\n",
    "#         # Handle single array input\n",
    "#         if isinstance(features, np.ndarray):\n",
    "#             features = {\"default\": features}\n",
    "\n",
    "#         # Validate all feature arrays have same number of samples\n",
    "#         n_samples_list = [len(feat) for feat in features.values()]\n",
    "#         if len(set(n_samples_list)) > 1:\n",
    "#             raise ValueError(f\"All feature arrays must have same number of samples: {n_samples_list}\")\n",
    "\n",
    "#         n_samples = n_samples_list[0]\n",
    "\n",
    "#         # Generate sample IDs\n",
    "#         sample_ids = list(range(self._next_sample_id, self._next_sample_id + n_samples))\n",
    "#         self._next_sample_id += n_samples\n",
    "\n",
    "#         # Handle origin assignment\n",
    "#         if origin is None:\n",
    "#             # Default: samples are their own origin (original samples)\n",
    "#             origins = sample_ids\n",
    "#         elif isinstance(origin, int):\n",
    "#             # All samples share same origin\n",
    "#             origins = [origin] * n_samples\n",
    "#         elif isinstance(origin, list):\n",
    "#             if len(origin) != n_samples:\n",
    "#                 raise ValueError(f\"Origin list length {len(origin)} != n_samples {n_samples}\")\n",
    "#             origins = origin\n",
    "#         else:\n",
    "#             raise ValueError(\"Origin must be None, int, or list of ints\")\n",
    "\n",
    "#         # Add features to store\n",
    "#         for source_name, source_data in features.items():\n",
    "#             metadata = source_metadata.get(source_name, {}) if source_metadata else {}\n",
    "#             self.features.add_source(source_name, source_data, sample_ids, metadata)\n",
    "\n",
    "#         # Add targets if provided\n",
    "#         if targets is not None:\n",
    "#             self.targets.add_targets(sample_ids, targets)\n",
    "\n",
    "#         # Add sample metadata\n",
    "#         new_samples = pl.DataFrame({\n",
    "#             \"sample_id\": sample_ids,\n",
    "#             \"partition\": [partition] * n_samples,\n",
    "#             \"group\": [group] * n_samples,\n",
    "#             \"branch\": [branch] * n_samples,\n",
    "#             \"processing\": [processing] * n_samples,\n",
    "#             \"origin\": origins,  # Track origin relationships\n",
    "#             \"augmentation_type\": [augmentation_type] * n_samples,\n",
    "#             \"created_at\": [datetime.now()] * n_samples,\n",
    "#         })\n",
    "\n",
    "#         self.samples = pl.concat([self.samples, new_samples])\n",
    "#         return sample_ids\n",
    "\n",
    "#     def query(self) -> Query:\n",
    "#         \"\"\"Create a new enhanced query builder\"\"\"\n",
    "#         return Query()\n",
    "\n",
    "#     def get_sample_ids(self, query: Optional[Query] = None) -> List[int]:\n",
    "#         \"\"\"Get sample IDs matching query with enhanced filtering\"\"\"\n",
    "#         if query is None:\n",
    "#             return self.samples[\"sample_id\"].to_list()\n",
    "\n",
    "#         filtered = query.apply_to_dataframe(self.samples)\n",
    "#         return filtered[\"sample_id\"].to_list()\n",
    "\n",
    "#     def get_sample_metadata(self, query: Optional[Query] = None) -> pl.DataFrame:\n",
    "#         \"\"\"Get sample metadata matching query\"\"\"\n",
    "#         if query is None:\n",
    "#             return self.samples.clone()\n",
    "\n",
    "#         return query.apply_to_dataframe(self.samples)\n",
    "\n",
    "#     def get_features(self,\n",
    "#                      query: Optional[Query] = None,\n",
    "#                      source_names: Optional[List[str]] = None,\n",
    "#                      concatenate: bool = True,\n",
    "#                      strict: bool = True,\n",
    "#                      parallel: bool = False,\n",
    "#                      auto_select_sources: bool = True) -> Union[np.ndarray, Dict[str, np.ndarray]]:\n",
    "#         \"\"\"Get features with enhanced options and error handling\"\"\"\n",
    "#         sample_ids = self.get_sample_ids(query)\n",
    "\n",
    "#         if not sample_ids:\n",
    "#             if concatenate:\n",
    "#                 return np.array([]).reshape(0, 0)\n",
    "#             else:\n",
    "#                 return {}            # Auto-select compatible sources if not specified\n",
    "#             if source_names is None and auto_select_sources:\n",
    "#                 # Find sources that have data for these sample IDs\n",
    "#                 compat_report = self.features.get_source_compatibility(sample_ids)\n",
    "#                 source_names = [name for name, info in compat_report.items()\n",
    "#                               if info['coverage'] > 0]\n",
    "\n",
    "#                 if not source_names:\n",
    "#                     import warnings\n",
    "#                     warnings.warn(\"No compatible sources found for the given sample IDs\")\n",
    "#                     if concatenate:\n",
    "#                         return np.array([]).reshape(len(sample_ids), 0)\n",
    "#                     else:\n",
    "#                         return {}\n",
    "\n",
    "#         if concatenate:\n",
    "#             return self.features.concatenate_sources(sample_ids, source_names, strict, parallel)\n",
    "#         else:\n",
    "#             return self.features.get_features(sample_ids, source_names, strict, parallel)\n",
    "\n",
    "#     def get_targets(self, query: Optional[Query] = None,\n",
    "#                    encoded: bool = True, strict: bool = True) -> Union[np.ndarray, Tuple[List[int], np.ndarray]]:\n",
    "#         \"\"\"Get targets with enhanced options\"\"\"\n",
    "#         sample_ids = self.get_sample_ids(query)\n",
    "\n",
    "#         if not sample_ids:\n",
    "#             if strict:\n",
    "#                 return np.array([])\n",
    "#             else:\n",
    "#                 return ([], np.array([]))\n",
    "\n",
    "#         return self.targets.get_targets(sample_ids, encoded=encoded, strict=strict)\n",
    "\n",
    "#     def augment_samples(self,\n",
    "#                        source_query: Query,\n",
    "#                        n_copies: int = 1,\n",
    "#                        augmentation_type: str = \"augmented\",\n",
    "#                        target_partition: Optional[str] = None,\n",
    "#                        augmentation_func: Optional[Callable[[np.ndarray], np.ndarray]] = None) -> List[int]:\n",
    "#         \"\"\"Create augmented copies with enhanced functionality\"\"\"\n",
    "#         source_sample_ids = self.get_sample_ids(source_query)\n",
    "\n",
    "#         if not source_sample_ids:\n",
    "#             return []\n",
    "\n",
    "#         new_sample_ids = []\n",
    "\n",
    "#         # Get source metadata\n",
    "#         source_metadata = self.samples.filter(pl.col(\"sample_id\").is_in(source_sample_ids))\n",
    "\n",
    "#         for copy_idx in range(n_copies):\n",
    "#             # Get features for source samples\n",
    "#             features_dict = self.features.get_features(source_sample_ids, strict=False)\n",
    "\n",
    "#             if not features_dict:\n",
    "#                 continue\n",
    "\n",
    "#             # Apply augmentation function if provided\n",
    "#             if augmentation_func is not None:\n",
    "#                 features_dict = {name: augmentation_func(features)\n",
    "#                                for name, features in features_dict.items()}\n",
    "\n",
    "#             # Get targets for source samples (if available)\n",
    "#             try:\n",
    "#                 if self.targets.has_targets(source_sample_ids):\n",
    "#                     source_targets = [self.targets.targets[sid] for sid in source_sample_ids]\n",
    "#                 else:\n",
    "#                     source_targets = None\n",
    "#             except:\n",
    "#                 source_targets = None\n",
    "\n",
    "#             # Create new sample IDs\n",
    "#             n_new = len(source_sample_ids)\n",
    "#             new_ids = list(range(self._next_sample_id, self._next_sample_id + n_new))\n",
    "#             self._next_sample_id += n_new\n",
    "#             new_sample_ids.extend(new_ids)\n",
    "\n",
    "#             # Add augmented features\n",
    "#             for source_name, features in features_dict.items():\n",
    "#                 # Use the original source name for augmented data to maintain compatibility\n",
    "#                 aug_source_name = f\"{source_name}_aug_{copy_idx}\"\n",
    "#                 aug_metadata = {'augmentation_copy': copy_idx, 'original_source': source_name}\n",
    "#                 self.features.add_source(aug_source_name, features, new_ids, aug_metadata)\n",
    "\n",
    "#             # Add augmented targets\n",
    "#             if source_targets is not None:\n",
    "#                 self.targets.add_targets(new_ids, source_targets)\n",
    "\n",
    "#             # Add augmented sample metadata\n",
    "#             new_metadata = []\n",
    "#             for i, row in enumerate(source_metadata.iter_rows(named=True)):\n",
    "#                 new_metadata.append({\n",
    "#                     \"sample_id\": new_ids[i],\n",
    "#                     \"partition\": target_partition or row[\"partition\"],\n",
    "#                     \"group\": row[\"group\"],\n",
    "#                     \"branch\": row[\"branch\"],\n",
    "#                     \"processing\": row[\"processing\"],\n",
    "#                     \"origin\": row[\"sample_id\"],  # Keep track of original sample\n",
    "#                     \"augmentation_type\": augmentation_type,\n",
    "#                     \"created_at\": datetime.now(),\n",
    "#                 })\n",
    "\n",
    "#             new_samples_df = pl.DataFrame(new_metadata)\n",
    "#             self.samples = pl.concat([self.samples, new_samples_df])\n",
    "\n",
    "#         return new_sample_ids\n",
    "\n",
    "#     def create_branches(self,\n",
    "#                        source_query: Query,\n",
    "#                        n_branches: int,\n",
    "#                        branch_prefix: str = \"branch\") -> Dict[int, List[int]]:\n",
    "#         \"\"\"Create parallel processing branches\"\"\"\n",
    "#         source_sample_ids = self.get_sample_ids(source_query)\n",
    "\n",
    "#         if not source_sample_ids:\n",
    "#             return {}\n",
    "\n",
    "#         branch_sample_ids = {}\n",
    "\n",
    "#         for branch_id in range(1, n_branches + 1):  # Start from 1, assuming 0 is original\n",
    "#             # Create copies for this branch\n",
    "#             copy_query = self.query().sample_ids(source_sample_ids)\n",
    "#             new_ids = self.augment_samples(\n",
    "#                 source_query=copy_query,\n",
    "#                 n_copies=1,\n",
    "#                 augmentation_type=f\"{branch_prefix}_{branch_id}\",\n",
    "#             )\n",
    "\n",
    "#             # Update branch information\n",
    "#             if new_ids:\n",
    "#                 self.samples = self.samples.with_columns(\n",
    "#                     pl.when(pl.col(\"sample_id\").is_in(new_ids))\n",
    "#                     .then(pl.lit(branch_id))\n",
    "#                     .otherwise(pl.col(\"branch\"))\n",
    "#                     .alias(\"branch\")\n",
    "#                 )\n",
    "\n",
    "#             branch_sample_ids[branch_id] = new_ids\n",
    "\n",
    "#         return branch_sample_ids\n",
    "\n",
    "#     def get_compatibility_report(self, query: Optional[Query] = None) -> Dict[str, Any]:\n",
    "#         \"\"\"Get comprehensive compatibility report for features and targets\"\"\"\n",
    "#         sample_ids = self.get_sample_ids(query)\n",
    "\n",
    "#         if not sample_ids:\n",
    "#             return {'sample_ids': [], 'n_samples': 0, 'features': {}, 'targets': {}}\n",
    "\n",
    "#         feature_compat = self.features.get_source_compatibility(sample_ids)\n",
    "#         target_has_all = self.targets.has_targets(sample_ids)\n",
    "\n",
    "#         return {\n",
    "#             'sample_ids': sample_ids,\n",
    "#             'n_samples': len(sample_ids),\n",
    "#             'features': feature_compat,\n",
    "#             'targets': {\n",
    "#                 'has_all_targets': target_has_all,\n",
    "#                 'available_targets': sum(1 for sid in sample_ids if sid in self.targets.targets),\n",
    "#                 'missing_targets': [sid for sid in sample_ids if sid not in self.targets.targets]\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#     def get_info(self) -> Dict[str, Any]:\n",
    "#         \"\"\"Get comprehensive dataset information\"\"\"\n",
    "#         partitions = self.samples[\"partition\"].unique().to_list()\n",
    "#         groups = sorted(self.samples[\"group\"].unique().to_list())\n",
    "#         branches = sorted(self.samples[\"branch\"].unique().to_list())\n",
    "#         processing_levels = self.samples[\"processing\"].unique().to_list()\n",
    "\n",
    "#         # Origin analysis\n",
    "#         origin_stats = self.samples.group_by(\"origin\").agg([\n",
    "#             pl.count(\"sample_id\").alias(\"n_derived_samples\"),\n",
    "#             pl.col(\"augmentation_type\").unique().alias(\"augmentation_types\")\n",
    "#         ])\n",
    "\n",
    "#         return {\n",
    "#             \"n_samples\": len(self.samples),\n",
    "#             \"sample_id_range\": (self.samples[\"sample_id\"].min(), self.samples[\"sample_id\"].max()),\n",
    "#             \"partitions\": partitions,\n",
    "#             \"groups\": groups,\n",
    "#             \"branches\": branches,\n",
    "#             \"processing_levels\": processing_levels,\n",
    "#             \"features\": self.features.get_summary(),\n",
    "#             \"targets\": self.targets.get_target_stats(),\n",
    "#             \"results\": self.results.get_experiment_summary(),\n",
    "#             \"origin_analysis\": {\n",
    "#                 \"n_original_samples\": len(origin_stats),\n",
    "#                 \"total_derived\": len(self.samples) - len(origin_stats),\n",
    "#                 \"augmentation_types\": self.samples[\"augmentation_type\"].unique().to_list()\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         info = self.get_info()\n",
    "#         feature_info = info['features']\n",
    "#         target_info = info['targets']\n",
    "\n",
    "#         return f\"\"\"SpectraDataset(\n",
    "#     samples={info['n_samples']} (origins: {info['origin_analysis']['n_original_samples']}, derived: {info['origin_analysis']['total_derived']}),\n",
    "#     partitions={info['partitions']},\n",
    "#     groups={info['groups']},\n",
    "#     branches={info['branches']},\n",
    "#     features={feature_info['n_sources']} sources ({feature_info['total_features']} total features),\n",
    "#     targets={target_info.get('task_type', 'none')} ({target_info.get('n_samples', 0)} samples),\n",
    "#     results={info['results']['n_experiments']} experiments\n",
    "# )\"\"\"\n",
    "\n",
    "\n",
    "# # Usage examples demonstrating enhanced functionality:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create enhanced dataset\n",
    "#     dataset = SpectraDataset(max_workers=4)\n",
    "\n",
    "#     # Add multi-source training data\n",
    "#     nirs_features = np.random.randn(100, 1000)  # NIRS with 1000 features\n",
    "#     raman_features = np.random.randn(100, 500)   # Raman with 500 features\n",
    "#     train_targets = np.random.choice(['A', 'B', 'C'], 100)\n",
    "\n",
    "#     train_ids = dataset.add_data(\n",
    "#         features={\n",
    "#             \"nirs\": nirs_features,\n",
    "#             \"raman\": raman_features\n",
    "#         },\n",
    "#         targets=train_targets,\n",
    "#         partition=\"train\",\n",
    "#         group=1,\n",
    "#         source_metadata={\n",
    "#             \"nirs\": {\"wavelength_range\": \"800-2500nm\", \"resolution\": \"2nm\"},\n",
    "#             \"raman\": {\"laser\": \"785nm\", \"range\": \"200-3000cm-1\"}\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # Add test data with different feature sizes (demonstrate flexibility)\n",
    "#     test_nirs = np.random.randn(30, 1000)  # Same NIRS size\n",
    "#     test_raman = np.random.randn(30, 300)   # Different Raman size - will be separate source\n",
    "#     test_targets = np.random.choice(['A', 'B', 'C'], 30)\n",
    "\n",
    "#     test_ids = dataset.add_data(\n",
    "#         features={\n",
    "#             \"nirs_test\": test_nirs,    # Different source name for different size\n",
    "#             \"raman_test\": test_raman\n",
    "#         },\n",
    "#         targets=test_targets,\n",
    "#         partition=\"test\",\n",
    "#         group=1\n",
    "#     )\n",
    "\n",
    "#     # Complex queries using enhanced Query builder\n",
    "\n",
    "#     # Query 1: All train samples with their original data (origin == sample_id)\n",
    "#     original_train_query = dataset.query().partition(\"train\").originals_only()\n",
    "\n",
    "#     # Query 2: All samples (original + augmented) that originated from specific samples\n",
    "#     samples_from_origin = dataset.query().samples_with_origin(train_ids[0])\n",
    "\n",
    "#     # Query 3: All augmented samples in train partition\n",
    "#     augmented_train_query = dataset.query().partition(\"train\").augmented_only()\n",
    "\n",
    "#     # Get features with different strategies\n",
    "#     X_train_concat = dataset.get_features(original_train_query, concatenate=True, strict=False)\n",
    "#     X_train_separate = dataset.get_features(original_train_query, concatenate=False, strict=False)\n",
    "#     y_train = dataset.get_targets(original_train_query)\n",
    "\n",
    "#     print(\"Training data shape (concatenated):\", X_train_concat.shape)\n",
    "#     print(\"Training targets shape:\", y_train.shape)\n",
    "#     print(\"Separate sources:\", {k: v.shape for k, v in X_train_separate.items()})\n",
    "\n",
    "#     # Augment training data with origin tracking\n",
    "#     augmentation_query = dataset.query().partition(\"train\").group(1).originals_only()\n",
    "\n",
    "#     def add_noise(features):\n",
    "#         return features + np.random.normal(0, 0.1, features.shape)\n",
    "\n",
    "#     augmented_ids = dataset.augment_samples(\n",
    "#         source_query=augmentation_query,\n",
    "#         n_copies=2,\n",
    "#         augmentation_type=\"noise_augmented\",\n",
    "#         augmentation_func=add_noise\n",
    "#     )\n",
    "\n",
    "#     print(f\"Created {len(augmented_ids)} augmented samples\")\n",
    "\n",
    "#     # Demonstrate complex origin-based queries\n",
    "#     print(\"\\n=== Origin Analysis ===\")\n",
    "\n",
    "#     # Find all samples derived from first train sample\n",
    "#     derived_query = dataset.query().origin(train_ids[0])\n",
    "#     derived_samples = dataset.get_sample_metadata(derived_query)\n",
    "#     print(f\"Samples derived from train_id {train_ids[0]}: {len(derived_samples)}\")\n",
    "#     print(derived_samples[[\"sample_id\", \"origin\", \"augmentation_type\", \"partition\"]])\n",
    "\n",
    "#     # Get compatibility report\n",
    "#     print(\"\\n=== Compatibility Report ===\")\n",
    "#     compat_report = dataset.get_compatibility_report(original_train_query)\n",
    "#     print(\"Feature source compatibility:\")\n",
    "#     for source, info in compat_report['features'].items():\n",
    "#         print(f\"  {source}: {info['coverage']:.2%} coverage\")\n",
    "\n",
    "#     print(\"\\nDataset Summary:\")\n",
    "#     print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focused demonstration of enhanced SpectraDataset features\n",
    "print(\"=== Enhanced SpectraDataset Demonstration ===\\n\")\n",
    "\n",
    "# Create a clean dataset for demonstration\n",
    "demo_dataset = SpectraDataset(max_workers=2)\n",
    "\n",
    "# 1. Multi-source data with origin tracking\n",
    "print(\"1. Adding multi-source data with origin tracking:\")\n",
    "nirs_data = np.random.randn(50, 100)  # 50 samples, 100 features\n",
    "raman_data = np.random.randn(50, 80)   # 50 samples, 80 features\n",
    "targets = np.random.choice(['Class_A', 'Class_B', 'Class_C'], 50)\n",
    "\n",
    "train_ids = demo_dataset.add_data(\n",
    "    features={\"nirs\": nirs_data, \"raman\": raman_data},\n",
    "    targets=targets,\n",
    "    partition=\"train\",\n",
    "    group=1\n",
    ")\n",
    "\n",
    "print(f\"Added {len(train_ids)} training samples with origins: {train_ids[:5]}...{train_ids[-5:]}\")\n",
    "\n",
    "# 2. Complex queries with origin field\n",
    "print(\"\\n2. Complex queries with origin field:\")\n",
    "\n",
    "# Original samples only (origin == sample_id)\n",
    "original_query = demo_dataset.query().partition(\"train\").originals_only()\n",
    "original_ids = demo_dataset.get_sample_ids(original_query)\n",
    "print(f\"Original train samples: {len(original_ids)}\")\n",
    "\n",
    "# 3. Augmentation with origin tracking\n",
    "print(\"\\n3. Augmentation with origin tracking:\")\n",
    "augmented_ids = demo_dataset.augment_samples(\n",
    "    source_query=original_query,\n",
    "    n_copies=1,\n",
    "    augmentation_type=\"noise_augmented\"\n",
    ")\n",
    "print(f\"Created {len(augmented_ids)} augmented samples\")\n",
    "\n",
    "# 4. Origin-based queries\n",
    "print(\"\\n4. Origin-based queries:\")\n",
    "\n",
    "# Find all samples derived from first original sample\n",
    "derived_query = demo_dataset.query().origin(train_ids[0])\n",
    "derived_ids = demo_dataset.get_sample_ids(derived_query)\n",
    "print(f\"Samples derived from origin {train_ids[0]}: {derived_ids}\")\n",
    "\n",
    "# All augmented samples\n",
    "augmented_query = demo_dataset.query().augmented_only()\n",
    "all_augmented_ids = demo_dataset.get_sample_ids(augmented_query)\n",
    "print(f\"All augmented samples: {len(all_augmented_ids)}\")\n",
    "\n",
    "# 5. Multi-source feature extraction\n",
    "print(\"\\n5. Multi-source feature extraction:\")\n",
    "\n",
    "# Get concatenated features with auto-source selection\n",
    "X_concat = demo_dataset.get_features(original_query, concatenate=True, strict=False)\n",
    "print(f\"Concatenated features shape: {X_concat.shape}\")\n",
    "\n",
    "# Get separate source features\n",
    "X_separate = demo_dataset.get_features(original_query, concatenate=False, strict=False)\n",
    "print(\"Separate source shapes:\", {k: v.shape for k, v in X_separate.items()})\n",
    "\n",
    "# 6. Source compatibility analysis\n",
    "print(\"\\n6. Source compatibility analysis:\")\n",
    "compat_report = demo_dataset.get_compatibility_report()\n",
    "print(\"Feature compatibility:\")\n",
    "for source, info in compat_report['features'].items():\n",
    "    print(f\"  {source}: {info['coverage']:.1%} coverage ({info['available_samples']}/{info['total_requested']} samples)\")\n",
    "\n",
    "# 7. Dataset summary with origin analysis\n",
    "print(f\"\\n7. Enhanced dataset info:\")\n",
    "print(demo_dataset)\n",
    "\n",
    "print(\"\\n=== Key Improvements Demonstrated ===\")\n",
    "print(\"✓ Origin field support for tracking sample relationships\")\n",
    "print(\"✓ Complex queries (originals_only, augmented_only, samples_with_origin)\")\n",
    "print(\"✓ Multi-source management with different feature sizes\")\n",
    "print(\"✓ Enhanced augmentation with origin tracking\")\n",
    "print(\"✓ Source compatibility analysis\")\n",
    "print(\"✓ Flexible feature extraction (concatenated vs separate)\")\n",
    "print(\"✓ Maintained modular, readable design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a68261",
   "metadata": {},
   "source": [
    "## Summary: Enhanced Modular SpectraDataset\n",
    "\n",
    "The modular SpectraDataset prototype has been successfully improved to match the features of the full implementation while maintaining its clean, readable design. Here are the key enhancements:\n",
    "\n",
    "### 🎯 **Core Improvements**\n",
    "\n",
    "#### 1. **Origin Field Support**\n",
    "- Added `origin` field to track sample relationships\n",
    "- Enables querying all samples derived from a specific original sample\n",
    "- Supports complex augmentation scenarios with full lineage tracking\n",
    "\n",
    "#### 2. **Enhanced Query System**\n",
    "- **Complex Queries**: `originals_only()`, `augmented_only()`, `samples_with_origin()`\n",
    "- **Multi-field Filtering**: Combine partition, group, branch, origin, processing, etc.\n",
    "- **Custom Filters**: Support for arbitrary filter functions\n",
    "- **Fluent Interface**: Chainable query methods for readable code\n",
    "\n",
    "#### 3. **Robust Multi-Source Management**\n",
    "- **Different Feature Sizes**: Handle sources with different numbers of features (e.g., NIRS 1000, Raman 500)\n",
    "- **Parallel Processing**: Optional parallel feature extraction for performance\n",
    "- **Source Compatibility**: Automatic checking of which sources can provide requested samples\n",
    "- **Flexible Retrieval**: Concatenated or separate source access\n",
    "\n",
    "#### 4. **Enhanced Augmentation**\n",
    "- **Origin Tracking**: Augmented samples maintain link to original source\n",
    "- **Custom Functions**: Support for custom augmentation transformations\n",
    "- **Metadata Preservation**: Full sample metadata carried forward\n",
    "- **Flexible Targeting**: Augment to different partitions/groups\n",
    "\n",
    "### 🏗️ **Architecture Benefits**\n",
    "\n",
    "#### **Maintained Modularity**\n",
    "- `FeatureStore`: Manages multi-source features independently\n",
    "- `TargetManager`: Handles ML target encoding and validation  \n",
    "- `Query`: Provides intuitive filtering interface\n",
    "- `ResultsManager`: Separate experiment tracking\n",
    "- `SampleMetadata`: Clear metadata representation\n",
    "\n",
    "#### **Enhanced Usability**\n",
    "```python\n",
    "# Complex query example: All train samples with origin=sample_id  \n",
    "original_train = dataset.query().partition(\"train\").originals_only()\n",
    "\n",
    "# Multi-source with different sizes\n",
    "dataset.add_data({\n",
    "    \"nirs\": nirs_1000_features,    # 1000 features\n",
    "    \"raman\": raman_500_features    # 500 features  \n",
    "})\n",
    "\n",
    "# Automatic source compatibility handling\n",
    "X = dataset.get_features(query, strict=False)  # Auto-selects compatible sources\n",
    "```\n",
    "\n",
    "#### **Production-Ready Features**\n",
    "- **Error Handling**: Graceful handling of missing samples/sources\n",
    "- **Performance**: Parallel processing, lazy evaluation, optimized lookups\n",
    "- **Validation**: Comprehensive data validation and consistency checks\n",
    "- **Memory Efficiency**: Efficient storage and retrieval patterns\n",
    "\n",
    "### ✅ **Full Feature Parity**\n",
    "\n",
    "The enhanced prototype now provides all the key capabilities of the full SpectraDataset:\n",
    "\n",
    "| Feature | Full Implementation | Enhanced Prototype | \n",
    "|---------|-------------------|-------------------|\n",
    "| Multi-source features | ✅ | ✅ |\n",
    "| Origin tracking | ✅ | ✅ |\n",
    "| Complex queries | ✅ | ✅ |\n",
    "| Augmentation | ✅ | ✅ |\n",
    "| Branch management | ✅ | ✅ |\n",
    "| Target encoding | ✅ | ✅ |\n",
    "| Results tracking | ✅ | ✅ |\n",
    "| Parallel processing | ❌ | ✅ |\n",
    "| Source compatibility | ❌ | ✅ |\n",
    "\n",
    "**The modular design is now production-ready while maintaining its clean, readable architecture!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
