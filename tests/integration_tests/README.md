# Integration Test Suite for nirs4all

This directory contains comprehensive integration tests based on the example scripts (Q1-Q14).
These tests ensure that the library's core features work correctly and provide regression protection
during refactoring.

## Test Organization

### Example-Based Integration Tests

Each test file corresponds to one or more example scripts and tests the features they demonstrate:

| Test File | Example(s) | Features Tested |
|-----------|-----------|-----------------|
| `test_classification_integration.py` | Q1_classif.py, Q1_classif_tf.py | RandomForest classification, TensorFlow models, confusion matrix analysis, feature augmentation |
| `test_groupsplit_integration.py` | Q1_groupsplit.py | GroupKFold, StratifiedGroupKFold, Sample_ID metadata, leak prevention |
| `test_prediction_reuse_integration.py` | Q5_predict.py, Q5_predict_NN.py | Model persistence, prediction with entry, prediction with model ID, TensorFlow model reuse |
| `test_multisource_integration.py` | Q6_multisource.py | Multi-target regression, model reuse across targets |
| `test_shap_integration.py` | Q8_shap.py | SHAP explanations, different explainer types, visualizations, binning options |
| `test_flexible_inputs_integration.py` | Q11_flexible_inputs.py | Direct numpy arrays, tuple inputs, dict formats, backward compatibility |
| `test_sample_augmentation_integration.py` | Q12_sample_augmentation.py | Standard augmentation, balanced augmentation, leak prevention in CV |
| `test_pca_analysis_integration.py` | Q9_acp_spread.py | PCA preprocessing evaluation, cross-dataset metrics |
| `test_finetune_integration.py` | Q3_finetune.py | Hyperparameter optimization with Optuna, sampling strategies, TensorFlow finetuning |

### Existing Test Coverage

These features already have comprehensive tests elsewhere:
- **Basic pipelines**: `test_basic_pipeline.py`, `test_comprehensive_integration.py`
- **Regression & multi-model**: `test_comprehensive_integration.py`
- **Discretization (Q7)**: `test_comprehensive_integration.py::test_q7_style_classification_from_regression`
- **Resampler (Q10)**: `test_resampler.py`
- **Workspace (Q14)**: `tests/workspace/test_phase*.py`
- **Multi-dataset (Q4)**: `test_comprehensive_integration.py::test_q3_style_multi_dataset`
- **nm headers (Q13)**: Covered by resampler tests with unit conversion

## Running the Tests

### Run All Integration Tests
```bash
pytest tests/integration_tests/ -v
```

### Run Specific Test File
```bash
pytest tests/integration_tests/test_classification_integration.py -v
```

### Run Tests by Marker
```bash
# Only TensorFlow tests
pytest tests/integration_tests/ -m tensorflow -v

# Only SHAP tests
pytest tests/integration_tests/ -m shap -v

# Only Optuna finetuning tests
pytest tests/integration_tests/ -m optuna -v

# Skip slow tests
pytest tests/integration_tests/ -m "not slow" -v
```

### Run with Coverage
```bash
pytest tests/integration_tests/ --cov=nirs4all --cov-report=html
```

## Test Markers

The following pytest markers are used:

- `@pytest.mark.tensorflow` - Tests requiring TensorFlow
- `@pytest.mark.shap` - Tests requiring SHAP library
- `@pytest.mark.optuna` - Tests requiring Optuna for hyperparameter tuning
- `@pytest.mark.sklearn` - Tests specific to sklearn models
- `@pytest.mark.slow` - Tests that take longer to run

## Test Data

All tests use synthetic data generated by `tests/utils/test_data_generator.py` via the `TestDataManager` class.
No external data files are required. The test data manager automatically cleans up after each test.

## Key Testing Principles

### 1. Fast Execution
- Reduced epochs, n_splits, n_trials for speed
- Minimal preprocessing combinations
- Small synthetic datasets

### 2. Feature Coverage
- Each test verifies core functionality works
- Tests check for valid outputs, not perfect predictions
- Focus on API contracts and data flow

### 3. Regression Protection
- Tests ensure refactoring doesn't break existing features
- Predictions must be finite (not NaN/Inf)
- Metrics must be in valid ranges
- File I/O and persistence must work

### 4. Error Handling
- Tests include error cases where appropriate
- `continue_on_error=True` used when testing robustness
- Invalid configurations should fail gracefully

## What These Tests DON'T Do

These are integration tests, not:
- **Performance benchmarks** - Speed is not measured
- **Accuracy tests** - Prediction quality on real data is not verified
- **Visualization tests** - Charts are created but not validated
- **Unit tests** - Internal implementation details are not tested

## Adding New Tests

When adding a new example or feature:

1. Create a new test file or add to existing one
2. Use `TestDataManager` for synthetic data
3. Keep tests fast (reduce iterations, epochs, etc.)
4. Test the happy path and key error cases
5. Mark tests with appropriate markers (`@pytest.mark.tensorflow`, etc.)
6. Verify outputs are valid (finite, correct shape, etc.)

## Continuous Integration

These tests should be run:
- Before each commit (quick smoke tests)
- In CI pipeline (full suite)
- Before releases (with extended parameters)

## Troubleshooting

### TensorFlow Tests Fail
```bash
pip install tensorflow
```

### SHAP Tests Fail
```bash
pip install shap
```

### Optuna Tests Fail
```bash
pip install optuna
```

### Tests Run Too Slow
Use markers to skip slow tests:
```bash
pytest tests/integration_tests/ -m "not slow and not tensorflow"
```

## Test Maintenance

When refactoring the library:

1. **Run all tests first** to establish baseline
2. **Keep tests passing** during refactoring
3. **Update tests** if API intentionally changes
4. **Add new tests** for new features
5. **Don't delete tests** unless feature is removed

## Coverage Goals

These integration tests should cover:
- ✅ All pipeline operators (preprocessing, models, splitters)
- ✅ All input formats (numpy, dict, DatasetConfigs)
- ✅ All output formats (predictions, models, artifacts)
- ✅ Model persistence and reuse
- ✅ Multi-dataset and multi-target scenarios
- ✅ Cross-validation strategies
- ✅ Feature and sample augmentation
- ✅ Hyperparameter optimization
- ✅ Explainability (SHAP)
- ✅ Analysis tools (PCA, heatmaps)

## Success Criteria

A successful integration test run means:
- All tests pass without errors
- No NaN or Inf in predictions
- Files are created/saved when expected
- Models can be loaded and reused
- Different configurations produce different results
- Error cases are handled gracefully

This gives confidence that the library is working correctly across all major use cases.
